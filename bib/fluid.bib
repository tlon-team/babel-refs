
@InBook{Williams1973CritiqueOfUtilitarianism,
	pages = {77–150},
	author = {Williams, Bernard},
	langid = {english},
	database = {Tlön},
	booktitle = {Utilitarianism: For and Against},
	crossref = {Smart1973Utilitarianism},
	title = {A critique of utilitarianism},
	timestamp = {2023-08-06 11:00:57 (GMT)}
}

@online{Wiblin2021ArgumentsForWorking,
	database = {Tlön},
	title = {Arguments for working on global health and wellbeing},
	langid = {english},
	url = {https://80000hours.org/podcast/episodes/alexander-berger-improving-global-health-wellbeing-clear-direct-ways/?fbclid=IwAR1bX7dsPpAByVmqD0m5oM4cWmHDyomiVq54aNwckuOjGZsEHkDWCqW-UIE#arguments-for-working-on-global-health-and-wellbeing-003916},
	origtitle = {Alexander Berger on improving global health and wellbeing in clear and direct ways},
	crossref = {Wiblin2021AlexanderBergerImproving},
	timestamp = {2023-07-29 14:34:52 (GMT)}
}

@book{Smart1981UtilitarismoProContra,
	translation = {Smart1973Utilitarianism},
	crossref = {Smart1973Utilitarianism},
	langid = {spanish},
	translator = {Rodríguez Marín, Jesús},
	file = {~/Google Drive/library-pdf/Smart1981UtilitarismoProContra.pdf},
	database = {Tlön},
	title = {Utilitarismo: pro y contra},
	publisher = {Tecnos},
	date = {1981},
	isbn = {9788430908585},
	location = {Madrid},
	note = {{OCLC}: 778160112},
	shorttitle = {Utilitarismo},
	timestamp = {2023-08-07 22:47:29 (GMT)}
}

@InBook{Singer1993AboutEthics,
	database = {Tlön},
	pages = {1–15},
	langid = {english},
	booktitle = {Practical ethics},
	title = {About ethics},
	crossref = {Singer1993PracticalEthics},
	timestamp = {2023-07-26 21:42:06 (GMT)}
}

@Book{Scanlon2003QueNosDebemos,
	translation = {Scanlon2003QueNosDebemos},
	location = {Barcelona},
	langid = {spanish},
	crossref = {Scanlon1998WhatWeOwe},
	database = {Tlön},
	publisher = {Paidós},
	translator = {Weikert García, Ernest},
	date = {2003},
	title = {Lo que nos debemos unos a otros: ¿qué significa ser moral?},
	isbn = {9788449314643},
	timestamp = {2023-08-11 13:22:32 (GMT)}
}

@InBook{Railton2003FactsAndValues,
	database = {Tlön},
	pages = {43–68},
	title = {Facts and values},
	langid = {english},
	crossref = {Railton2003FactsValuesAnd},
	timestamp = {2023-07-27 18:52:59 (GMT)}
}

@InCollection{Peters1974NatureAndCulture,
	database = {Tlön},
	date = {1974},
	pages = {213–231},
	langid = {english},
	booktitle = {Animals, Men and Morals: An Inquiry Into The Maltreatment of Non-humans},
	crossref = {Godlovitch1974AnimalsMenAnd},
	title = {Nature and culture},
	author = {Peters, Michael},
	timestamp = {2023-07-04 19:41:06 (GMT)}
}

@InBook{Parfit1984PartTwoRationality,
	database = {Tlön},
	title = {Part two: Rationality and time},
	booktitle = {Reasons and persons},
	langid = {english},
	crossref = {Parfit1984ReasonsPersons},
	timestamp = {2023-07-27 09:29:12 (GMT)}
}

@InBook{Parfit1984HowBothHuman,
	pages = {453–454},
	title = {How both human history, and the history of ethics, may
                  be just beginning},
	abstract = {This  discusses the potential progress of Non-Religious Ethics and its significance. It argues that the systematic study of Non-Religious Ethics is relatively new, having gained momentum only around 1960. The author believes that Non-Religious Ethics, compared to other sciences, is the youngest and least advanced. The article emphasizes that if mankind were to be destroyed, it would be a significant loss not only in terms of the sum of happiness but also in terms of the potential achievements in the sciences, arts, and moral progress. The author suggests that the highest achievements in these fields are yet to come, particularly in Non-Religious Ethics. The author also points out that disbelief in God is a recent phenomenon and as such, Non-Religious Ethics is still in its early stages of development. Therefore, it is reasonable to hold high hopes for its future progress. – AI-generated abstract.},
	langid = {english},
	database = {Tlön},
	crossref = {Parfit1984ReasonsPersons},
	timestamp = {2023-06-21 07:17:08 (GMT)}
}

@InBook{Ord2020ExistentialRisk,
	pages = {35–64},
	title = {Existential risk},
	abstract = {Catastrophic natural events have intermittently occurred throughout Earth's history with varying degrees of severity, some leading to mass extinctions while others pose no genuine existential threat to humanity. The first category includes phenomena such as super-volcanic eruptions, asteroid/comet impacts, and stellar explosions (supernovae and gamma-ray bursts), while natural catastrophes like hurricanes or tsunamis fall under the second. There is considerable uncertainty regarding the specific probabilities of these threats, particularly those from natural catastrophes like super-volcanic eruptions for which reliable data is absent. Nonetheless, some upper bounds and best-guess estimates can be made through methods such as cataloging known risks, analyzing the fossil record, and considering humanity's resilience and adaptability advantages. These estimates place the natural extinction risk for humanity at around 0.5 percent per century with a best guess of 0.05 percent or lower, confirming an overwhelming consensus that natural existential risks are extremely low. – AI-generated abstract.},
	langid = {english},
	database = {Tlön},
	crossref = {Ord2020PrecipiceExistentialRisk},
	timestamp = {2023-06-21 07:28:19 (GMT)}
}

@InBook{Nagel1979Equality,
	database = {Tlön},
	title = {Equality},
	booktitle = {Mortal questions},
	langid = {english},
	pages = {106–127},
	crossref = {Nagel1979MortalQuestions},
	timestamp = {2023-08-05 16:37:28 (GMT)}
}

@InBook{Mill1969Utilitarianism,
	database = {Tlön},
	title = {Utilitarianism},
	langid = {english},
	volume = 10,
	pages = {203–259},
	author = {Mill, John Stuart},
	date = 1969,
	crossref = {Mill1991CollectedWorksOf}
}

@InBook{Mill1859Liberty,
	database = {Tlön},
	date = {1859},
	pages = {213–310},
	langid = {english},
	volume = {18},
	booktitle = {The Collected Works of John Stuart Mill},
	title = {On Liberty},
	crossref = {Mill1991CollectedWorksOf},
	timestamp = {2023-07-19 21:42:06 (GMT)}
}

@InBook{Marx1843BetrachtungEinesJunglings,
	pages = {454–459},
	database = {Tlön},
	langid = {german},
	author = {Marx, Karl},
	date = {1835},
	title = {Betrachtung eines Jünglings bei der Wahl eines
                  Berufes},
	crossref = {Marx1975GesamtausgabeI1},
	timestamp = {2023-06-23 22:35:46 (GMT)}
}

@InBook{Macaskill2023UtilitarismoEticaPractica,
	url = {https://www.utilitarismo.net/utilitarismo-y-etica-practica},
	crossref = {Chappell2023IntroduccionAlUtilitarismo},
	booktitle = {Una introducción al utilitarismo},
	bookauthor = {Chappell, Richard Yetter and Meissner, Darius and {MacAskill}, William},
	langid = {spanish},
	date = {2023},
	title = {Utilitarismo y ética práctica},
	database = {Tlön},
	author = {{MacAskill}, William and Meissner, Darius and Chappell, Richard Yetter},
	translator = {Tlön},
	translation = {MacAskill2023UtilitarianismPracticalEthics}
}

@InBook{Macaskill2023ObrarSegunUtilitarismo,
	date = {2023},
	title = {Obrar según el utilitarismo},
	database = {Tlön},
	author = {{MacAskill}, William and Meissner, Darius},
	crossref = {Chappell2023IntroduccionAlUtilitarismo},
	booktitle = {Una introducción al utilitarismo},
	bookauthor = {Chappell, Richard Yetter and Meissner, Darius and {MacAskill}, William},
	translator = {Tlön},
	langid = {spanish},
	translation = {MacAskill2023ActingUtilitarianism}
}

@InBook{Macaskill2023ObjecionDeExigencia,
	database = {Tlön},
	date = {2023},
	title = {La objeción de exigencia},
	author = {{MacAskill}, William and Meissner, Darius and
                  Chappell, Richard Yetter},
	crossref = {Chappell2023IntroduccionAlUtilitarismo},
	booktitle = {Una introducción al utilitarismo},
	bookauthor = {Chappell, Richard Yetter and Meissner, Darius and {MacAskill}, William},
	translator = {Tlön},
	langid = {spanish},
	translation = {MacAskill2020DemandingnessObjection}
}

@InBook{Macaskill2023IntroduccionAlUtilitarismo,
	url = {https://www.utilitarismo.net/introduccion-al-utilitarismo},
	crossref = {Chappell2023IntroduccionAlUtilitarismo},
	booktitle = {Una introducción al utilitarismo},
	bookauthor = {Chappell, Richard Yetter and Meissner, Darius and {MacAskill}, William},
	database = {Tlön},
	date = {2023},
	title = {Introducción al utilitarismo},
	author = {{MacAskill}, William and Meissner, Darius and
                  Chappell, Richard Yetter},
	translator = {Tlön},
	langid = {spanish},
	translation = {Macaskill2023IntroductionToUtilitarianism}
}

@InBook{Macaskill2023ElementosTiposDe,
	url = {https://www.utilitarismo.net/elementos-y-tipos-de-utilitarismo},
	crossref = {Chappell2023IntroduccionAlUtilitarismo},
	langid = {spanish},
	booktitle = {Una introducción al utilitarismo},
	bookauthor = {Chappell, Richard Yetter and Meissner, Darius and {MacAskill}, William},
	date = {2023},
	title = {Elementos y tipos de utilitarismo},
	database = {Tlön},
	author = {{MacAskill}, William and Meissner, Darius and
                  Chappell, Richard Yetter},
	translator = {Tlön},
	translation = {MacAskill2023ElementsTypesUtilitarianism}
}

@InBook{Macaskill2023ActuarConformeAl,
	database = {Tlön},
	url = {https://www.utilitarismo.net/actuar-conforme-al-utilitarismo},
	translator = {Tlön},
	langid = {spanish},
	translation = {MacAskill2023ActingUtilitarianism},
	crossref = {Chappell2023IntroduccionAlUtilitarismo},
	booktitle = {Una introducción al utilitarismo},
	bookauthor = {Chappell, Richard Yetter and Meissner, Darius and {MacAskill}, William},
	author = {{MacAskill}, William and Meissner, Darius},
	date = {2023},
	title = {Actuar conforme al utilitarismo},
	timestamp = {2023-09-18 10:20:12 (GMT)}
}

@InBook{Lyons1980IroquoisPerspective,
	database = {Tlön},
	date = {1980},
	pages = {171--74},
	langid = {english},
	author = {Lyons, Orens},
	title = {An Iroquois perspective},
	booktitle = {American Indian environments: ecological issues in native
                   American history},
	crossref = {Vecsey1980AmericanIndianEnvironments},
	timestamp = {2024-02-12 19:15:39 (GMT)}
}

@InCollection{Lin2015MonismAndPluralism,
	database = {Tlön},
	pages = {331–341},
	crossref = {Fletcher2015RoutledgeHandbookOf},
	langid = {english},
	title = {Monism and pluralism},
	author = {Lin, Eden},
	timestamp = {2023-07-27 18:42:52 (GMT)}
}

@InCollection{Heathwood2015DesireFulfilmentTheory,
	database = {Tlön},
	pages = {135–147},
	title = {Desire-Fulfilment theory},
	langid = {english},
	author = {Heathwood, Christopher C.},
	crossref = {Fletcher2015RoutledgeHandbookOf},
	timestamp = {2023-07-26 21:39:21 (GMT)}
}

@book{Galef2023MentalidadExplorador,
	database = {Tlön},
	location = {Barcelona},
	langid = {spanish},
	publisher = {Paidós},
	translator = {Borrajo, Fernando},
	translation = {Galef2021ScoutMindsetWhy},
	isbn = {978-8449340284},
	date = {2023},
	title = {La mentalidad del explorador: por qué algunas personas ven las cosas con claridad y otras no},
	crossref = {Galef2021ScoutMindsetWhy},
	timestamp = {2023-07-19 22:40:20 (GMT)}
}

@InBook{Driver2001ConsequentialistTheoryOf,
	database = {Tlön},
	pages = {63–83},
	title = {A consequentialist theory of virtue},
	langid = {english},
	booktitle = {Uneasy virtue},
	crossref = {Driver2001UneasyVirtue},
	timestamp = {2023-07-11 19:40:22 (GMT)}
}

@InCollection{Chappell2024ConsequentialismCoreAnd,
	database = {Tlön},
	title = {Consequentialism: core and expansion},
	author = {Chappell, Richard Yetter},
	langid = {english},
	crossref = {Copp2024OxfordHandbookOf},
	timestamp = {2023-07-11 14:03:36 (GMT)}
}

@InBook{Chappell2023TeoriasDelBienestar,
	url = {https://www.utilitarismo.net/teorias-del-bienestar},
	crossref = {Chappell2023IntroduccionAlUtilitarismo},
	langid = {spanish},
	booktitle = {Una introducción al utilitarismo},
	bookauthor = {Chappell, Richard Yetter and Meissner, Darius and {MacAskill}, William},
	date = {2023},
	title = {Teorías del bienestar},
	database = {Tlön},
	author = {Chappell, Richard Yetter and Meissner, Darius},
	translator = {Tlön},
	translation = {Chappell2023TheoriesWellbeing}
}

@InBook{Chappell2023RecursosLecturasAdicionales,
	database = {Tlön},
	date = {2023},
	title = {Recursos y lecturas adicionales},
	author = {Chappell, Richard Yetter and Meissner, Darius and
                  {MacAskill}, William},
	crossref = {Chappell2023IntroduccionAlUtilitarismo},
	booktitle = {Una introducción al utilitarismo},
	bookauthor = {Chappell, Richard Yetter and Meissner, Darius and {MacAskill}, William},
	translator = {Tlön},
	langid = {spanish},
	translation = {Chappell2023ResourcesAndFurther}
}

@InBook{Chappell2023ObjecionesAlUtlitarismo,
	database = {Tlön},
	url = {https://www.utilitarismo.net/objeciones-al-utilitarismo},
	crossref = {Chappell2023IntroduccionAlUtilitarismo},
	langid = {spanish},
	booktitle = {Una introducción al utilitarismo},
	bookauthor = {Chappell, Richard Yetter and Meissner, Darius and {MacAskill}, William},
	date = {2023},
	title = {Objeciones al utilitarismo y respuestas},
	author = {Chappell, Richard Yetter and Meissner, Darius and
                  {MacAskill}, William},
	translator = {Tlön},
	translation = {Chappell2023ObjectionsToUtilitarianism}
}

@InBook{Chappell2023ObjecionDeObligaciones,
	database = {Tlön},
	date = {2023},
	title = {La objeción de las obligaciones especiales},
	author = {Chappell, Richard Yetter and Meissner, Darius},
	crossref = {Chappell2023IntroduccionAlUtilitarismo},
	booktitle = {Una introducción al utilitarismo},
	bookauthor = {Chappell, Richard Yetter and Meissner, Darius and {MacAskill}, William},
	translator = {Tlön},
	langid = {spanish},
	translation = {Chappell2023SpecialObligationsObjection}
}

@InBook{Chappell2023ObjecionDeIncertidumbre,
	database = {Tlön},
	date = {2023},
	title = {La objeción de incertidumbre radical},
	author = {Chappell, Richard Yetter},
	crossref = {Chappell2023IntroduccionAlUtilitarismo},
	booktitle = {Una introducción al utilitarismo},
	bookauthor = {Chappell, Richard Yetter and Meissner, Darius and {MacAskill}, William},
	translator = {Tlön},
	langid = {spanish},
	translation = {Chappell2023CluelessnessObjection}
}

@InBook{Chappell2023ObjecionDeAbusabilidad,
	database = {Tlön},
	date = {2023},
	title = {La objeción de la abusabilidad},
	author = {Chappell, Richard Yetter},
	crossref = {Chappell2023IntroduccionAlUtilitarismo},
	booktitle = {Una introducción al utilitarismo},
	bookauthor = {Chappell, Richard Yetter and Meissner, Darius and {MacAskill}, William},
	translator = {Tlön},
	langid = {spanish},
	translation = {Chappell2023AbusabilityObjection}
}

@InBook{Chappell2023EticaDePoblacion,
	database = {Tlön},
	url = {https://www.utilitarismo.net/etica-de-la-poblacion},
	crossref = {Chappell2023IntroduccionAlUtilitarismo},
	langid = {spanish},
	booktitle = {Una introducción al utilitarismo},
	bookauthor = {Chappell, Richard Yetter and Meissner, Darius and {MacAskill}, William},
	translator = {Tlön},
	translation = {Chappell2023PopulationEthics},
	author = {Chappell, Richard Yetter and Meissner, Darius and {MacAskill}, William},
	date = {2023},
	title = {Ética de la población},
	timestamp = {2023-09-18 10:11:31 (GMT)}
}

@InBook{Chappell2023ArgumentosFavorDel,
	database = {Tlön},
	crossref = {Chappell2023IntroduccionAlUtilitarismo},
	booktitle = {Una introducción al utilitarismo},
	langid = {spanish},
	bookauthor = {Chappell, Richard Yetter and Meissner, Darius and {MacAskill}, William},
	date = {2023},
	title = {Argumentos a favor del utilitarismo},
	author = {Chappell, Richard Yetter and Meissner, Darius},
	translator = {Tlön},
	translation = {Chappell2023ArgumentsForUtilitarianism}
}

@InBook{Chappell2021PopulationEthics,
	booktitle = {Parfit's Ethics},
	title = {Chapter 7: Population ethics},
	database = {Tlön},
	langid = {english},
	crossref = {Chappell2021ParfitEthics},
	timestamp = {2023-07-12 14:49:47 (GMT)}
}

@Book{Bentham2008PrincipiosDeMoral,
	file = {~/Google Drive/library-pdf/Bentham2008PrincipiosDeMoral.pdf},
	database = {Tlön},
	langid = {spanish},
	isbn = {9789506202330},
	location = {Buenos Aires},
	publisher = {Claridad},
	date = {2008},
	translator = {Costa, Margarita},
	title = {Los principios de la moral y la legislacion},
	author = {Bentham, Jeremy},
	translation = {Bentham1789IntroductionPrinciplesMorals},
	crossref = {Bentham1789IntroductionPrinciplesMorals},
	timestamp = {2023-06-26 12:26:10 (GMT)}
}

@InCollection{Bentham1838PrinciplesOfPenal,
	database = {Tlön},
	pages = {365--580},
	volume = {1},
	langid = {english},
	title = {Principles of penal law},
	booktitle = {The works of Jeremy Bentham},
	crossref = {Bentham1838WorksJeremyBentham},
	timestamp = {2023-07-07 13:58:31 (GMT)}
}

@InBook{Bennett2017Chapter2Principles,
	database = {Tlön},
	title = {Chapter 2: Principles opposing the principle of utility},
	booktitle = {An Introduction to the Principles of Morals and Legislation},
	langid = {english},
	crossref = {Bentham2017IntroductionToPrinciples},
	timestamp = {2023-07-27 09:56:07 (GMT)}
}

@Dataset{1989OhioV,
	database = {Tlön},
	date = {1989},
	langid = {english},
	title = {Ohio v. U.S. Dept. Of the Interior, 880 F. 2d 432
                  (Court of Appeals, Dist. of Columbia Circuit 1989)},
	timestamp = {2023-06-08 08:17:44 (GMT)}
}

@Dataset{2010NondiscriminationBasisOf,
	database = {Tlön},
	date = {2010},
	title = {Nondiscrimination on the Basis of Disability in State and Local Government Services, 75 Federal Register 56164 (Sept. 15, 2010) (codified at 28 Code of Federal Regulations, pt. 35)},
	langid = {english},
	timestamp = {2023-07-28 15:26:21 (GMT)}
}

@Dataset{2015MichiganEtAl,
	database = {Tlön},
	date = {2015-07-29},
	langid = {english},
	title = {Michigan, et al. V. Environmental Protection Agency,
                  et al. (No. 14-46); Utility Air Regulatory Group v.
                  Environmental Protection Agency, et al. (No. 14-47);
                  National Mining Association v. Environmental
                  Protection Agency, et al. (No. 14-49), No. 14-46 (135
                  Supreme Court of the United States 2699 29 July 2015)},
	timestamp = {2023-06-08 08:19:26 (GMT)}
}

@Dataset{2022S,
	database = {Tlön},
	title = {S.3799 - PREVENT Pandemics Act},
	langid = {english},
	date = {2022},
	url = {https://www.congress.gov/bill/117th-congress/senate-bill/3799},
	timestamp = {2023-06-08 08:15:29 (GMT)}
}

@online{AIImpacts2014HumanlevelAI,
	database = {Tlön},
	title = {Human-level {AI}},
	abstract = {‘Human-level AI’ refers to AI which can reproduce everything a human can do,
approximately. Several variants of this concept are worth distinguishing.},
	url = {https://aiimpacts.org/human-level-ai/},
	journaltitle = {{AI} Impacts},
	author = {{AI Impacts}},
	urldate = {2022-05-02},
	date = {2014-01-23},
	langid = {english},
	file = {~/Google Drive/library-pdf/AIImpacts2014HumanLevelAI.pdf;~/Google Drive/library-html/human-level-ai.html}
}

@online{AIImpacts2015HowAITimelines,
	database = {Tlön},
	title = {How {AI} timelines are estimated},
	abstract = {A natural approach to informing oneself about when human-level AI will arrive is to check
what experts who have already investigated the question say about it. So we made this list of
analyses that we could find.},
	langid = {english},
	url = {https://aiimpacts.org/how-ai-timelines-are-estimated/},
	journaltitle = {{AI} Impacts},
	author = {{AI Impacts}},
	date = 2015,
	file = {~/Google Drive/library-pdf/AIImpacts2015HowAITimelines.pdf}
}

@online{AIImpacts20222022ExpertSurvey,
	database = {Tlön},
	title = {2022 Expert Survey on Progress in {AI}},
	abstract = {Collected data and analysis from a large survey of machine learning researchers.},
	url = {https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/},
	journaltitle = {{AI} Impacts},
	author = {{AI Impacts}},
	urldate = {2022-08-05},
	date = {2022-08-04},
	langid = {english},
	file = {~/Google Drive/library-pdf/AIImpacts20222022ExpertSurvey.pdf;~/Google Drive/library-html/2022-expert-survey-on-progress-in-ai.html}
}

@online{Aaronson2022OpenAI,
	database = {Tlön},
	title = {{OpenAI}!},
	abstract = {I have some exciting news (for me, anyway). Starting next week, I’ll be going on leave from {UT} Austin for one year, to work at {OpenAI}.},
	url = {https://scottaaronson.blog/?p=6484},
	journaltitle = {Shtetl-Optimized},
	author = {Aaronson, Scott},
	urldate = {2022-06-18},
	date = {2022-06-17},
	langid = {english},
	file = {~/Google Drive/library-pdf/Aaronson2022OpenAI.pdf;~/Google Drive/library-html/scottaaronson.blog.html}
}

@book{Adams1794DefenceOfConstitutions,
	note = {No translation found on 2023-06-30.},
	author = {Adams, John},
	langid = {english},
	title = {A defence of the constitutions of government of the
                  United States of America: against the attack of M.
                  Turgot in his letter to Dr. Price, dated the
                  twenty-second day of March, 1778},
	publisher = {Printed for John Stockdale},
	url = {http://www.heinonline.org/HOL/Index?index=beal/cgusa&collection=beal},
	database = {Tlön},
	date = {1794},
	location = {London},
	shorttitle = {A defence of the constitutions of government of the
                  United States of America},
	timestamp = {2023-07-01 00:04:58 (GMT)},
	urldate = {2023-07-01}
}

@online{Agarwalla2021WhatMetaEffective,
	database = {Tlön},
	title = {What is meta effective altruism?},
	abstract = {Meta Effective Altruism is one of the main focus areas of the Effective Altruism movement. It can include research to help direct efforts (Global Priorities Research) and efforts to build or support the {EA} movement and its members ({EA} Movement Building).  Some meta {EA} projects can also be focused on specific cause areas, professions or more (Within-Cause Meta).  Typically, meta effective altruism is at least one step removed from direct impact.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/Wx7LuMHbhABrtYrv9/what-is-meta-effective-altruism},
	shorttitle = {What is meta Effective Altruism?},
	journaltitle = {Effective Altruism Forum},
	author = {Agarwalla, Vaidehi},
	urldate = {2022-03-21},
	date = {2021-06-02},
	file = {~/Google Drive/library-pdf/Agarwalla2021WhatMetaEffective.pdf;~/Google Drive/library-html/what-is-meta-effective-altruism.html}
}

@online{Aguirre2016PredictingFutureLife,
	database = {Tlön},
	title = {Predicting the future (of life)},
	abstract = {It’s often said that the future is unpredictable. Of course, that’s not really true. With extremely high confidence, we can predict that the sun will rise in Santa Cruz, California at 7:12 am local time on Jan 30, 2016. We know the next total solar eclipse over the U.S. will be August 14, 2017, and we also know there will be one June 25, 2522.},
	file = {~/Google Drive/library-html/Aguirre2016PredictingFutureLife.html;~/Google Drive/library-pdf/Aguirre2016PredictingFutureLife.pdf},
	langid = {english},
	url = {https://futureoflife.org/2016/01/24/predicting-the-future-of-life/},
	journaltitle = {Future of Life Institute},
	author = {Aguirre, Anthony},
	date = {2016-01-24}
}

@online{Aguirre2019HowManyFLOPS,
	database = {Tlön},
	title = {How many {FLOPS} for human-level {AGI}?},
	langid = {english},
	url = {https://www.metaculus.com/questions/2646/what-will-the-necessary-computational-power-to-replicate-human-mental-capability-turn-out-to-be/},
	journaltitle = {Metaculus},
	author = {Aguirre, Anthony},
	date = {2019-03-13}
}

@online{Aird2020CollectionAllPriora,
	database = {Tlön},
	title = {Collection of all prior work I found that seemed
                  substantially relevant to information hazards},
	abstract = {A list of articles related to information hazards.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/EMKf4Gyee7BsY2RP8/michaela-s-shortform?commentId=dTghHNHmc5qf5znMQ},
	journaltitle = {Effective Altruism Forum},
	author = {Aird, Michael},
	date = {2020-02-24},
	file = {~/Google Drive/library-pdf/Aird2020CollectionAllPrior.pdf}
}

@online{Aird2020CollectionEvidenceViews,
	database = {Tlön},
	title = {Collection of evidence about views on longtermism,
                  time discounting, population ethics, significance of
                  suffering vs happiness, etc. among non-{EAs}},
	abstract = {A list of articles related to longtermism, time discounting, population ethics, and significance of suffering vs happiness.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/EMKf4Gyee7BsY2RP8/michaela-s-shortform?commentId=GgW24uSGwTvwP7Hwr},
	journaltitle = {Effective Altruism Forum},
	author = {Aird, Michael},
	date = {2020-05-10},
	file = {~/Google Drive/library-pdf/Aird2020CollectionEvidenceViews.pdf}
}

@online{Aird2020CollectionOfSourcesb,
	journaltitle = {Effective Altruism Forum},
	date = {2020-03-30},
	abstract = {Comment by {MichaelA} - Collection of sources related to dystopias and "robust totalitarianism".

(See also Books on authoritarianism, Russia, China, {NK}, democratic backsliding, etc.?)

The Precipice - Toby Ord (Chapter 5 has a section on Dystopian Scenarios).

The Totalitarian Threat - Bryan Caplan (if that link stops working, a link to a Word doc version can be found on this page) (some related discussion on the 80k podcast here; use the "find" function).

Reducing long-term risks from malevolent actors - David Althaus and Tobias Baumann, 2020

The Centre for the Governance of {AI}’s research agenda - Allan Dafoe (this contains discussion of "robust totalitarianism", and related matters).

A shift in arguments for {AI} risk - Tom Sittler (this has a brief but valuable section on robust totalitarianism) (discussion of the overall piece here).

Existential Risk Prevention as Global Priority - Nick Bostrom (this discusses the concepts of "permanent stagnation" and "flawed realisation", and very briefly touches on their relevance to e.g. lasting totalitarianism).

The Future of Human Evolution - Bostrom, 2009 (I think some scenarios covered there might count as dystopias, depending on definitions).

The Vulnerable World Hypothesis - Bostrom, 2019.

80,000 Hours interview with Tyler Cowen - 2018.

Various works of fiction, most notably Orwell's 1984.

Some sources on dictatorships/totalitarianism in general (without a focus on long-term future consequences).

Dikötter, F. (2019). How to Be a Dictator: The Cult of Personality in the Twentieth Century. Bloomsbury Publishing.

Glad, B. (2002). Why tyrants go too far: Malignant narcissism and absolute power. Political Psychology, 23(1), 1-2.*.

Chang, J., \& Halliday, J. (2007). Mao: The unknown story. Vintage.*.

*Asterisks indicate I haven't read that source myself, and thus that the source might not actually be a good fit for this list.

I intend to add to this list over time. If you know of other relevant work, please mention it in a comment.},
	author = {Aird, Michael},
	database = {Tlön},
	langid = {english},
	timestamp = {2023-07-20 13:47:40 (GMT)},
	title = {Collection of sources related to dystopias and "robust totalitarianism"},
	url = {https://forum.effectivealtruism.org/posts/EMKf4Gyee7BsY2RP8/michaela-s-shortform?commentId=8GJtZ6DrEn5MDf6dZ},
	urldate = {2023-07-20}
}

@online{Aird2020CollectionSourcesThat,
	database = {Tlön},
	title = {Collection of sources that are highly relevant to the
                  idea of the long reflection},
	abstract = {Throughout history we’ve consistently believed, as common sense, truly horrifying things by today’s standards. According to University of Oxford Professor Will {MacAskill}, it’s extremely likely that we’re in the same boat today. If we accept that we’re probably making major moral errors, how should we proceed?.
If our morality is tied to common sense intuitions, we’re probably just preserving these biases and moral errors. Instead we need to develop a moral view that criticises common sense intuitions, and gives us a chance to move beyond them. And if humanity is going to spread to the stars it could be worth dedicating hundreds or thousands of years to moral reflection, lest we spread our errors far and wide.
Will is an Associate Professor in Philosophy at Oxford University, author of Doing Good Better, and one of the co-founders of the effective altruism community. In this interview we discuss a wide range of topics:.
How would we go about a ‘long reflection’ to fix our moral errors?[others].},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/H2zno3ggRJaph9P6c/quotes-about-the-long-reflection?commentId=z2ybSC353mPHpCjbn},
	journaltitle = {Effective Altruism Forum},
	author = {Aird, Michael},
	date = {2020-06-20}
}

@online{Aird2020CollectionWorkValue,
	database = {Tlön},
	title = {Collection of work on value drift that isn't on the
                  {EA} forum},
	abstract = {A list of articles related to value drift.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/EMKf4Gyee7BsY2RP8/michaela-s-shortform?commentId=a3dN6Mehzzipyu5sg},
	journaltitle = {Effective Altruism Forum},
	author = {Aird, Michael},
	date = {2020-04-10},
	file = {~/Google Drive/library-pdf/Aird2020CollectionWorkValue.pdf}
}

@online{Aird2020FailuresTechnologyForecasting,
	database = {Tlön},
	title = {Failures in technology forecasting? A reply to Ord and
                  Yudkowsky},
	abstract = {This article emphasizes the tendency of renowned scientists to make inaccurate forecasts about technological developments. Specifically, the author focuses on failed technology forecasts related to atomic energy, nuclear engineering, the invention of aircraft, and artificial general intelligence (AGI). The author claims that such forecasts often fall short due to direct or indirect connections between the predictions and their disproof, poor communication, and biased samples of historical cases. Furthermore, the author discusses the idea that technological development can take place largely in secret, highlighting the example of nuclear weapons. The author concludes that experts' technology forecasts should be treated with caution, as they might not accurately reflect the pace and timing of technological advancements. – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/3qypPmmNHEmqegoFF/failures-in-technology-forecasting-a-reply-to-ord-and},
	journaltitle = {{LessWrong}},
	author = {Aird, Michael},
	date = {2020-05-08},
	file = {~/Google Drive/library-pdf/Aird2020FailuresTechnologyForecasting.pdf}
}

@online{Aird2020ListThingsVe,
	database = {Tlön},
	title = {List of things I've written or may write that are
                  relevant to The Precipice},
	abstract = {A collection of shorter posts by Michael Aird related to Toby Ord’s book, The Precipice.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/EMKf4Gyee7BsY2RP8/michaela-s-shortform},
	journaltitle = {Effective Altruism Forum},
	author = {Aird, Michael},
	urldate = {2022-01-08},
	date = {2020-04-06},
	file = {~/Google Drive/library-pdf/Aird2020ListThingsVe.pdf;~/Google Drive/library-html/michaela-s-shortform.html}
}

@online{Aird2020QuotesLongReflection,
	database = {Tlön},
	title = {Quotes about the long reflection},
	abstract = {Throughout history we’ve consistently believed, as common sense, truly horrifying things by today’s standards. According to University of Oxford Professor Will {MacAskill}, it’s extremely likely that we’re in the same boat today. If we accept that we’re probably making major moral errors, how should we proceed?.
If our morality is tied to common sense intuitions, we’re probably just preserving these biases and moral errors. Instead we need to develop a moral view that criticises common sense intuitions, and gives us a chance to move beyond them. And if humanity is going to spread to the stars it could be worth dedicating hundreds or thousands of years to moral reflection, lest we spread our errors far and wide.
Will is an Associate Professor in Philosophy at Oxford University, author of Doing Good Better, and one of the co-founders of the effective altruism community. In this interview we discuss a wide range of topics:.
How would we go about a ‘long reflection’ to fix our moral errors?[others].},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/H2zno3ggRJaph9P6c/quotes-about-the-long-reflection},
	journaltitle = {Effective Altruism Forum},
	author = {Aird, Michael},
	date = {2020-03-05},
	file = {~/Google Drive/library-pdf/Aird2020QuotesLongReflection.pdf}
}

@online{Aird2020WhatExistentialSecuritya,
	database = {Tlön},
	title = {What is existential security?},
	abstract = {In The Precipice, Toby Ord defines an existential risk as “a risk that threatens the destruction of humanity’s longterm potential”. This could involve extinction, an unrecoverable collapse, or an unrecoverable dystopia. (See also.).
Ord uses the term existential security to refer to “a place of safety - a place where existential risk is low and stays low”. This doesn’t require reaching a state with zero existential risk per year. But it requires that existential risk per year either (a) indefinitely trends downwards (on average), or (b) is extremely low and roughly stable. This is because even a very low but stable risk per year can practically guarantee existential catastrophe happens at some point, given a long enough time.
My own one-sentence description of existential security would therefore be: A state where the total existential risk across all time is low, such that humanity’s long-term potential is preserved and protected.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/gugGJGdakND6HtbDx/what-is-existential-security},
	shorttitle = {What is existential security?},
	journaltitle = {Effective Altruism Forum},
	author = {Aird, Michael},
	urldate = {2022-04-12},
	date = {2020-09-01},
	file = {~/Google Drive/library-pdf/Aird2020WhatExistentialSecuritya.pdf;~/Google Drive/library-html/what-is-existential-security.html}
}

@online{Aird2021IndependentImpressions,
	database = {Tlön},
	title = {Independent impressions},
	abstract = {Your independent impression about something is essentially what you'd believe about that thing if you weren't updating your beliefs in light of peer disagreement - i.e., if you weren't taking into account your knowledge about what other people believe and how trustworthy their judgement seems on this topic. Your independent impression can take into account the reasons those people have for their beliefs (inasmuch as you know those reasons), but not the mere fact that they believe what they believe.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/2WS3i7eY4CdLH99eg/independent-impressions},
	journaltitle = {Effective Altruism Forum},
	author = {Aird, Michael},
	urldate = {2022-03-25},
	date = {2021-09-26},
	file = {~/Google Drive/library-pdf/Aird2021IndependentImpressions.pdf;~/Google Drive/library-html/independent-impressions.html}
}

@online{Aird2021QuickNotesWhatb,
	database = {Tlön},
	url = {https://docs.google.com/document/d/10AWy7V5DYpCJEk7OazfT8UkOUkSR41KUT_6mIRmwFeY/edit#},
	date = {2021},
	langid = {english},
	journaltitle = {Google Docs},
	title = {Quick notes on what I mean by "research training programs"},
	author = {Aird, Michael},
	timestamp = {2023-07-20 20:48:06 (GMT)}
}

@online{Aird2022InterestedEALongtermist,
	database = {Tlön},
	title = {Interested in {EA}/longtermist research careers? Here
                  are my top recommended resources - {EA} Forum},
	abstract = {Since 2020, I estimate I've given career advice to {\textgreater}200 people in the {EA} community. This post is my up-to-date, prioritized list of recommended resources for people who are interested in research careers, longtermism-aligned careers, and/or working at {EA} orgs (rather than e.g. high-impact roles at non-{EA} orgs that work on relevant topics). The more you're interested in each of those three things, the more likely it is that this post will be useful to you - but I expect it would also be useful to many people interested in just one or two of those things.},
	url = {https://forum.effectivealtruism.org/posts/Na6pkfpZrfyKBhEcp/interested-in-ea-longtermist-research-careers-here-are-my},
	shorttitle = {Interested in {EA}/longtermist research careers?},
	journaltitle = {Effective Altruism Forum},
	author = {Aird, Michael},
	urldate = {2022-06-27},
	date = {2022-06-26},
	langid = {english},
	file = {~/Google Drive/library-pdf/Aird2022InterestedEALongtermist.pdf;~/Google Drive/library-html/interested-in-ea-longtermist-research-careers-here-are-my.html}
}

@online{Aird2023CuestionesCrucialesPara,
	date = {2023},
	title = {Cuestiones cruciales para los largoplacistas},
	database = {Tlön},
	author = {Aird, Michael},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Aird2020CrucialQuestionsLongtermists}
}

@online{Alexander2010ConfidenceLevelsOutside,
	database = {Tlön},
	title = {Confidence levels inside and outside an argument},
	abstract = {In circumstances where one has limited information about an event, such as an election, and relies solely on a model or argument to estimate the probability of that event, it is important to differentiate between the internal and external levels of confidence. The internal confidence level is the probability assigned by the specific model or argument, while the external confidence level should be lower due to potential flaws or limitations in the model or argument. This is because even if the model or argument provides a very high internal confidence level, there is always a chance that it is mistaken. The article emphasizes the need to consider the external confidence level, especially when evaluating arguments that yield extremely high internal confidence levels. – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/GrtbTAPfkJa4D6jjH/confidence-levels-inside-and-outside-an-argument},
	journaltitle = {{LessWrong}},
	author = {Alexander, Scott},
	date = {2010-12-16},
	file = {~/Google Drive/library-pdf/Alexander2010ConfidenceLevelsOutside.pdf}
}

@online{Alexander2013YouReProbably,
	database = {Tlön},
	title = {You’re probably wondering why I’ve called you here
                  today},
	abstract = {Due to an oversight by the ancient Greeks, there is no Muse of blogging. Denied the ability to begin with a proper Invocation To The Muse, I will compensate with some relatively boring introduction…},
	url = {https://slatestarcodex.com/2013/02/12/youre-probably-wondering-why-ive-called-you-here-today/},
	journaltitle = {Slate Star Codex},
	author = {Alexander, Scott},
	urldate = {2021-12-09},
	date = {2013-02-12},
	langid = {english},
	file = {~/Google Drive/library-pdf/Alexander2013YouReProbably.pdf;~/Google Drive/library-html/youre-probably-wondering-why-ive-called-you-here-today.html}
}

@online{Alexander2014FiveYearsOne,
	database = {Tlön},
	title = {Five years and one week of Less Wrong},
	abstract = {Last week was the fifth birthday of Less Wrong. I thought I remembered it was started March 11, but it seems to have been more like March 5. I was going to use that time to talk about it, but now I will just have to talk about it awkwardly five years and one week after it was started.},
	langid = {english},
	url = {https://slatestarcodex.com/2014/03/13/five-years-and-one-week-of-less-wrong/},
	journaltitle = {Slate star codex},
	author = {Alexander, Scott},
	date = {2014-03-13},
	file = {~/Google Drive/library-pdf/Alexander2014FiveYearsOne.pdf}
}

@online{Alexander2014NobodyIsPerfect,
	database = {Tlön},
	file = {~/Google Drive/library-html/Alexander2014NobodyIsPerfect.html;~/Google Drive/library-pdf/Alexander2014NobodyIsPerfect.pdf},
	abstract = {I. Recently spotted on Tumblr: “This is going to be an unpopular opinion but I see stuff about ppl not wanting to reblog ferguson things and awareness around the world because they do not want negativity in their life plus it will cause them to have anxiety. They come to tumblr to escape n feel happy which think is a load of bull. There r literally ppl dying who live with the fear of going outside their homes to be shot and u cant post a fucking picture because it makes u a little upset?” “Can yall maybe take some time away from reblogging fandom or humor crap and read up and reblog pakistan because the privilege you have of a safe bubble is not one shared by others?”},
	author = {Alexander, Scott},
	date = {2014-12-19},
	langid = {english},
	timestamp = {2023-02-17 22:10:05 (GMT)},
	title = {Nobody Is Perfect, Everything Is Commensurable},
	journaltitle = {Slate Star Codex},
	url = {https://slatestarcodex.com/2014/12/19/nobody-is-perfect-everything-is-commensurable/},
	urldate = {2023-02-17}
}

@online{Alexander2015BewareSystemicChange,
	database = {Tlön},
	title = {Beware systemic change},
	abstract = {One of the most common critiques of effective altruism is that it focuses too much on specific monetary interventions rather than fighting for “systemic change”, usually billed as fighting inequitable laws or capitalism in general.},
	langid = {english},
	url = {https://slatestarcodex.com/2015/09/22/beware-systemic-change/},
	journaltitle = {Slate star codex},
	author = {Alexander, Scott},
	date = {2015-09-22},
	file = {~/Google Drive/library-pdf/Alexander2015BewareSystemicChange.pdf}
}

@online{Alexander2015EthicsOffsets,
	database = {Tlön},
	title = {Ethics offsets},
	abstract = {Some people buy voluntary carbon offsets. Suppose they worry about global warming and would feel bad taking a long unnecessary plane trip that pollutes the atmosphere. So instead of not doing it, they take the plane trip, then pay for some environmental organization to clean up an amount of carbon equal to or greater than the amount of carbon they emitted. They’re happy because they got their trip, future generations are happy because the atmosphere is cleaner, everyone wins.},
	langid = {english},
	url = {https://slatestarcodex.com/2015/01/04/ethics-offsets/},
	journaltitle = {Slate Star Codex},
	author = {Alexander, Scott},
	date = {2015-01-04}
}

@online{Alexander2015ThingsThatSometimes,
	database = {Tlön},
	title = {Things that sometimes work if you have anxiety},
	abstract = {Anxiety disorders are the most common class of psychiatric disorders. Their US prevalence is about 20\%. They’re also among the least recognized and least treated. We have sort of finally beaten into people’s thick skulls that depression isn’t just being sad, and you can’t just turn your frown upside down or something – but the most common response to anxiety disorders is still “Anxiety? So what, everyone gets that sometimes.”},
	langid = {english},
	url = {https://slatestarcodex.com/2015/07/13/things-that-sometimes-work-if-you-have-anxiety/},
	journaltitle = {Slate Star Codex},
	author = {Alexander, Scott},
	date = {2015-07-13},
	file = {~/Google Drive/library-pdf/Alexander2015ThingsThatSometimes.pdf}
}

@online{Alexander2016SuperintelligenceFAQ,
	database = {Tlön},
	title = {Superintelligence {FAQ}},
	langid = {english},
	url = {https://www.lesswrong.com/posts/LTtNXM9shNM9AC2mp/superintelligence-faq},
	journaltitle = {{LessWrong}},
	author = {Alexander, Scott},
	date = {2016-09-20}
}

@online{Alexander2018BookReviewHistory,
	database = {Tlön},
	title = {Book review: History of the Fabian Society},
	abstract = {A spectre is haunting Europe. Several spectres, actually. One of them is the spectre of communism. The others are literal ghosts. They live in abandoned mansions. Sometimes they wail eerily or make floorboards creak. If you arrange things just right, you might be able to capture them on film.},
	langid = {english},
	url = {https://slatestarcodex.com/2018/04/30/book-review-history-of-the-fabian-society/},
	journaltitle = {Slate Star Codex},
	author = {Alexander, Scott},
	date = {2018-04-30},
	file = {~/Google Drive/library-pdf/Alexander2018BookReviewHistory.pdf}
}

@online{Alexander2018HighlightsCommentsConflict,
	database = {Tlön},
	title = {Highlights from the comments on conflict vs. mistake},
	abstract = {Thanks to everyone who commented on the posts about conflict and mistake theory. aciddc writes: I’m a leftist (and I guess a Marxist in the same sense I guess I’m a Darwinist despite knowing evolutionary theory has passed him by) fan of this blog. I’ve thought about this “conflict theory vs. mistake theory” dichotomy a lot, though I’ve been thinking of it as what distinguishes “leftists” from “liberals.”….},
	langid = {english},
	url = {https://slatestarcodex.com/2018/01/29/highlights-from-the-comments-on-conflict-vs-mistake/},
	journaltitle = {Slate Star Codex},
	author = {Alexander, Scott},
	date = {2018-01-29}
}

@online{Alexander20191960YearSingularity,
	database = {Tlön},
	title = {1960: the year the singularity was cancelled},
	abstract = {[Epistemic status: Very speculative, especially Parts 3 and 4. Like many good things, this post is based on a conversation with Paul Christiano; most of the good ideas are his, any errors are mine…},
	langid = {english},
	url = {https://slatestarcodex.com/2019/04/22/1960-the-year-the-singularity-was-cancelled/},
	journaltitle = {Slate star codex},
	author = {Alexander, Scott},
	date = {2019-04-22},
	file = {~/Google Drive/library-pdf/Alexander20191960YearSingularity.pdf}
}

@online{Alexander2019ReviewEricDrexler,
	database = {Tlön},
	title = {Review of Eric Drexler, *Reframing Superintelligence:
                  Comprehensive {AI} Services as General Intelligence*},
	abstract = {Ten years ago, everyone was talking about superintelligence, the singularity, the robot apocalypse. What happened? I think the main answer is: the field matured. Why isn’t everyone talking about nuclear security, biodefense, or counterterrorism? Because there are already competent institutions working on those problems, and people who are worried about them don’t feel the need to take their case directly to the public. The past ten years have seen AI goal alignment reach that level of maturity too. There are all sorts of new research labs, think tanks, and companies working on it – the Center For Human-Compatible AI at UC Berkeley, OpenAI, Ought, the Center For The Governance Of AI at Oxford, the Leverhulme Center For The Future Of Intelligence at Cambridge, etc. Like every field, it could still use more funding and talent. But it’s at a point where academic respectability trades off against public awareness at a rate where webzine articles saying CARE ABOUT THIS OR YOU WILL DEFINITELY DIE are less helpful…},
	langid = {english},
	url = {https://slatestarcodex.com/2019/08/27/book-review-reframing-superintelligence/},
	journaltitle = {Slate star codex},
	author = {Alexander, Scott},
	date = {2019-08-27}
}

@online{Alexander2020FailureNotPrediction,
	database = {Tlön},
	title = {A failure, but not of prediction},
	abstract = {Vox asks What Went Wrong With The Media’s Coronavirus Coverage? They conclude that the media needs to be better at “not just saying what we do know, but what we don’t know”…},
	url = {https://slatestarcodex.com/2020/04/14/a-failure-but-not-of-prediction/},
	journaltitle = {Slate Star Codex},
	author = {Alexander, Scott},
	urldate = {2022-05-22},
	date = {2020-04-15},
	langid = {english},
	file = {~/Google Drive/library-pdf/Alexander2020FailureNotPrediction.pdf;~/Google Drive/library-html/a-failure-but-not-of-prediction.html}
}

@online{Alexander2021ACXGrantsResults,
	database = {Tlön},
	title = {{ACX} Grants results},
	abstract = {...},
	langid = {english},
	url = {https://astralcodexten.substack.com/p/acx-grants-results},
	journaltitle = {Astral Codex Ten},
	type = {Substack newsletter},
	author = {Alexander, Scott},
	urldate = {2021-12-29},
	date = {2021-12-28},
	file = {~/Google Drive/library-pdf/Alexander2021ACXGrantsResults.pdf;~/Google Drive/library-html/acx-grants-results.html}
}

@online{Alexander2021BookReviewScout,
	database = {Tlön},
	title = {Book Review: The Scout Mindset},
	abstract = {...},
	langid = {english},
	url = {https://astralcodexten.substack.com/p/book-review-the-scout-mindset},
	shorttitle = {Book Review},
	journaltitle = {Astral Codex Ten},
	author = {Alexander, Scott},
	urldate = {2021-09-29},
	date = {2021-09-29},
	file = {~/Google Drive/library-pdf/Alexander2021BookReviewScout.pdf;~/Google Drive/library-html/book-review-the-scout-mindset.html}
}

@online{Alexander2021WhatAstralCodex,
	database = {Tlön},
	title = {What is Astral Codex Ten?},
	abstract = {P(A{\textbar}B) = [P(A)*P(B{\textbar}A)]/P(B), all the rest is commentary. Click to read Astral Codex Ten, by Scott Alexander, a Substack publication.},
	langid = {english},
	url = {https://astralcodexten.substack.com/about},
	journaltitle = {Astral Codex Ten},
	author = {Alexander, Scott},
	date = 2021,
	file = {~/Google Drive/library-pdf/Alexander2021WhatAstralCodex.pdf}
}

@online{Alexander2022BookReviewWhat,
	database = {Tlön},
	title = {Book Review: /What We Owe the Future/},
	abstract = {...},
	langid = {english},
	url = {https://astralcodexten.substack.com/p/book-review-what-we-owe-the-future},
	shorttitle = {Book Review},
	journaltitle = {Astral Codex Ten},
	type = {Substack newsletter},
	author = {Alexander, Scott},
	urldate = {2022-08-23},
	date = {2022-08-23},
	file = {~/Google Drive/library-html/book-review-what-we-owe-the-future.html}
}

@online{Alexander2022LongtermismVsExistential,
	database = {Tlön},
	title = {"Long-termism" vs. "existential risk"},
	abstract = {The phrase "long-termism" is occupying an increasing share of {EA} community "branding". For example, the Long-Term Future Fund, the {FTX} Future Fund ("we support ambitious projects to improve humanity's long-term prospects"), and the impending launch of What We Owe The Future ("making the case for long-termism").},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/KDjEogAqWNTdddF9g/long-termism-vs-existential-risk},
	journaltitle = {Effective Altruism Forum},
	author = {Alexander, Scott},
	urldate = {2022-04-11},
	date = {2022-04-06},
	file = {~/Google Drive/library-pdf/Alexander2022LongtermismVsExistential.pdf;~/Google Drive/library-html/long-termism-vs-existential-risk.html}
}

@online{Alexander2022WhatCaused2020,
	database = {Tlön},
	title = {What caused the 2020 homicide spike?},
	abstract = {...},
	langid = {english},
	url = {https://astralcodexten.substack.com/p/what-caused-the-2020-homicide-spike},
	journaltitle = {Astral Codex Ten},
	type = {Substack newsletter},
	author = {Alexander, Scott},
	urldate = {2022-06-30},
	date = {2022-06-28},
	file = {~/Google Drive/library-pdf/Alexander2022WhatCaused2020.pdf}
}

@online{Alexander2022YudkowskyContraChristiano,
	database = {Tlön},
	title = {Yudkowsky contra Christiano on {AI} takeoff speeds},
	abstract = {...},
	langid = {english},
	url = {https://astralcodexten.substack.com/p/yudkowsky-contra-christiano-on-ai},
	journaltitle = {Astral Codex Ten},
	type = {Substack newsletter},
	author = {Alexander, Scott},
	urldate = {2022-04-30},
	date = {2022-04-04},
	file = {~/Google Drive/library-pdf/Alexander2022YudkowskyContraChristiano.pdf;~/Google Drive/library-html/yudkowsky-contra-christiano-on-ai.html}
}

@online{Alexander2023CompensacionesEticas,
	database = {Tlön},
	keywords = {compensación moral},
	date = {2023},
	langid = {spanish},
	author = {Alexander, Scott},
	title = {Compensaciones éticas},
	translator = {Humarán, Aurora},
	translation = {Alexander2015EthicsOffsets}
}

@online{Alexander2023NadieEsPerfecto,
	database = {Tlön},
	date = {2023},
	title = {Nadie es perfecto, todo es conmensurable},
	author = {Alexander, Scott},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Alexander2014NobodyIsPerfect}
}

@online{Alexanian2021BiosecurityBioriskReading,
	database = {Tlön},
	title = {A biosecurity and biorisk reading+ list},
	abstract = {Here are some readings (+courses, videos, and podcasts) to help you get oriented in biosecurity and biorisk reduction.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/iAowzcZm87wNrTQCb/a-biosecurity-and-biorisk-reading-list},
	journaltitle = {Effective Altruism Forum},
	author = {Alexanian, Tessa},
	urldate = {2022-02-03},
	date = {2021-03-13},
	eventdate = {2021-03-16},
	file = {~/Google Drive/library-pdf/Alexanian2021BiosecurityBioriskReading.pdf;~/Google Drive/library-html/a-biosecurity-and-biorisk-reading-list.html}
}

@online{Allen1995AnimalConsciousness,
	database = {Tlön},
	title = {Animal consciousness},
	abstract = {Questions about animal consciousness — in particular, whichanimals have consciousness and what (if anything) that consciousnessmight be like — are  both  scientific andphilosophical. They are scientific because answering them will requiregathering information using scientific techniques — no amount ofarm-chair pondering, conceptual analysis, logic, a prioritheory-building, transcendental inference or introspection will tellus whether a platypus, an iguana, or a squid (to take a few examples)enjoy a life of subjective experience — at some point we’ll haveto learn something about the animals. Just what sort(s) of science canbear on these questions is a live question, but at the least this willinclude investigations of the behavior and neurophysiology of a widetaxonomic range of animals, as well as the phylogenetic relationshipsamong taxa. But these questions are deeply philosophical as well, withepistemological, metaphysical, and phenomenologicaldimensions. Progress will therefore ultimately requireinterdisciplinary work by philosophers willing to engage with theempirical details of animal biology, as well as scientists who aresensitive to the philosophical complexities of the issue.},
	langid = {english},
	url = {https://plato.stanford.edu/entries/consciousness-animal},
	journaltitle = {Stanford Encyclopedia of Philosophy},
	author = {Allen, Colin and Trestman, Michael},
	date = {1995-12-23},
	eventdate = {2016-10-24},
	file = {~/Google Drive/library-pdf/Allen2016AnimalConsciousness.pdf}
}

@incollection{Alston1967Pleasure,
	database = {Tlön},
	location = {New York},
	abstract = {The first English-language reference of its kind, The Encyclopedia of Philosophy was hailed as 'a remarkable and unique work' (Saturday Review) that contained 'the international who's who of philosophy and cultural history' (Library Journal).},
	langid = {english},
	title = {Pleasure},
	volume = 6,
	isbn = {0-02-864651-7},
	pages = {341–347},
	booktitle = {The Encyclopedia of Philosophy},
	publisher = {Macmillan},
	author = {Alston, William},
	editor = {Edwards, Paul},
	date = 1967
}

@online{Althaus2020ReducingLongtermRisks,
	database = {Tlön},
	title = {Reducing long-term risks from malevolent actors},
	abstract = {Dictators who exhibited highly narcissistic, psychopathic, or sadistic traits were involved in some of the greatest catastrophes in human history. We argue that further work on reducing malevolence would be valuable from many moral perspectives and constitutes a promising focus area for longtermist {EAs}.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/LpkXtFXdsRd4rG8Kb/reducing-long-term-risks-from-malevolent-actors},
	journaltitle = {Effective Altruism Forum},
	author = {Althaus, David and Baumann, Tobias},
	date = {2020-04-29},
	file = {~/Google Drive/library-pdf/Althaus2020ReducingLongtermRisks.pdf}
}

@online{AltruismoEficaz2023IntroduccionAlAltruismo,
	database = {Tlön},
	keywords = {altruismo eficaz},
	date = {2023},
	langid = {spanish},
	author = {{Altruismo Eficaz}},
	title = {Introducción al altruismo eficaz},
	translator = {Rodríguez, Cristina and Corpas, Javier},
	translation = {EffectiveAltruism2016IntroductionToEffective}
}

@online{Anderljung2022AnnouncingGovAIPolicy,
	database = {Tlön},
	title = {Announcing the {GovAI} Policy Team},
	abstract = {The {AI} governance space needs more rigorous work on what influential actors (e.g. governments and {AI} labs) should do in the next few years to prepare the world for advanced {AI}.
We’re setting up a Policy Team at the Centre for the Governance of {AI} ({GovAI}) to help address this gap. The team will primarily focus on {AI} policy development from a long-run perspective. It will also spend some time on advising and advocating for recommendations, though we expect to lean heavily on other actors for that. Our work will be most relevant for the governments of the {US}, {UK}, and {EU}, as well as {AI} labs.},
	url = {https://forum.effectivealtruism.org/posts/jatnoouJuCpcKpnoh/announcing-the-govai-policy-team},
	journaltitle = {Effective Altruism Forum},
	author = {Anderljung, Markus},
	urldate = {2022-08-02},
	date = {2022-08-01},
	langid = {english},
	file = {~/Google Drive/library-pdf/Anderljung2022AnnouncingGovAIPolicy.pdf;~/Google Drive/library-html/announcing-the-govai-policy-team.html}
}

@online{Andreev2018ArbitalPostmortem,
	database = {Tlön},
	title = {Arbital postmortem},
	abstract = {The Arbital project proposed to build a website that integrated functions of a blog, wiki, and social media platform, including claims voting, discussion boards, and a karma system. The key innovation was its integration of modular explanations into a knowledge base that would enable users to understand complex topics in a progressive manner. The project faced many challenges, including difficulty finding users, prioritizing features, and balancing the roles of the co-founder and advisor (Eliezer Yudkowsky), who had strongly divergent visions for the project. Ultimately, the project was terminated after burning through its \$300,000 seed funding. – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/kAgJJa3HLSZxsuSrf/arbital-postmortem},
	journaltitle = {{LessWrong}},
	author = {Andreev, Alexei},
	date = {2018-01-30},
	file = {~/Google Drive/library-pdf/Andreev2018ArbitalPostmortem.pdf}
}

@online{Anello2022WhoProtectingAnimals,
	database = {Tlön},
	title = {Who is protecting animals in the long-term future?},
	abstract = {Dear Amazing {EA} Forum Users,.
Thank you for dedicating yourselves to doing good.
Today I read an article in Watt Poultry that stoked an old fear. It's a fear that comes up for me—and I think for some other animal-focused {EAs}—when thinking about the longterm future. My fear is: Will the longterm future mean expanded factory farming, which goes on forever? (And therefore produces exponentially more suffering than factory farming currently produces?) Who is looking out for animals in the longterm future?.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/JSEencwkDuwWzYny8/who-is-protecting-animals-in-the-long-term-future},
	shorttitle = {Who is protecting animals in the long-term future?},
	journaltitle = {Effective Altruism Forum},
	author = {Anello, Alene},
	urldate = {2022-04-11},
	date = {2022-04-09},
	file = {~/Google Drive/library-pdf/Anello2022WhoProtectingAnimals.pdf;~/Google Drive/library-html/who-is-protecting-animals-in-the-long-term-future.html}
}

@online{AnimalAsk2020IntroducingAnimalAsk,
	database = {Tlön},
	title = {Introducing Animal Ask},
	abstract = {Animal Ask is a new research organisation conceived through the 2020 Charity Entrepreneurship Incubation Programme. We aim to optimise and prioritise future asks to assist animal advocacy organisations in their efforts to reduce farmed animal suffering.
Billions of animals suffer on factory farms every year. The animal advocacy movement has so far done great work to help alleviate animal suffering, building up expertise in both public and corporate campaigns and spending a significant amount of resources on these methods.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/rq6rh7BfzGzKkb7MM/introducing-animal-ask},
	journaltitle = {Effective Altruism Forum},
	author = {{Animal Ask}},
	urldate = {2022-03-14},
	date = {2020-11-09},
	file = {~/Google Drive/library-pdf/AnimalAsk2020IntroducingAnimalAsk.pdf}
}

@online{AnimalCharityEvaluators2020HumaneLeagueReview,
	database = {Tlön},
	title = {The Humane League review},
	abstract = {Read our 2023 review of The Humane League. Support this {ACE}'s Recommended Charity to help the greatest number of animals!.},
	langid = {english},
	url = {https://animalcharityevaluators.org/charity-review/the-humane-league/},
	journaltitle = {Animal Charity Evaluators},
	author = {{Animal Charity Evaluators}},
	date = {2020-11},
	file = {~/Google Drive/library-pdf/AnimalCharityEvaluators2020HumaneLeagueReview.pdf}
}

@online{AnimalEthics2023InsensibilidadAlAlcance,
	database = {Tlön},
	keywords = {insensibilidad al alcance, sesgo cognitivo, bienestar
                  animal},
	date = {2023},
	langid = {spanish},
	author = {{Animal Ethics}},
	title = {Insensibilidad al alcance: no apreciar el número de
                  quienes necesitan nuestra ayuda},
	translator = {{Animal Ethics}},
	translation = {AnimalEthics2020ScopeInsensitivityFailing}
}

@book{Anonymous1673GrandConcernEngland,
	database = {Tlön},
	location = {London},
	langid = {english},
	title = {The grand concern of England explained; in several
                  proposals offered to the consideration of the
                  parliament},
	url = {https://www.worldcat.org/oclc/1086430282},
	author = {{Anonymous}},
	date = 1673
}

@report{Aos2004BenefitsCostsPreventiona,
	database = {Tlön},
	title = {Benefits and costs of prevention programs for youth:
                  technical appendix},
	langid = {english},
	url = {https://www.wsipp.wa.gov/ReportFile/882},
	institution = {Washington State Institute for Public Policy},
	author = {Aos, Steve and Lieb, Roxanne and Mayfield, Jim and
                  Miller, Marna and Pennucci, Annie},
	date = 2004,
	number = {document no. 04-07-3901},
	file = {~/Google Drive/library-pdf/Aos2004BenefitsCostsPreventiona.pdf}
}

@book{Aquino1988SumaDeTeologia,
	author = {de Aquino, Tomás},
	title = {Suma de teología},
	langid = {spanish},
	database = {Tlön},
	publisher = {Biblioteca de Autores Cristianos},
	date = {1988},
	isbn = {9788479142773},
	location = {Madrid},
	note = {{OCLC}: 776498799},
	timestamp = {2023-06-23 16:33:35 (GMT)}
}

@online{Arbital2018ShiftTowardsHypothesis,
	database = {Tlön},
	url = {https://arbital.com/p/flee_from_surprise/},
	abstract = {When you see new evidence, ask: which hypothesis is *least surprised?*.},
	date = {2015},
	langid = {english},
	journaltitle = {Arbital},
	title = {Shift towards the hypothesis of least surprise},
	author = {Arbital},
	timestamp = {2023-08-31 12:13:47 (GMT)}
}

@online{Arbital2023PerspectivaBayesianaDe,
	database = {Tlön},
	translator = {Tlön},
	langid = {spanish},
	translation = {Arbital2021BayesianViewOf},
	date = {2023},
	journaltitle = {Biblioteca Altruismo Eficaz},
	title = {Perspectiva bayesiana de las virtudes científicas},
	author = {Arbital},
	timestamp = {2023-08-30 17:32:35 (GMT)}
}

@online{Armstrong2015HedoniumSemanticProblem,
	database = {Tlön},
	title = {Hedonium's semantic problem},
	abstract = {The problem of understanding semantics from a purely syntactic process is examined. The author argues that only processes with symbols corresponding to objects in reality can be truly understood, since processes with symbols referring to nothing or to things that don't exist in reality lack certain features that processes with symbols corresponding to real objects possess. This means that simply copying descriptions of happy states to create greater amounts of happiness throughout the universe might not result in an increase in genuine happiness, and creating simulations of humans in bad situations could create genuine suffering – a problem called 'mind crimes'. – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/295KiqZKAb55YLBzF/hedonium-s-semantic-problem},
	journaltitle = {{LessWrong}},
	author = {Armstrong, Stuart},
	date = {2015-04-09},
	file = {~/Google Drive/library-pdf/Armstrong2015HedoniumSemanticProblem.pdf}
}

@online{Armstrong2022WhyCofoundingAligned,
	database = {Tlön},
	title = {Why I'm co-founding Aligned {AI}},
	abstract = {This article addresses several philosophical and ethical issues raised by the potential development of the metaverse, a virtual shared space inhabited by various users. It explores both the potential benefits and risks of the metaverse by discussing questions about consequences for wrongdoing, governance, and morality surrounding the interactions between users. The authors propose the metaverse as opening up exciting new possibilities for human connection and experience, but also highlight the urgent need for critical engagement with the associated ethical dilemmas in order to shape its development responsibly. – AI-generated abstract.},
	langid = {english},
	url = {https://www.alignmentforum.org/posts/vBoq5yd7qbYoGKCZK/why-i-m-co-founding-aligned-ai},
	journaltitle = {{AI} Alignment Forum},
	author = {Armstrong, Stuart},
	urldate = {2022-02-22},
	date = {2022-02-17},
	file = {~/Google Drive/library-pdf/Armstrong2022WhyCofoundingAligned.pdf;~/Google Drive/library-html/why-i-m-co-founding-aligned-ai.html}
}

@online{ArnoldRaymond2012HighImpactNetwork,
	database = {Tlön},
	title = {The High Impact Network ({THINK}) - Launching now},
	abstract = {This article introduces THINK, an organization devoted to promoting effective altruism. The organization's mission is to help individuals and groups optimize their efforts to have the greatest positive impact on the world, considering efficiency, evidence, and rationality. The article emphasizes that THINK is not bound by a particular ethical framework or cause but rather provides a process and a commitment to intellectual rigor. It aims to establish meetups and create educational materials to help newcomers learn about effective altruism and develop the skills to tackle complex problems. Additionally, THINK encourages collaboration on high-impact projects to address some of the world's most pressing challenges. – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/KEcWJSzxFzcYvyLsW/the-high-impact-network-think-launching-now},
	journaltitle = {{LessWrong}},
	author = {Arnold, Raymond},
	urldate = {2022-01-01},
	date = {2012-08-07},
	file = {~/Google Drive/library-pdf/ArnoldRaymond2012HighImpactNetwork.pdf;~/Google Drive/library-html/the-high-impact-network-think-launching-now.html}
}

@book{Aron1965DemocratieTotalitarisme,
	database = {Tlön},
	location = {Paris},
	langid = {french},
	title = {Démocratie et totalitarisme},
	publisher = {Gallimard},
	author = {Aron, Raymond},
	date = 1965,
	file = {~/Google Drive/library-pdf/Aron1965DemocratieTotalitarisme.pdf}
}

@book{Asimov1993AmenazasDeNuestro,
	translator = {Solanas, Montserrat},
	database = {Tlön},
	langid = {spanish},
	author = {Asimov, Isaac},
	title = {Las amenazas de nuestro mundo},
	publisher = {Plaza \& Janés},
	date = {1980},
	edition = {1},
	isbn = {9788401450976},
	location = {Barcelona},
	note = {{OCLC}: 45503308},
	translation = {Asimov1979ChoiceCatastrophes},
	timestamp = {2023-06-24 21:55:32 (GMT)}
}

@online{Askell2023GarantizarSeguridadDe,
	database = {Tlön},
	date = {2023},
	title = {Garantizar la seguridad de la inteligencia artificial},
	author = {Askell, Amanda},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Askell2021EnsuringSafetyArtificial}
}

@online{Askell2023ValorMoralDe,
	database = {Tlön},
	date = {2023},
	title = {El valor moral de la información},
	author = {Askell, Amanda},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Askell2017MoralValueInformation}
}

@online{Azhar2022MeaningLifeMetaverse,
	database = {Tlön},
	title = {The meaning of life in the metaverse (with David
                  Chalmers)},
	abstract = {This article reflects on the practice of crypto firms hiring high-profile figures from traditional finance and government, often with the sole purpose of boosting their reputation and attracting investors. The author argues that this practice is misleading and undermines the credibility of the crypto industry. Rather than relying on such gimmicks, companies should focus on hiring talented and dedicated individuals who genuinely contribute to their success. The article highlights the stories of several FTX employees who have made significant contributions to the company's growth, emphasizing their skills and dedication rather than their past affiliations. – AI-generated abstract.},
	url = {https://www.listennotes.com/podcasts/azeem-azhars/the-meaning-of-life-in-the-MpjwEbt5TzU/},
	journaltitle = {Azeem Azhar's Exponential View},
	author = {Azhar, Azeem},
	urldate = {2022-03-31},
	date = {2022-03-23},
	langid = {english},
	file = {~/Google Drive/library-html/the-meaning-of-life-in-the-MpjwEbt5TzU.html}
}

@online{Bailey2009NormanBorlaugMan,
	database = {Tlön},
	title = {Norman Borlaug: the man who saved more human lives
                  than any other has died},
	abstract = {Norman Borlaug, the man who saved more human lives than anyone else in history, has died at age 95. Borlaug was the Father of the Green Revolution, the dramatic improvement in agricultural productivity that swept the globe in the 1960s. For spearheading this achievement, he was awarded the Nobel Peace Prize in 1970. One of the great privileges of my life was meeting and talking with Borlaug many times over the past few years.},
	url = {https://reason.com/2009/09/13/norman-borlaug-the-man-who-sav/},
	shorttitle = {Norman Borlaug},
	journaltitle = {Reason.com},
	author = {Bailey, Ronald},
	urldate = {2022-05-09},
	date = {2009-09-13},
	langid = {english},
	note = {Section: Science/Tech},
	file = {~/Google Drive/library-pdf/Bailey2009NormanBorlaugMan.pdf;~/Google Drive/library-html/norman-borlaug-the-man-who-sav.html}
}

@Report{BancoMundial2016PobrezaProsperidadCompartida,
	url = {https://openknowledge.worldbank.org/server/api/core/bitstreams/972eca5c-861c-5e4c-9962-717f4cd22e58/content},
	date = {2016},
	langid = {spanish},
	database = {Tlön},
	institution = {Banco Internacional de Reconstrucción y Fomento/Banco Mundial},
	title = {La pobreza y la prosperidad compartida 2022: Corregir el rumbo},
	author = {{Banco Mundial}},
	timestamp = {2023-07-20 09:32:46 (GMT)}
}

@online{Bankman-Fried2020RaisingBar,
	database = {Tlön},
	title = {Raising the bar},
	abstract = {In this piece, the author, Sam Bankman-Fried, discusses the practice of hiring accomplished individuals for the sake of positive media attention, rather than their actual contributions to the company. The author contends that news outlets often present these hires as meaningful, which leads people to make investment decisions based on misleading information. The author contrasts this with FTX's hiring process, which focuses on hiring people who will genuinely contribute to the company. The author then provides examples of individuals who have made significant contributions to FTX's success, highlighting their specific achievements and qualities. – AI-generated abstract.},
	url = {https://blog.ftx.com/blog/raising-the-bar/},
	shorttitle = {Raising the bar},
	journaltitle = {{FTX} Research},
	author = {Bankman-Fried, Sam},
	urldate = {2021-12-30},
	date = {2020-05-17},
	langid = {english},
	file = {~/Google Drive/library-pdf/Bankman-Fried2020RaisingBar.pdf;~/Google Drive/library-html/raising-the-bar.html}
}

@online{Bankman-Fried2022SamBankmanFried,
	database = {Tlön},
	title = {Sam Bankman-Fried},
	abstract = {Those who join the Giving Pledge often write a letter explaining their decision to make this public commitment, their reasons for engaging in philanthropy, and the causes that motivate them. We invite you to explore these letters.},
	url = {https://givingpledge.org/pledger?pledgerId=445},
	journaltitle = {Giving Pledge},
	author = {Bankman-Fried, Sam},
	urldate = {2022-06-02},
	date = {2022-06-01},
	langid = {english},
	file = {~/Google Drive/library-pdf/Bankman-Fried2022SamBankmanFried.pdf;~/Google Drive/library-html/pledger.html}
}

@online{Barnett2017ImpactNowRethink,
	database = {Tlön},
	title = {.Impact is now rethink charity},
	abstract = {I’m Tee Barnett – and in the last couple of months I’ve transitioned into the Executive Director role at .impact. I wanted to reintroduce myself and brief everyone on significant developments over here, including our rebranding, unification of major projects, and staff updates. You can reach me here if you’d like to chat about it!.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/a3sScjxdgNTCseBMJ/impact-is-now-rethink-charity},
	journaltitle = {Effective Altruism Forum},
	author = {Barnett, Tee},
	date = {2017-05-30},
	file = {~/Google Drive/library-pdf/Barnett2017ImpactNowRethink.pdf}
}

@online{Barnett2022MostImportantCentury,
	database = {Tlön},
	title = {The most important century: the animation},
	abstract = {This is a linkpost for the Rational Animations' video based on The Most Important Century sequence of posts by Holden Karnofsky.Below, the whole script of the video.Matthew Barnett has written most of it. Several paragraphs were written by Ajeya Cotra, during the feedback process. Holden Karnofsky has also reviewed the script and made suggestions. I made some light edits and additions.Matthew has also made some additions that weren't in the original sequence by Holden.Crossposted to {LessWrong}.},
	url = {https://forum.effectivealtruism.org/posts/p3sNDLotTfAktSD69/the-most-important-century-the-animation},
	shorttitle = {The Most Important Century},
	journaltitle = {Effective Altruism Forum},
	author = {Barnett, Matthew},
	urldate = {2022-07-27},
	date = {2022-07-24},
	langid = {english},
	file = {~/Google Drive/library-pdf/Barnett2022MostImportantCentury.pdf;~/Google Drive/library-html/the-most-important-century-the-animation.html}
}

@online{Base2022ThingsUsuallyEnd,
	database = {Tlön},
	title = {Things usually end slowly},
	abstract = {It’s sometimes claimed or suggested that if an extinction event were to happen, it would happen very quickly (i.e. days, weeks, months). However, when empires fall or species go extinct, they usually do so very slowly (years, decades, millennia). These base rates  suggest we should have a reasonably strong prior on slow decline when thinking about civilisational collapse and, at more of a stretch, existential risks. This sketch is very rough, and I’d love to see someone work out these base rates, and how they might be changing, more carefully.},
	url = {https://forum.effectivealtruism.org/posts/qLwtCuh6nDCsrsrMK/things-usually-end-slowly},
	journaltitle = {Effective Altruism Forum},
	author = {Base, Ollie},
	urldate = {2022-06-08},
	date = {2022-06-07},
	langid = {english},
	file = {~/Google Drive/library-pdf/Base2022ThingsUsuallyEnd.pdf;~/Google Drive/library-html/things-usually-end-slowly.html}
}

@report{Baum2018ModelImpactsNuclear,
	database = {Tlön},
	title = {A model for the impacts of nuclear war},
	langid = {english},
	url = {https://gcrinstitute.org/papers/043_nuclear-impacts.pdf},
	institution = {Global Catastrophic Risk Institute},
	author = {Baum, Seth D. and Barrett, Anthony M.},
	date = {2018-04-03},
	number = {working paper 18-2}
}

@online{Baum2022EarlyReflectionsResources,
	database = {Tlön},
	title = {Early reflections and resources on the Russian
                  invasion of Ukraine},
	abstract = {The opening days of the Russian invasion of Ukraine brought a flurry of dramatic changes to the world. Now, three weeks into the war, conditions on the ground have somewhat stabilized. Further major changes remain very much possible, but meanwhile, there is now an opportunity to reflect on what has happened and what that means for the world moving forward. This post provides some reflections oriented mainly, but not exclusively, for a global catastrophic risk audience, with some emphasis on nuclear weapons.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/dd3cwxk65sH5bHJBR/early-reflections-and-resources-on-the-russian-invasion-of},
	journaltitle = {Effective Altruism Forum},
	author = {Baum, Seth D.},
	urldate = {2022-03-18},
	date = 2022,
	file = {~/Google Drive/library-pdf/Baum2022EarlyReflectionsResources.pdf;~/Google Drive/library-html/early-reflections-and-resources-on-the-russian-invasion-of.html}
}

@online{Baumann2017ArgumentsMoralAdvocacy,
	database = {Tlön},
	title = {Arguments for and against moral advocacy},
	abstract = {Spreading beneficial values is a promising intervention for effective altruists. We may hope that our advocacy has a lasting influence on our society (or its successor), thus having an indirect impact on the far future. Alternatively, we may view moral advocacy a form of movement building, that is, it may inspire additional people to work on the issues we care most about.

This post analyses key strategic questions on moral advocacy, such as:

What does moral advocacy look like in practice? Which values should we spread, and how?
How effective is moral advocacy compared to other interventions such as directly influencing new technologies?
What are the most important arguments for and against focusing on moral advocacy?},
	url = {https://prioritizationresearch.com/arguments-for-and-against-moral-advocacy/},
	journaltitle = {Cause Prioritization Research},
	author = {Baumann, Tobias},
	urldate = {2022-05-24},
	date = {2017-07-05},
	langid = {english},
	file = {~/Google Drive/library-pdf/Baumann2017ArgumentsMoralAdvocacy.pdf;~/Google Drive/library-html/arguments-for-and-against-moral-advocacy.html}
}

@online{Baumann2020LongtermismAnimalAdvocacy,
	database = {Tlön},
	title = {Longtermism and animal advocacy},
	abstract = {A discussion of whether animal advocacy or, more generally, expanding the moral circle, should be a priority for longtermists.},
	url = {https://centerforreducingsuffering.org/longtermism-and-animal-advocacy/},
	journaltitle = {Center for Reducing Suffering},
	author = {Baumann, Tobias},
	urldate = {2022-02-01},
	date = {2020-11-11},
	langid = {english},
	file = {~/Google Drive/library-pdf/Baumann2020LongtermismAnimalAdvocacy.pdf;~/Google Drive/library-html/longtermism-and-animal-advocacy.html}
}

@online{Baumann2020SummaryEricDrexler,
	database = {Tlön},
	title = {Summary of Eric Drexler’s work on reframing {AI}
                  safety},
	abstract = {This post contains a bullet point summary of Reframing Superintelligence: Comprehensive {AI} Services as General Intelligence. (I wrote this in 2017, so it does not necessarily refer to the most up-to-date version of Drexler’s work.)

I find Drexler’s work very interesting because he has a somewhat unusual perspective on AI. My take is that his ideas have some merit, and I like that he’s questioning key assumptions. But I’m less sure I would agree with all the details, and I think we should be much more uncertain about AI than his texts often (implicitly) suggest.

The key ideas are:

He thinks AGI isn’t necessarily agent-like. Instead, we might build “comprehensive AI services” (CAIS) which are superintelligent, but don’t act like an opaque agent.
He thinks the usual concept of intelligence is misguided, and that AI is radically unlike human intelligence.
He thinks humans might retain control of high-level strategic decisions.
In the following, I will summarise the chapters that I found most interesting.},
	langid = {english},
	url = {https://s-risks.org/summary-of-eric-drexlers-work-on-reframing-ai-safety/},
	journaltitle = {Reducing Risks of Future Suffering},
	author = {Baumann, Tobias},
	date = {2020-05-21}
}

@online{Baumann2020ThoughtsSpaceColonisation,
	database = {Tlön},
	title = {Thoughts on space colonisation},
	abstract = {Longtermist EAs tend to believe that space colonisation will become feasible at some point,  which would result in humanity spreading throughout the cosmos.
In this post, I’d like to offer some thoughts on how likely this is.},
	langid = {english},
	url = {https://s-risks.org/thoughts-on-space-colonisation/},
	journaltitle = {Reducing Risks of Future Suffering},
	author = {Baumann, Tobias},
	date = {2020-05-20},
	file = {~/Google Drive/library-pdf/Baumann2020ThoughtsSpaceColonisation.pdf}
}

@online{Baumann2023LargoplacismoDefensaDe,
	database = {Tlön},
	date = {2023},
	title = {El largoplacismo y la defensa de los animales},
	author = {Baumann, Tobias},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Baumann2020LongtermismAnimalAdvocacy}
}

@online{BayAreaHuman2021CommonKnowledgeFacts,
	database = {Tlön},
	title = {Common knowledge facts about Leverage Research 1.0},
	abstract = {This article, written as a report from a civilizational observer on Earth in 2022, speculates about the potential trajectory of human civilization in the coming decades, particularly in relation to the development of artificial intelligence (AI). The author suggests that intellectual automation may lead to a period of rapid acceleration, with AI taking over many cognitive tasks currently performed by humans. This could potentially lead to a "post-civilization" characterized by qualitatively different modes of thought and organization. The author also discusses the potential for the emergence of Omohundro-complete thinking machines, which would pursue fully general power over the future, and the implications of superhuman generalization by AI. The article emphasizes that the timing and nature of these changes are uncertain, but that they could have profound consequences for humanity. – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/Kz9zMgWB5C27Pmdkh/common-knowledge-facts-about-leverage-research-1-0-1},
	journaltitle = {{LessWrong}},
	author = {{BayAreaHuman}},
	urldate = {2021-10-01},
	date = {2021-09-24},
	file = {~/Google Drive/library-pdf/BayAreaHuman2021CommonKnowledgeFacts.pdf;~/Google Drive/library-html/BayAreaHuman2021CommonKnowledgeFacts.html}
}

@article{Beard2013WrongnessHumanExtinction,
	database = {Tlön},
	title = {On the wrongness of human extinction},
	abstract = {Walter Sinnott-Armstrong and Frank Miller's article is an intelligent, interesting and important discussion. Its central thesis is that what makes killing wrong is not that killing causes death or loss of consciousness, but that killing causes an individual to be completely, irreversibly disabled. The first of two main implications is that it is not even pro tanto wrong to kill someone who is already in such a thoroughly disabled state. The second is that the dead donor rule in the context of vital organ transplantation should be abandoned.
 
I embrace the second main implication. As the authors argue, the dead donor rule is routinely violated in contemporary vital organ procurement; but what should change are not the standard practices in which the rule is violated, but the rule itself. In general, I believe that the practical implications of the authors’ account of the wrongness of killing are more justified than the account itself. My judgment is possible because their account has mostly the same implications as what I take to be the best account—at least regarding the killing of human beings.},
	langid = {english},
	volume = 39,
	issn = 03066800,
	doi = {10.1136/medethics-2012-100652},
	pages = 9,
	number = 1,
	journaltitle = {Journal of Medical Ethics},
	author = {Beard, Simon and Kaczmarek, Patrick Krystof},
	date = 2013,
	file = {~/Google Drive/library-pdf/Beard2013WrongnessHumanExtinction.pdf}
}

@online{Beard2020ParfitBio,
	database = {Tlön},
	file = {~/Google Drive/library-pdf/Beard2020ParfitBio.pdf;~/Google Drive/library-html/Beard2020ParfitBio.html},
	abstract = {For centuries a debate on the ethics of future generations has been ongoing, and a conclusion referred to as the Repugnant Conclusion emerged. This conclusion states that there would be some much larger number of people whose existence would be worse than a smaller number of people with high-quality lives. The idea drew mixed opinions, including a claim that it proved a non-existence of God. However, it remains a popular finding in philosophy. Despite its popularity, the origins of the Repugnant Conclusion were never fully grasped until after the philosopher who coined it, Derek Parfit, offered a potential solution shortly before his death. Parfit believed he had finally arrived at an answer to the dilemma that plagued philosophers for so long. After studying ethical principles and real-world issues, Parfit ultimately landed on a concept called the Simple Value. This view states every life worth living must be inherently good and enhance the universe. However, he went on to argue that while considering the harms brought upon future generations is important, solely depending on it creates absurd and counterintuitive problems. To resolve the issue, Parfit proposed that one accepts that every life worth living is innately valuable. He called this Perfectionism. Even so, Parfit noted that Perfectionism had flaws of its own. In the end, Parfit arrived at the position that different values cannot be measured on a single scale, and art, science, love, and friendship must not be valuable simply because they are preferred by those who enjoy them, but they must be valuable in ways that other kinds of good things are not. – AI-generated abstract.},
	date = {2020},
	journaltitle = {Simon Beard's Website},
	author = {Beard, Simon},
	langid = {english},
	timestamp = {2023-02-04 04:55:47 (GMT)},
	title = {Parfit Bio},
	url = {https://sjbeard.weebly.com/parfit-bio.html},
	urldate = {2023-02-04}
}

@online{Becker2022ApplyJoinSHELTER,
	database = {Tlön},
	title = {Apply to join {SHELTER} Weekend this August},
	abstract = {{SHELTER} (Safe Haven to Evade Long-Term Extinction Risk) Weekend is a 3-4 day event focused on gaining strategic clarity on exactly what’s needed to build civilizational shelters, and perhaps kickstarting an organization to do so.
It will take place August 5th-8th 2022, at a residential retreat in Oxford, {UK}.},
	url = {https://forum.effectivealtruism.org/posts/ZkkeLBwRGgxmsiqrh/apply-to-join-shelter-weekend-this-august},
	journaltitle = {Effective Altruism Forum},
	author = {Becker, Joel and Cotton-Barratt, Owen and Flidrova,
                  Tereza and Lodemann, Sebastian},
	urldate = {2022-06-15},
	date = {2022-06-15},
	langid = {english},
	file = {~/Google Drive/library-pdf/Becker2022ApplyJoinSHELTER.pdf;~/Google Drive/library-html/apply-to-join-shelter-weekend-this-august.html}
}

@online{Beckstead2013ProposedAdjustmentAstronomical,
	database = {Tlön},
	title = {A proposed adjustment to the astronomical waste
                  argument},
	abstract = {In this post, I argue that, though Bostrom’s argument supports the conclusion that maximizing humanity’s long term potential is extremely important, it does not provide strong evidence that reducing existential risk is the best way of maximizing humanity’s future potential. There is a much broader class of actions which may affect humanity’s long-term potential, and Bostrom’s argument does not uniquely favor existential risk over other members in this class.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/RXpJbWKDJ7WFWqEin/a-proposed-adjustment-to-the-astronomical-waste-argument},
	journaltitle = {Effective Altruism Forum},
	author = {Beckstead, Nick},
	date = {2013-05-27},
	file = {~/Google Drive/library-pdf/Beckstead2013ProposedAdjustmentAstronomical.pdf}
}

@online{Beckstead2014ImprovingDisasterShelters,
	database = {Tlön},
	title = {Improving disaster shelters to increase the chances of
                  recovery from a global catastrophe},
	abstract = {Civilization might not recover from some possible global catastrophes. Conceivably, people with access to disaster shelters or other refuges may be more likely to survive and help civilization recover. However, existing disaster shelters (sometimes built to ensure continuity of government operations and sometimes built to protect individuals), people working on submarines, largely uncontacted peoples, and people living in very remote locations may serve this function to some extent. Other interventions may also increase the chances that humanity would recover from a global catastrophe, but this review focuses on disaster shelters. Proposed methods of improving disaster shelter networks include stocking shelters with appropriately trained people and resources that would enable them to rebuild civilization in case of a near-extinction event, keeping some shelters constantly full of people, increasing food reserves, and building more shelters. A philanthropist could pay to improve existing shelter networks in the above ways, or they could advocate for private shelter builders or governments to make some of the improvements listed above.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/fTDhRL3pLY4PNee67/improving-disaster-shelters-to-increase-the-chances-of},
	journaltitle = {Effective Altruism Forum},
	author = {Beckstead, Nick},
	date = {2014-02-19},
	file = {~/Google Drive/library-pdf/Beckstead2014ImprovingDisasterShelters.pdf}
}

@online{Beckstead2015LongtermSignificanceReducing,
	database = {Tlön},
	title = {The long-term significance of reducing global
                  catastrophic risks},
	abstract = {Note: Before the launch of the Open Philanthropy Project Blog, this post appeared on the {GiveWell} Blog. Uses of “we” and “our” in the below post may refer to the Open Philanthropy Project or to {GiveWell} as an organization. Additional comments may be available at the original post.},
	langid = {english},
	url = {https://www.openphilanthropy.org/blog/long-term-significance-reducing-global-catastrophic-risks},
	journaltitle = {Open philanthropy},
	author = {Beckstead, Nick},
	date = {2015-08-13},
	file = {~/Google Drive/library-pdf/Beckstead2015LongtermSignificanceReducing.pdf}
}

@online{Beckstead2015RisksAtomicallyPrecise,
	database = {Tlön},
	title = {Risks from atomically precise manufacturing},
	abstract = {This is a writeup of a shallow investigation, a brief look at an area that we use to decide how to prioritize further research.
Background:
Atomically precise manufacturing is a proposed technology for assembling macroscopic objects defined by data files by using very small parts to build the objects with atomic precision using earth-abundant materials. There is little consensus about its feasibility, how to develop it, or when, if ever, it might be developed. This page focuses primarily on potential risks from atomically precise manufacturing. We may separately examine its potential benefits and development pathways in more detail in the future.

What is the problem?
If created, atomically precise manufacturing would likely radically lower costs and expand capabilities in computing, materials, medicine, and other areas. However, it would likely also make it substantially easier to develop new weapons and quickly and inexpensively produce them at scale with an extremely small manufacturing base. In addition, some argue that it would help make it possible to create tiny self-replicating machines that could consume the Earth’s resources in a scenario known as “grey goo,” but such machines would have to be designed deliberately and we are highly uncertain of whether it would be possible to make them.

What are possible interventions?
A philanthropist could seek to influence research and development directions or support policy research. Potential goals could include achieving consensus regarding the feasibility of atomically precise manufacturing, identifying promising development strategies, and/or mitigating risks from possible military applications. We are highly uncertain about how to weigh the possible risks and benefits from accelerating progress toward APM and about the effectiveness of policy research in the absence of greater consensus regarding the feasibility of the technology.

Who else is working on it?
A few small non-profit organizations have explicitly focused on research, development, and policy analysis related to atomically precise manufacturing. Atomically precise manufacturing receives little explicit attention in academia, but potential enabling technologies such as DNA nanotechnology and scanning probe microscopy are active fields of research.},
	langid = {english},
	url = {https://www.openphilanthropy.org/research/cause-reports/atomically-precise-manufacturing},
	journaltitle = {Open philanthropy},
	author = {Beckstead, Nick},
	date = {2015-06},
	file = {~/Google Drive/library-pdf/Beckstead2015RisksAtomicallyPrecise.pdf}
}

@incollection{Beckstead2016BubblesWallpaperHealthcare,
	database = {Tlön},
	location = {Chichester, West Sussex},
	langid = {english},
	edition = {3},
	title = {Bubbles under the wallpaper: healthcare rationing and
                  discrimination},
	isbn = {978-1-118-94151-5 978-1-118-94152-2},
	booktitle = {Bioethics: an anthology},
	publisher = {John Wiley \& Sons Inc},
	author = {Beckstead, Nick and Ord, Toby},
	editor = {Kuhse, Helga and Schüklenk, Udo and Singer, Peter},
	date = 2016
}

@online{Beckstead2022ClarificationsFutureFund,
	database = {Tlön},
	title = {Some clarifications on the Future Fund's approach to
                  grantmaking},
	abstract = {This post comments on concerns people have about the {FTX} Foundation and the Future Fund, and our contribution to free-spending {EA} worries.
I think there are a lot of important and reasonable concerns about spending money too quickly and without adequate oversight. We're making decisions that directly affect how many people spend a majority of their working hours, those decisions are hard to make, and there can be community-wide consequences to making them badly. It's also possible that some of our grantees are spending more than is optimal, and our funding is contributing to that. (If there are particular {FTX} Foundation grants that you think were a significant mistake, we would love feedback about those grants! You can share the feedback (anonymously if you prefer) via this form.).},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/hDK9CZJwH2Cqc9n9J/some-clarifications-on-the-future-fund-s-approach-to},
	journaltitle = {Effective Altruism Forum},
	author = {Beckstead, Nick},
	urldate = {2022-05-10},
	date = {2022-05-10},
	file = {~/Google Drive/library-pdf/Beckstead2022ClarificationsFutureFund.pdf;~/Google Drive/library-html/some-clarifications-on-the-future-fund-s-approach-to.html}
}

@online{Beckstead2022FutureFundProject,
	database = {Tlön},
	title = {The Future Fund’s project ideas competition},
	abstract = {The {FTX} Foundation's Future Fund is a philanthropic fund making grants and investments to ambitious projects in order to improve humanity's long-term prospects.
We have a longlist of project ideas that we’d be excited to help launch.
We’re now announcing a prize for new project ideas to add to this longlist. If you submit an idea, and we like it enough to add to the website, we’ll pay you a prize of \$5,000 (or more in exceptional cases). We’ll also attribute the idea to you on the website (unless you prefer to be anonymous).},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/KigFfo4TN7jZTcqNH/the-future-fund-s-project-ideas-competition},
	journaltitle = {Effective Altruism Forum},
	author = {Beckstead, Nick and Ramakrishnan, Ketan and
                  Aschenbrenner, Leopold and {MacAskill}, William},
	urldate = {2022-03-10},
	date = {2022-02-28},
	file = {~/Google Drive/library-pdf/Beckstead2022FutureFundProject.pdf}
}

@online{Beckstead2023PropuestaDeAjuste,
	database = {Tlön},
	date = {2023},
	title = {Una propuesta de ajuste al argumento del desperdicio
                  astronómico},
	author = {Beckstead, Nick},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Beckstead2013ProposedAdjustmentAstronomical}
}

@article{Bedke2010MightAllNormativity,
	database = {Tlön},
	author = {Bedke, Matthew S.},
	abstract = {Here I discuss the conceptual structure and core semantic commitments of reason-involving
thought and discourse needed to underwrite the claim that ethical normativity is not
uniquely queer. This deflates a primary source of ethical scepticism and it vindicates
so-called partner in crime arguments. When it comes to queerness objections, all
reason-implicating normative claims—including those concerning Humean reasons to pursue
one's ends, and epistemic reasons to form true beliefs—stand or fall together.},
	title = {Might All Normativity Be Queer?},
	volume = {88},
	number = {1},
	pages = {41–58},
	doi = {10.1080/00048400802636445},
	url = {http://www.tandfonline.com/doi/abs/10.1080/00048400802636445},
	date = {2010-03},
	issn = {0004-8402, 1471-6828},
	journaltitle = {Australasian Journal of Philosophy},
	langid = {english},
	shortjournal = {Australasian Journal of Philosophy},
	timestamp = {2023-07-27 18:51:33 (GMT)},
	urldate = {2023-07-27}
}

@online{Behar2020WhatAreMost,
	database = {Tlön},
	title = {What are the most common objections to “multiplier”
                  organizations that raise funds for other effective
                  charities?},
	abstract = {Numerous {EA} organizations use a “multiplier” model in which they try to leverage each dollar they spend on their own operations by fundraising multiple dollars for other effective charities. My strong impression is that the number of donors who give to effective charities doing direct work is much larger than the number of donors who give to organizations that fundraise for effective charities doing direct work. I would like to understand why this is the case.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/jZSbfvZsAk5TM9vpz/what-are-the-most-common-objections-to-multiplier#67NXmPWhSGwFwfo4z},
	journaltitle = {Effective Altruism Forum},
	author = {Behar, Jon},
	date = {2020-12-08},
	file = {~/Google Drive/library-pdf/Behar2020WhatAreMost.pdf}
}

@book{Bender2006BendersDictionaryNutrition,
	database = {Tlön},
	location = {Boca Raton},
	langid = {english},
	edition = 8,
	title = {Benders' dictionary of nutrition and food technology},
	isbn = {978-1-84569-051-9},
	series = {Woodhead Publishing in food science and technology},
	publisher = {{CRC} Press},
	author = {Bender, David A.},
	date = 2006,
	file = {~/Google Drive/library-pdf/Bender2006BendersDictionaryNutrition.pdf}
}

@book{Bennett1995ActItself,
	database = {Tlön},
	location = {Oxford},
	langid = {english},
	title = {The Act Itself},
	isbn = {0-19-823548-8},
	publisher = {Clarendon Press},
	author = {Bennett, Jonathan},
	date = 1995,
	file = {~/Google Drive/library-pdf/Bennett1998ActItself.pdf}
}

@book{Bentham1789IntroductionPrinciplesMorals,
	database = {Tlön},
	location = {London},
	langid = {spanish},
	title = {An Introduction to the Principles of Morals and
                  Legislation},
	publisher = {T. Payne \& Son},
	author = {Bentham, Jeremy},
	date = 1789,
	translation = {Bentham2008PrincipiosDeMoral}
}

@Book{Bentham1825TheoriePeinesEt,
	volume = {1},
	title = {Théorie des peines et des récompenses},
	langid = {french},
	author = {Bentham, Jeremy},
	translator = {Dumont, Étienne},
	editor = {Dumont, Étienne},
	database = {Tlön},
	address = {Paris},
	location = {Paris},
	publisher = {Bossange frères},
	file = {~/Google Drive/library-pdf/Dumont1825TheoriePeinesEt.pdf},
	date = {1825},
	edition = {3},
	timestamp = {2023-07-27 17:39:56 (GMT)}
}

@Book{Bentham1838TeoriaDePenas,
	volume = {1},
	author = {Bentham, Jeremy},
	langid = {french},
	editor = {Dumont, Étienne},
	database = {Tlön},
	translation = {Bentham1825TheoriePeinesEt},
	file = {~/Google Drive/library-pdf/Dumont1838TeoriaDePenas.pdf},
	translator = {D. L. B.},
	address = {Barcelona},
	location = {Barcelona},
	publisher = {D. Manuel Saurí},
	date = {1838},
	title = {Teoría de las penas y de las recompensas},
	timestamp = {2023-07-27 17:40:24 (GMT)}
}

@Book{Bentham2017IntroductionToPrinciples,
	database = {Tlön},
	publisher = {Early Modern Texts},
	editor = {Bennett, Jonathan},
	langid = {english},
	file = {~/Google Drive/library-pdf/Bentham2017IntroductionToPrinciples.pdf},
	url = {https://www.earlymoderntexts.com/assets/pdfs/bentham1780.pdf},
	date = {2017},
	journaltitle = {Early Modern Texts},
	title = {An Introduction to the Principles of Morals and Legislation},
	author = {Bentham, Jeremy},
	timestamp = {2023-07-04 10:58:57 (GMT)}
}

@online{Bergal2019ConversationRobinHanson,
	database = {Tlön},
	title = {Conversation with Robin Hanson},
	abstract = {Mankind fought smallpox for centuries, but using smallpox vaccine, the disease was eradicated in the 20th century. This achievement was due to factors including widespread vaccination, isolation of contagious people, and education about the disease. Research into the eradication of smallpox provides insights for dealing with similar diseases such as poliomyelitis and dracunculiasis. – AI-generated abstract.},
	url = {https://aiimpacts.org/conversation-with-robin-hanson/},
	journaltitle = {{AI} Impacts},
	author = {Bergal, Asya and Long, Robert},
	urldate = {2021-10-04},
	date = {2019-11-13},
	langid = {english},
	note = {Section: Conversation notes},
	file = {~/Google Drive/library-pdf/Bergal2019ConversationRobinHanson.pdf;~/Google Drive/library-html/conversation-with-robin-hanson.html}
}

@online{Bergal2021LongTermFutureFund,
	database = {Tlön},
	title = {Long-Term Future Fund: May 2021 grant recommendations},
	abstract = {Since November, we’ve made 27 grants worth a total of \$1,650,795 (with the expectation that we will get up to \$220,000 back), reserved \$26,500 for possible later spending, and referred two grants worth a total of \$120,000 to private funders. We anticipate being able to spend \$3–8 million this year (up from \$1.4 million spent in all of 2020). To fill our funding gap, we’ve applied for a \$1–1.5 million grant from the Survival and Flourishing Fund, and we hope to receive more funding from small and large longtermist donors.
The composition of the fund has changed since our last grant round. The new regular fund team consists of Asya Bergal (chair), Adam Gleave, and Oliver Habryka, though we may take on additional regular fund managers over the next several months. Notably, we’re experimenting with a “guest manager” system where we invite people to act as temporary fund managers under close supervision. Our guest managers this round were Daniel Eth, Evan Hubinger, and Ozzie Gooen.},
	url = {https://forum.effectivealtruism.org/posts/diZWNmLRgcbuwmYn4/long-term-future-fund-may-2021-grant-recommendations},
	shorttitle = {Long-Term Future Fund},
	journaltitle = {Effective Altruism Forum},
	author = {Bergal, Asya},
	urldate = {2022-06-13},
	date = {2021-05-27},
	langid = {english},
	file = {~/Google Drive/library-pdf/Bergal2021LongTermFutureFund.pdf}
}

@online{Berger2014PotentialGlobalCatastrophic,
	database = {Tlön},
	title = {Potential global catastrophic risk focus areas},
	abstract = {As part of our work on the Open Philanthropy Project, we’ve been exploring the possibility of getting involved in efforts to ameliorate potential global catastrophic risks (GCRs), by which we mean risks that could be bad enough to change the very long-term trajectory of humanity in a less favorable direction (e.g. ranging from a dramatic slowdown in the improvement of global standards of living to the end of industrial civilization or human extinction). Examples of such risks could include a large asteroid striking earth, worse-than-expected consequences of climate change, or a threat from a novel technology, such as an engineered pathogen.},
	langid = {english},
	url = {https://www.openphilanthropy.org/blog/potential-global-catastrophic-risk-focus-areas},
	journaltitle = {Open philanthropy},
	author = {Berger, Alexander},
	date = {2014-06-26},
	file = {~/Google Drive/library-pdf/Berger2014PotentialGlobalCatastrophic.pdf}
}

@online{Berger20212021AllocationGiveWell,
	database = {Tlön},
	title = {2021 allocation to {GiveWell} top charities: why we’re
                  giving more going forward},
	abstract = {As we wrote last week, we’re substantially growing our overall giving in Global Health and Wellbeing, with the bar in that broad portfolio continuing to be set by the cost-effective, evidence-backed charities recommended by {GiveWell}.},
	url = {https://www.openphilanthropy.org/blog/2021-allocation-givewell-top-charities-why-we-re-giving-more-going-forward},
	shorttitle = {2021 Allocation to {GiveWell} Top Charities},
	journaltitle = {Open Philanthropy},
	author = {Berger, Alexander},
	urldate = {2021-11-23},
	date = {2021-11-22},
	langid = {english},
	file = {~/Google Drive/library-pdf/Berger20212021AllocationGiveWell.pdf;~/Google Drive/library-html/2021-allocation-givewell-top-charities-why-we-re-giving-more-going-forward.html}
}

@online{Bernard2020IntroductionGlobalPriorities,
	database = {Tlön},
	title = {An introduction to global priorities research for
                  economists},
	abstract = {It’s difficult for aspiring economists to understand how they can contribute to global priorities research. This post shares a syllabus and extended literature for an introduction to global priorities for economists, provides background on the development of the syllabus, its use for a reading group, and advice to others who want to use it. The syllabus was designed with graduate students in mind, but advanced undergraduates will likely be able to get the relevant points from almost all papers. We hope this will be useful for aspiring economists who want to contribute to global priorities research but don’t know where to start.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/dia3NcGCqLXhWmsaX/an-introduction-to-global-priorities-research-for-economists},
	journaltitle = {Effective Altruism Forum},
	author = {Bernard, David Rhys},
	urldate = {2022-04-04},
	date = {2020-08-28},
	file = {~/Google Drive/library-pdf/Bernard2020IntroductionGlobalPriorities.pdf;~/Google Drive/library-html/an-introduction-to-global-priorities-research-for-economists.html}
}

@online{Bernard2021InterventionReportCharter,
	database = {Tlön},
	title = {Intervention report: Charter cities},
	abstract = {The value of charter cities can be divided into three main buckets: (1) direct benefits from providing an engine of growth that increase the incomes and wellbeing of people living in and around the city, (2) domestic indirect benefits from scaling up successful charter city policies across the host country, and (3) global indirect benefits from providing a laboratory to experiment with new policies, regulations, and governance structures. We think it is unlikely that charter cities will be more cost-effective than {GiveWell} top charities in terms of directly improving wellbeing.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/EpaSZWQkAy9apupoD/intervention-report-charter-cities},
	journaltitle = {Effective Altruism Forum},
	author = {Bernard, David and Schukraft, Jason},
	date = {2021-06-12},
	file = {~/Google Drive/library-pdf/Bernard2021InterventionReportCharter.pdf}
}

@online{Besiroglu2019RagnarokSeriesResults,
	database = {Tlön},
	title = {Ragnarök series — Results so far},
	langid = {english},
	url = {https://www.metaculus.com/questions/2568/ragnar%25C3%25B6k-seriesresults-so-far/},
	journaltitle = {Metaculus},
	author = {Besiroglu, Tamay},
	date = {2019-10-15},
	eventdate = {2021-02-10}
}

@online{Bikhchandani2008InformationCascadesAnd,
	database = {Tlön},
	langid = {english},
	journaltitle = {info-cascades.info},
	author = {Bikhchandani and Hirshleifer, David and Welch, Ivo},
	date = {2008-02-15},
	timestamp = {2023-08-30 09:03:10 (GMT)},
	title = {Information cascades and rational herding: an annotated bibliography and resource reference},
	url = {https://web.archive.org/web/20080215131441/http://www.info-cascades.info/},
	urldate = {2023-08-30}
}

@Report{BipartisanCommissiononBiodefense2022AthenaAgendaAdvancing,
	url = {https://biodefensecommission.org/wp-content/uploads/2022/04/Athena-Report_v7.pdf},
	database = {Tlön},
	langid = {english},
	file = {~/Google Drive/library-pdf/Biodefense2022AthenaAgendaAdvancing.pdf},
	date = {2022-04},
	institution = {Bipartisan Commission on Biodefense},
	title = {The Athena Agenda: Advancing the Apollo Program for
                  Biodefense},
	author = {{Bipartisan Commission on Biodefense}},
	timestamp = {2023-06-07 16:14:36 (GMT)}
}

@report{Birch2021ReviewEvidenceSentience,
	database = {Tlön},
	title = {Review of the evidence of sentience in cephalopod
                  molluscs and decapod crustaceans},
	langid = {english},
	url = {https://www.lse.ac.uk/News/News-Assets/PDFs/2021/Sentience-in-Cephalopod-Molluscs-and-Decapod-Crustaceans-Final-Report-November-2021.pdf},
	institution = {London School of Economics},
	author = {Birch, Jonathan and Burn, Charlotte and Schnell,
                  Alexandra and Browning, Heather and Crump, Andrew},
	date = {2021-11},
	file = {~/Google Drive/library-pdf/Birch2021ReviewEvidenceSentience.pdf}
}

@book{Blair1993LogicAccidentalNuclear,
	database = {Tlön},
	location = {Washington, D.C},
	langid = {english},
	title = {The logic of accidental nuclear war},
	isbn = {978-0-8157-0984-8 978-0-8157-0983-1},
	pagetotal = 364,
	publisher = {Brookings Institution},
	author = {Blair, Bruce G.},
	date = 1993
}

@article{Bliss2022ExistentialAdvocacy,
	database = {Tlön},
	title = {Existential advocacy},
	issn = {1556-5068},
	url = {https://www.ssrn.com/abstract=4217687},
	doi = {10.2139/ssrn.4217687},
	journaltitle = {{SSRN} Electronic Journal},
	shortjournal = {{SSRN} Journal},
	author = {Bliss, John},
	urldate = {2022-11-02},
	date = 2022,
	langid = {english},
	file = {~/Google Drive/library-pdf/Bliss2022ExistentialAdvocacy.pdf}
}

@online{Bloom2019DataAnalysisLW,
	database = {Tlön},
	title = {Data analysis of {LW}: Activity levels + age
                  distribution of user accounts},
	langid = {english},
	url = {https://www.lesswrong.com/posts/9dA6GfuDca3Zh3RMa/data-analysis-of-lw-activity-levels-age-distribution-of-user},
	journaltitle = {{LessWrong}},
	author = {Bloom, Ruben},
	date = {2019-05-14}
}

@online{Blough2022WargamingAGIDevelopment,
	database = {Tlön},
	title = {Wargaming {AGI} development},
	abstract = {Wargaming can be useful for exploring scenarios in AGI development. A wargame of this kind could help to answer questions about timelines, takeoff scenarios, risk mitigation, and other variables that affect AGI development. Simulating the future development of AGI will allow researchers to explore concrete situations including capabilities that do not exist yet, against which they can compare their models of the future. Exploring future AGI in a controlled scenario will allow for end-to-end testing of hypotheses and improved prediction accuracy. – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/ouFnZoYaKqicC6jH8/wargaming-agi-development},
	journaltitle = {{LessWrong}},
	author = {Blough, Ryan},
	urldate = {2022-06-12},
	date = {2022-03-19},
	file = {~/Google Drive/library-pdf/RyanBlough2022WargamingAGIDevelopment.pdf}
}

@online{Boey2015EffectiveAltruismIts,
	database = {Tlön},
	title = {Effective altruism and its blind spots},
	abstract = {Effective altruism, a growing social movement, seeks to maximize the effectiveness of altruism. By focusing on causes and organizations with the highest impact per dollar spent, effective altruists aim to create the greatest amount of good possible. However, the quest for efficiency may lead to supporting unjust systems and neglecting the fight against systemic inequality. – AI-generated abstract.},
	langid = {english},
	url = {https://www.3quarksdaily.com/3quarksdaily/2015/08/effective-altruism-and-its-blind-spots.html},
	journaltitle = {3 Quarks Daily},
	author = {Boey, Grace},
	date = {2015-08-31},
	file = {~/Google Drive/library-pdf/Boey2015EffectiveAltruismIts.pdf}
}

@online{Bogosian2019OverviewCapitalismSocialism,
	database = {Tlön},
	title = {Overview of capitalism and socialism for effective
                  altruism},
	abstract = {Capitalism is an economic system where the means of production - land, machinery, investment capital - are mostly controlled by private owners. Socialism is an economic system where they are controlled by the public in a collective or governmental organization. For a variety of reasons, it could be important to take a stance on the question of which is preferable.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/ktEfsoGfBFGsaiY46/overview-of-capitalism-and-socialism-for-effective-altruism},
	journaltitle = {Effective Altruism Forum},
	author = {Bogosian, Kyle},
	date = {2019-05-16},
	file = {~/Google Drive/library-pdf/Bogosian2019OverviewCapitalismSocialism.pdf;~/Google Drive/library-pdf/Bogosian2019OverviewCapitalismSocialism.pdf}
}

@online{Bonde2022SnakebitesKill100,
	database = {Tlön},
	title = {Snakebites kill 100,000 people every year, here's what
                  you should know},
	abstract = {{WHO} estimates between 81,000-138,000 die every year from venomous snakebites. What on earth!? When I first heard this I refused to believe it. How had I managed to call myself an {EA} for so many years and not known snakebites were this big a deal? Not only do snakebites kill an unfathomable number of people, another four hundred thousand are permanently disabled every year.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/WyqGircJgCBG9ivNL/snakebites-kill-100-000-people-every-year-here-s-what-you},
	journaltitle = {Effective Altruism Forum},
	author = {Bonde, Mathias Kirk},
	urldate = {2022-04-30},
	date = {2022-04-27},
	file = {~/Google Drive/library-pdf/Bonde2022SnakebitesKill100.pdf;~/Google Drive/library-html/snakebites-kill-100-000-people-every-year-here-s-what-you.html}
}

@online{Bostrom1997PredictionsPhilosophy,
	database = {Tlön},
	title = {Predictions from philosophy?},
	abstract = {The purpose of this paper, boldly stated, is to propose a new type of philosophy, a philosophy whose aim is prediction. The pace of technological progress is increasing very rapidly: it looks as if we are witnessing an exponential growth, the growth-rate being proportional to the size already obtained, with scientific knowledge doubling every 10 to 20 years since the second world war, and with computer processor speed doubling every 18 months or so. It is argued that this technological development makes urgent many empirical questions which a philosopher could be well-suited to help answering. I try to cover a broad range of interesting problems and approaches, which means that I won't go at all deeply into any of them; I only try to say enough to show what some of the problems are, how one can begin to work with them, and why philosophy is relevant. My hope is that this will whet your appetite to deal with these questions, or at least increase general awareness that they worthy tasks for first-class intellects, including ones which might belong to philosophers. – AI-generated abstract.},
	langid = {english},
	url = {https://nickbostrom.com/old/predict},
	journaltitle = {Nick Bostrom's website},
	author = {Bostrom, Nick},
	date = {1997-04-09},
	eventdate = {1998-09-19},
	file = {~/Google Drive/library-pdf/Bostrom1997PredictionsPhilosophy.pdf}
}

@online{Bostrom2001PrimerDoomsdayArgument,
	database = {Tlön},
	title = {A primer on the Doomsday argument},
	abstract = {The Doomsday argument is a probabilistic argument that attempts to show that the probability that humanity will go extinct soon is much higher than generally believed. The argument follows simple and seemingly plausible premises, but it has faced scrutiny and challenges since its formulation. The argument proceeds in three steps: in the first, it establishes the self-sampling assumption, which states that observers should reason as if they were a random sample from the set of all observers. In the second step, using this assumption, it argues that the number of humans that will ever exist has a very low probability of being very large, based on the distribution of birth ranks. In the third step, the argument applies this conclusion to the present situation, suggesting that the probability of human extinction within a century is very high. Possible objections and alternative interpretations to the argument are also discussed – AI-generated abstract.},
	langid = {english},
	url = {https://web.archive.org/web/20011029231255/https://ephilosopher.com:80/feature/feature.htm},
	journaltitle = {ephilosopher.com},
	author = {Bostrom, Nick},
	date = 2001,
	file = {~/Google Drive/library-pdf/Bostrom2001PrimerDoomsdayArgument.pdf}
}

@online{Bostrom2015AstronomicalStakes,
	database = {Tlön},
	title = {Astronomical stakes},
	langid = {english},
	url = {https://www.youtube.com/watch?v=fmQkLKLmfQU},
	journaltitle = {Effective Altruism Global},
	author = {Bostrom, Nick},
	urldate = {2021-12-24},
	date = {2015-11-25}
}

@InBook{Bostrom2020PublicPolicySuperintelligent,
	pages = {293--326},
	booktitle = {Ethics of artificial intelligence},
	abstract = {This chapter considers the speculative prospect of superintelligent AI and its normative implications for governance and global policy. Machine superintelligence would be a transformative development that would present a host of political challenges and opportunities. This chapter identifies a set of distinctive features of this hypothetical policy context, from which it derives a correlative set of policy desiderata (efficiency, allocation, population, and process)—considerations that should be given extra weight in long-term AI policy compared to in other policy contexts. This chapter describes a desiderata “vector field” showing the directional change from a variety of possible normative baselines or policy positions. The focus on directional normative change should make the findings in this chapter relevant to a wide range of actors, although the development of concrete policy options that meet these abstractly formulated desiderata will require further work.},
	langid = {english},
	database = {Tlön},
	location = {Oxford},
	title = {Public policy and superintelligent {AI}},
	isbn = {978-0-19-090504-0},
	publisher = {Oxford University Press},
	author = {Bostrom, Nick and Dafoe, Allan and Flynn, Carrick},
	editor = {Liao, S. Matthew},
	date = 2020,
	doi = {10.1093/oso/9780190905033.003.0011},
	file = {~/Google Drive/library-pdf/Bostrom2020PublicPolicySuperintelligent.pdf}
}

@online{Bostrom2023ConsideracionesCrucialesFilantropia,
	keywords = {consideración crucial, macroestrategia, riesgo existencial},
	database = {Tlön},
	date = {2023},
	title = {Consideraciones cruciales y filantropía sabia},
	author = {Bostrom, Nick},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Bostrom2014CrucialConsiderationsWise}
}

@online{Bostrom2023HipotesisDelMundo,
	date = {2023},
	title = {La hipótesis del mundo vulnerable},
	database = {Tlön},
	author = {Bostrom, Nick},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Bostrom2019VulnerableWorldHypothesis}
}

@online{Bostrom2023PrevencionDelRiesgo,
	database = {Tlön},
	date = {2023},
	title = {La prevención del riesgo existencial como prioridad
                  global},
	author = {Bostrom, Nick},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Bostrom2013ExistentialRiskPrevention}
}

@online{Bostrom2023TresFormasDe,
	database = {Tlön},
	date = {2023},
	author = {Bostrom, Nick},
	langid = {spanish},
	title = {Tres formas de hacer avanzar la ciencia},
	translator = {Humarán, Aurora},
	translation = {Bostrom2008ThreeWaysAdvance}
}

@online{Bottger2020WhatCognitiveBiases,
	file = {~/Google Drive/library-pdf/Bottger2020WhatCognitiveBiases.pdf;~/Google Drive/library-html/Bottger2020WhatCognitiveBiases.html},
	date = {2020-01-03},
	abstract = {This document outlines the strategy of Effective Altruism London, an organization that coordinates people's efforts in promoting effective altruism in London. The organization's vision is to support individuals who are interested in effective altruism, while its mission is to coordinate and support these people in their efforts. It focuses on coordination, such as community-wide activities and meta-activities, rather than areas such as retreats and bespoke communities. The organization will measure its success through metrics such as the number of people engaged in effective altruism, the quality of such engagement, and the level of coordination within the community. – AI-generated abstract.},
	database = {Tlön},
	journaltitle = {LessWrong},
	author = {Böttger, Daniel},
	title = {What cognitive biases feel like from the inside},
	url = {https://www.lesswrong.com/posts/ERWeEA8op6s6tYCKy/what-cognitive-biases-feel-like-from-the-inside},
	langid = {english},
	timestamp = {2023-06-20 20:05:23 (GMT)},
	urldate = {2023-06-20}
}

@online{Boudry2017ThinkYouGive,
	database = {Tlön},
	title = {Think before you give: charity should be more rational
                  and less emotional - areo},
	abstract = {Charity work would be more effective if it was driven more by cool-headed rationality and less by empathy and emotions. Despite popular belief, all good causes are not equally good, and the differences can be immense. An EA-inspired non-profit called GiveWell maintains a list of the best charitable organizations, based on rigorous criteria such as cost-effectiveness and the impact measurement using objective metrics. Many people would argue that charity work should be driven by empathy, but research has shown that rational calculations enhance the impact of good intentions, promoting greater human well-being. – AI-generated abstract.},
	langid = {english},
	url = {https://areomagazine.com/2017/09/01/think-before-you-give-charity-should-be-more-rational-and-less-emotional/},
	journaltitle = {Aero},
	author = {Boudry, Maarten},
	date = {2017-09-01},
	file = {~/Google Drive/library-pdf/Boudry2017ThinkYouGive.pdf}
}

@online{BowermanAIPolicyIntroductory,
	database = {Tlön},
	title = {{AI} policy introductory reading list},
	abstract = {This article discusses the strategy of an organization dedicated to promoting effective altruism in London. It focuses on activities that encourage coordination and collaboration among individuals interested in the cause. The organization's priorities include promoting greater collaboration amongst existing local effective altruism groups, facilitating communication and knowledge sharing, and providing opportunities for members to engage with each other. The document also outlines the organization's vision, mission, and metrics for measuring the success of its strategies. – AI-generated abstract.},
	url = {https://docs.google.com/document/d/1W7KmO7TcHbM1-0CFW7TrbvG8PFqEcoYeie-vv994d6E/edit?usp=embed_facebook},
	author = {Bowerman, Niel},
	urldate = {2022-03-27},
	langid = {english},
	file = {~/Google Drive/library-html/edit.html}
}

@online{Bradshaw2020InformationHazardsVery,
	database = {Tlön},
	title = {Information hazards: a very simple typology},
	abstract = {It seems that everyone who looks into information hazards eventually develops their own typology. This isn't so surprising; Bostrom's original paper lists about thirty different categories of information hazards and isn't even completely exhaustive, and those who wish to work with a simpler system have many ways of dividing up that space.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/X5S2ZB4RcPxZGN68T/information-hazards-a-very-simple-typology},
	journaltitle = {Effective Altruism Forum},
	author = {Bradshaw, William},
	date = {2020-07-13},
	file = {~/Google Drive/library-pdf/Bradshaw2020InformationHazardsVery.pdf}
}

@online{Bradshaw2022AnnouncingNucleicAcid,
	database = {Tlön},
	title = {Announcing the Nucleic Acid Observatory project for
                  early detection of catastrophic biothreats},
	abstract = {The Nucleic Acid Observatory project aims to protect the world from catastrophic biothreats by detecting novel agents spreading in the human population or environment. We are developing the experimental tools, computational approaches, and hands-on knowledge needed to achieve reliable early detection of any future outbreak. We are hiring for research and support roles.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/gLPEAFicFBW8BKCnr/announcing-the-nucleic-acid-observatory-project-for-early},
	journaltitle = {Effective Altruism Forum},
	author = {Bradshaw, William and {McLaren}, Michael and Gopal,
                  Anjali},
	urldate = {2022-04-29},
	date = {2022-04-29},
	file = {~/Google Drive/library-pdf/Bradshaw2022AnnouncingNucleicAcid.pdf;~/Google Drive/library-html/announcing-the-nucleic-acid-observatory-project-for-early.html}
}

@online{Branwen2009ColderWars,
	database = {Tlön},
	title = {Colder wars},
	abstract = {Inspired by Ender's Game's controversial ending, this essay analyzes futuristic space warfare from a Cold War perspective. In a space-based Cold War, every system is vulnerable to surprise attacks from multiple directions, particularly from planet-shattering asteroids. Mitigating these attacks poses significant challenges, as traditional defenses like fortifications are not possible in space. Conventional warfare's limitations provide no respite in space. Countries can't fall back and regroup, and there is no time for decisive battles. The vastness of space and the difficulty in detecting incoming objects make a second-strike impossible. Unlike in Cold War nuclear warfare, there may not be someone to hold accountable for space-based attacks, as objects could be stolen or laundered. Overall, the analysis suggests that a space-based Cold War would be disastrous, with no clear advantage for defenders. – AI-generated abstract.},
	rights = {https://creativecommons.org/publicdomain/zero/1.0/},
	url = {https://www.gwern.net/Colder-Wars},
	journaltitle = {Gwern.net},
	author = {Branwen, Gwern},
	urldate = {2022-02-15},
	date = {2009-06-07},
	langid = {english},
	note = {22 June 2013},
	file = {~/Google Drive/library-html/Colder-Wars.html}
}

@online{Brennan2019ClimateChangeGeneral,
	database = {Tlön},
	title = {Climate change is, in general, not an existential
                  risk},
	abstract = {Climate change is not an existential risk to humanity. While severe consequences are to be expected, such as the death of many people, the spread of diseases, a global food crisis, and geopolitical instability, scientific consensus claims that it will not lead to the extinction of humans. – AI-generated abstract},
	langid = {english},
	url = {https://thingofthings.wordpress.com/2019/01/11/climate-change-is-not-an-existential-risk/},
	journaltitle = {Thing of things},
	author = {Brennan, Ozy},
	date = {2019-01-11},
	file = {~/Google Drive/library-pdf/Brennan2019ClimateChangeGeneral.pdf}
}

@online{Brewer2022VulnerableWorldHypothesis,
	database = {Tlön},
	title = {On the vulnerable world hypothesis},
	abstract = {First, I consider the premises and assumptions of the Vulnerable World Hypothesis. Second, I consider the costs and benefits of surveillance. Third, I relax these assumptions one by one. As things stand, I don't think ubiquitous real-time worldwide surveillance is a good idea.},
	url = {https://forum.effectivealtruism.org/posts/Kj5jsfzb5JJgiofbw/on-the-vulnerable-world-hypothesis},
	journaltitle = {Effective Altruism Forum},
	author = {Brewer, Catherine},
	urldate = {2022-08-01},
	date = {2022-08-01},
	langid = {english},
	file = {~/Google Drive/library-pdf/Brewer2022VulnerableWorldHypothesis.pdf;~/Google Drive/library-html/on-the-vulnerable-world-hypothesis.html}
}

@book{Broad1930FiveTypesEthical,
	database = {Tlön},
	location = {London},
	langid = {english},
	title = {Five types of ethical theory},
	publisher = {Kegan Paul \& Co.},
	author = {Broad, C. D.},
	date = 1930,
	file = {~/Google Drive/library-pdf/Broad1930FiveTypesEthical.pdf}
}

@article{Bronsteen2013WellbeingAnalysisVs,
	database = {Tlön},
	title = {Well-being analysis vs. cost-benefit analysis},
	langid = {english},
	volume = 62,
	issn = {1556-5068},
	url = {https://scholarship.law.duke.edu/dlj/vol62/iss8/3/},
	pages = {1603–1689},
	journaltitle = {Duke Law Journal},
	author = {Bronsteen, John and Buccafusco, Christopher and
                  Masur, Jonathan S.},
	date = 2013,
	file = {~/Google Drive/library-pdf/Bronsteen2013WellbeingAnalysisVs.pdf}
}

@online{Bruers2016MoralIllusionsAnd,
	database = {Tlön},
	file = {~/Google Drive/library-html/Bruers2016MoralIllusionsAnd.html;~/Google Drive/library-pdf/Bruers2016MoralIllusionsAnd.pdf},
	abstract = {Despite the significant suffering endured by wild animals, the importance of this issue is largely neglected due to various moral illusions. These illusions include speciesism, which values humans over other animals; the naturalistic fallacy, which assumes that natural processes are inherently good; status quo bias, which favors the current state of nature; scope neglect, which disregards the vast number of suffering animals; the just world hypothesis, which views suffering as a deserved punishment; and futility thinking, which dismisses efforts to address large-scale problems. These illusions contribute to an underestimation of the problem and a reluctance to intervene in nature to reduce animal suffering. Tackling wild animal suffering requires more scientific research, debiasing of moral judgments, and a re-evaluation of speciesism. – AI-generated abstract.},
	author = {Bruers, Stijn},
	date = {2016-07-20},
	journaltitle = {The rational ethicist},
	langid = {dutch},
	timestamp = {2023-06-24 10:46:00 (GMT)},
	title = {Moral illusions and wild animal suffering neglect},
	url = {https://stijnbruers.wordpress.com/2016/07/20/moral-illusions-and-wild-animal-suffering-neglect/},
	urldate = {2023-06-24}
}

@online{Brundage2022WhyAGITimeline,
	database = {Tlön},
	title = {Why {AGI} timeline research/discourse might be
                  overrated},
	abstract = {A very common subject of discussion among {EAs} is “{AGI} timelines.” Roughly, {AGI} timelines, as a research or discussion topic, refer to the time that it will take before very general {AI} systems meriting the moniker “{AGI}” are built, deployed, etc. (one could flesh this definition out and poke at it in various ways, but I don’t think the details matter much for my thesis here—see “What this post isn’t about” below). After giving some context and scoping, I argue below that while important in absolute terms, improving the quality of {AGI} timelines isn’t as useful as it may first appear.},
	url = {https://forum.effectivealtruism.org/posts/SEqJoRL5Y8cypFasr/why-agi-timeline-research-discourse-might-be-overrated},
	journaltitle = {Effective Altruism Forum},
	author = {Brundage, Miles},
	urldate = {2022-07-03},
	date = {2022-07-03},
	langid = {english},
	file = {~/Google Drive/library-pdf/Brundage2022WhyAGITimeline.pdf;~/Google Drive/library-html/why-agi-timeline-research-discourse-might-be-overrated.html}
}

@online{Buck2020HowGoodIs,
	date = {2020-07-21},
	database = {Tlön},
	abstract = {This work considers the question of whether humanity is good or bad at coordination. Two opposing viewpoints are presented. The first emphasizes the many near-misses and risky incidents in human history, especially regarding nuclear weapons, as evidence of humanity's poor coordination skills. The second emphasizes the fact that no nuclear weapons have been used or detonated accidentally since 1945, arguing that this proves humanity's competence in handling dangerous technology. The author finds weaknesses in both viewpoints and suggests that the question of humanity's coordination abilities may be more nuanced than either side admits. – AI-generated abstract.},
	file = {~/Google Drive/library-pdf/Buck2020HowGoodIs.pdf;~/Google Drive/library-html/Buck2020HowGoodIs.html},
	journaltitle = {LessWrong},
	author = {{Buck}},
	title = {How good is humanity at coordination?},
	url = {https://www.lesswrong.com/posts/y3jDSoTTdBD9Nj3Gx/how-good-is-humanity-at-coordination},
	langid = {english},
	timestamp = {2023-09-26 14:35:43 (GMT)},
	urldate = {2023-09-26}
}

@online{Bullock2019SHICWillSuspend,
	database = {Tlön},
	title = {{SHIC} will suspend outreach operations},
	abstract = {2018 saw strong uptake, but difficulty securing long-term engagement - Within a year of instructor-led workshops, we presented 106 workshops, reaching 2,580 participants at 40 (mostly high school) institutions. We experienced strong student engagement and encouraging feedback from both teachers and students. However, we struggled in getting students to opt into advanced programming, which was our behavioral proxy for further engagement. By the end of April, {SHIC} outreach will function in minimal form, requiring very little staff time - Over the next two months, our team will gradually wind down delivered workshops at schools. We plan on maintaining a website with resources and fielding inquiries through a contact form for those who are looking for information on how best to implement {EA} education.The most promising elements of {SHIC} may be incorporated into other high-impact projects - The {SHIC} curriculum could likely be repurposed for other high-impact projects within the wider Rethink Charity umbrella. For example, it could be a tool for engaging potential high-net-worth donors, or as content to provide local group leaders.We believe in the potential of educational outreach and hope to revisit this in the future - While we acknowledge the possibility that poor attendance at advanced workshops is indicative of general interest level in our program and/or {EA} in general, it's also possible that the methods we used to facilitate long term engagement were inadequate. We think that under the right circumstances, educational outreach could be more fruitful.{SHIC} will release an exhaustive evaluation of our experience with educational outreach in the coming months.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/3HaXa7dtu86NQNEZJ/shic-will-suspend-outreach-operations},
	journaltitle = {Effective Altruism Forum},
	author = {Bullock, Baxter and Low, Catherine},
	urldate = {2022-01-16},
	date = {2019-03-07},
	file = {~/Google Drive/library-pdf/Bullock2019SHICWillSuspend.pdf;~/Google Drive/library-html/shic-will-suspend-outreach-operations.html}
}

@book{Burke1790ReflectionsRevolutionFrance,
	database = {Tlön},
	location = {London},
	langid = {english},
	title = {Reflections on the Revolution in France},
	publisher = {James Dodsley},
	author = {Burke, Edmund},
	date = 1790,
	file = {~/Google Drive/library-pdf/Burke1790ReflectionsRevolutionFrance.pdf}
}

@online{Burt20212021ProgressChallenging,
	database = {Tlön},
	title = {2021: progress during challenging times},
	abstract = {Target Malaria’s scientific progress and global health impact campaign for 2020 is reported. The COVID-19 pandemic led to adaptations for scientific research continuity and safety measures. WHO officially supported research on genetically modified mosquitoes for vector-borne disease control. Imperial College London successfully created a gene drive mosquito strain with male bias, and modeling indicated the successful use of female sterility modification. The team at Polo d’Innovazione di Genomica, Genetica e Biologia determined the short lifespan of an autosomal male bias strain, Target Malaria Burkina Faso released sterile male mosquitoes for monitoring and submitted a report to their regulatory agency, Target Malaria Cape Verde launched its project, Target Malaria Ghana paused its studies due to COVID restrictions, Target Malaria Mali concluded its work and engaged with stakeholders and the community, and Target Malaria Uganda began studying a non-GM mosquito variant. Target Malaria focused on stakeholder engagement, organizing acceptance and consent workshops to ensure community collaboration, a vital component of its work. – AI-generated abstract.},
	langid = {english},
	url = {https://targetmalaria.org/2021-progress-during-challenging-times/},
	journaltitle = {Target Malaria},
	author = {Burt, Austin},
	date = {2021-01-18},
	file = {~/Google Drive/library-pdf/Burt20212021ProgressChallenging.pdf}
}

@online{Bush2013LowhangingFruitImproving,
	database = {Tlön},
	title = {Low-hanging fruit: Improving wikipedia entries},
	abstract = {Wikipedia entries are often people's first impressions of topics and can even influence a person's decision to pursue related productive resources. Improving Wikipedia entries about topics of particular interest to people who frequent LessWrong, such as rationality, existential risks, and decision theory, could positively impact people's perception of these topics and, thus, promote more productive actions. Hence, the author decides to improve Wikipedia entries on such topics and calls for help to make this a collaborative project. – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/eP5ACNfhivJRccwu3/low-hanging-fruit-improving-wikipedia-entries},
	journaltitle = {{LessWrong}},
	author = {Bush, Lance S.},
	date = {2013-07-23},
	file = {~/Google Drive/library-pdf/Bush2013LowhangingFruitImproving.pdf}
}

@online{Buterin2019QuadraticVotingSortition,
	database = {Tlön},
	title = {Quadratic voting with sortition},
	abstract = {In order to address the issue of uninformed votes in elections, the paper proposes a combination of quadratic voting with sortition. Sortition is a method of selecting a representative subset of participants to vote, giving each participant a larger chance of influencing the result. By allowing individuals with stronger preferences to use their votes to have a greater impact, this approach aims to incentivize voters to deeply engage with the issues and make informed decisions. The paper explores three different schemes for combining quadratic voting and sortition, each with its own advantages and disadvantages. The author argues that sortition can help to mitigate the influence of random noise and ensure that a broad range of preferences is represented in the final outcome. – AI-generated abstract.},
	url = {https://ethresear.ch/t/quadratic-voting-with-sortition/6065},
	journaltitle = {Ethereum Research},
	author = {Buterin, Vitalik},
	urldate = {2022-06-20},
	date = {2019-08-31},
	langid = {english},
	file = {~/Google Drive/library-pdf/Buterin2019QuadraticVotingSortition.pdf;~/Google Drive/library-html/6065.html}
}

@online{Bye2019HowMuchYour,
	database = {Tlön},
	title = {How much is your time worth?},
	abstract = {Productivity loss can be substantial due to heat in the workplace with one study positing an average drop of 2.6% in productivity for every degree increase beyond 24°C. Purchasing an air conditioner (AC) has been examined to determine whether it is a worthwhile productivity investment.

The article uses a survey to determine an individual's hourly income as a method of equating increased productivity to financial gain. Using this method, the article finds that in areas with extended periods of high heat, the purchase and use of an AC is a worthwhile financial investment as opposed to other time-saving options such as laundry or meal delivery services. Additionally, the author uses research that documents gains in sleep quality as another benefit of AC usage. – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/y5RoNDPcfJqm3vfQA/how-much-is-your-time-worth},
	shorttitle = {How Much is Your Time Worth?},
	journaltitle = {{LessWrong}},
	author = {Bye, Lynette},
	urldate = {2022-04-19},
	date = {2019-09-02},
	file = {~/Google Drive/library-pdf/Bye2019HowMuchYour.pdf;~/Google Drive/library-html/how-much-is-your-time-worth.html}
}

@online{Byrnes2019DefenseOracleTool,
	database = {Tlön},
	title = {In defense of oracle ("tool") {AI} research},
	abstract = {The article defends the idea of non-self-improving AI systems designed to only answer questions without taking actions (oracle AIs) as opposed to agent AIs that additionally have the capacity to take actions, send emails, and control actuators. The author claims that oracle AIs are safer and easier to coordinate and proposes that humanity should focus on developing them rather than riskier agent AIs. The author also argues that the coordination problem (i.e., the problem of preventing powerful AIs from endangering humanity) is not a compelling argument against oracle AIs because it is harder to solve the coordination problem with agent AIs, because there will always be incentives for people to develop more powerful and less safe AGIs, and because coordination will have to be solved at some point in AI development anyway. – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/AKtn6reGFm5NBCgnd/in-defense-of-oracle-tool-ai-research},
	journaltitle = {{LessWrong}},
	author = {Byrnes, Steve},
	date = {2019-08-07},
	file = {~/Google Drive/library-pdf/Byrnes2019DefenseOracleTool.pdf}
}

@report{Calvert2019CauseAreaReport,
	database = {Tlön},
	title = {Cause area report: education},
	langid = {english},
	url = {https://founderspledge.com/research/fp-education},
	institution = {Founders Pledge},
	author = {Calvert, Callum},
	date = {2019-10},
	file = {~/Google Drive/library-pdf/Calvert2019CauseAreaReport.pdf}
}

@book{CamposBoralevi1984BenthamOppressed,
	database = {Tlön},
	location = {Berlin},
	langid = {english},
	title = {Bentham and the oppressed},
	isbn = {3-11-009974-8},
	publisher = {Walter de Gruyter},
	author = {Campos Boralevi, Lea},
	date = 1984,
	doi = {10.1515/9783110869835},
	file = {~/Google Drive/library-pdf/CamposBoralevi1984BenthamOppressed.pdf}
}

@online{Caplan2011IdeologicalTuringTest,
	database = {Tlön},
	title = {The ideological turing test},
	abstract = {Liberals understand conservative thought more accurately than conservatives understand liberal thought. This is a symptom of objectivity and wisdom because someone who can explain a position that they don't necessarily agree with understands it better than someone who cannot. If someone can explain a position that they don't agree with and end up agreeing with it, that position is likely more correct. This is called passing the "ideological Turing test." Liberals can more accurately explain conservative thought than conservatives can explain liberal thought. This can be tested and has been challenged. – AI-generated abstract.},
	langid = {english},
	url = {https://www.econlib.org/archives/2011/06/the_ideological.html},
	journaltitle = {Econlog},
	author = {Caplan, Bryan},
	date = {2011-06-20},
	file = {~/Google Drive/library-pdf/Caplan2011IdeologicalTuringTest.pdf}
}

@online{Caplan2012DealDelusion,
	database = {Tlön},
	title = {The deal delusion},
	abstract = {Examining Robin Hanson's doctrine of "dealism," which upholds deals and agreements as morally superior and practicable for mutual benefit, this article argues that the approach is actually impractical and lacks efficacy. The author contends that dealism remains a far doctrine, given Robin Hanson's history of unsuccessful policy proposals and his neglect of human nature's resistance to non-conventional ideas and deals. – AI-generated abstract},
	langid = {english},
	url = {https://www.econlib.org/archives/2012/02/the_deal_delusi.html},
	journaltitle = {{EconLog}},
	author = {Caplan, Bryan},
	date = {2012-02-06},
	file = {~/Google Drive/library-pdf/Caplan2012DealDelusion.pdf;~/Google Drive/library-pdf/Caplan2012DealDelusion.pdf}
}

@online{Caplan2014ReadScottAlexander,
	database = {Tlön},
	title = {Read Scott Alexander},
	abstract = {Bryan Caplan introduces Scott Alexander, an original and engaging writer known for his calm, measured, and inter-disciplinary approach.  Caplan draws attention to three of Alexander's arguments: (1) True tolerance is not simply feeling benign neglect towards outgroups; (2) Being against Fox News may be less effective than considered opposition to more dangerous groups like ISIS; (3) Neoreactionaries, often dismissed as bizarre, may be less anachronistic than Enlightenment proponents who claim a permanent hold on human affairs. – AI-generated abstract.},
	langid = {english},
	url = {https://www.econlib.org/archives/2014/10/read_scott_alex.html},
	journaltitle = {{EconLog}},
	author = {Caplan, Bryan},
	date = {2014-10-28},
	file = {~/Google Drive/library-pdf/Caplan2014ReadScottAlexander.pdf}
}

@online{Caplan2016MissingMoods,
	database = {Tlön},
	title = {The missing moods},
	abstract = {People’s moods can indicate if their views are reliable. This is because certain views should cause certain emotions, and if a proponent of a position does not display the expected emotion, that is a sign that their position is less well thought out than it could be. This insight is most useful against popular positions whose reasonable moods are almost never expressed, especially in the presence of hawks, immigration restrictionists, and labor market regulation supporters. Nevertheless, this fallibility can also be seen in anti-positions, e.g., against pacifism and libertarianism. Moreover, different emotions are appropriate for different contexts, meaning that this method of assessing the reliability of a view is context-sensitive. – AI-generated abstract.},
	langid = {english},
	url = {https://www.econlib.org/archives/2016/01/the_invisible_t.html},
	journaltitle = {{EconLog}},
	author = {Caplan, Bryan},
	date = {2016-01},
	file = {~/Google Drive/library-pdf/Caplan2016MissingMoods.pdf}
}

@online{Caplan2020MyCompleteBet,
	database = {Tlön},
	title = {My complete bet wiki},
	abstract = {By popular demand, I’ve created a publicly-viewable wiki for my Complete Bet Inventory. From now on, I’ll edit it when I make new bets or when old bets resolve.},
	langid = {english},
	url = {https://www.econlib.org/my-complete-bet-wiki/},
	journaltitle = {Econlib},
	author = {Caplan, Bryan},
	date = {2020-01-02}
}

@online{Caplan2022OpenBordersUltraeffective,
	database = {Tlön},
	title = {Open borders as ultra-effective altruism},
	abstract = {This article argues that open borders are a highly effective means of alleviating global poverty and increasing global production. It claims that immigration restrictions prevent people from moving to countries with higher productivity, leading to a loss of economic opportunities. The article cites evidence that open borders would double GWP and that the gains would be broadly beneficial. It addresses concerns about swamping by arguing that migration would begin slowly and snowball over time, giving societies time to adjust. The article concludes by arguing that open borders are an excellent cause for effective altruists to support due to their demonstrable gains and low probability of negative consequences – AI-generated abstract.},
	url = {https://betonit.substack.com/p/open-borders-as-ultra-effective-altruism},
	journaltitle = {Bet On It},
	author = {Caplan, Bryan},
	urldate = {2022-03-31},
	date = {2022-03-24},
	langid = {english},
	file = {~/Google Drive/library-pdf/Caplan2022OpenBordersUltraeffective.pdf;~/Google Drive/library-html/open-borders-as-ultra-effective-altruism.html}
}

@report{Capriati2018CauseAreaReport,
	database = {Tlön},
	title = {Cause area report: Corporate campaigns for animal
                  welfare},
	langid = {english},
	url = {https://founderspledge.com/research/fp-animal-welfare},
	institution = {Founders Pledge},
	author = {Capriati, Marinella},
	date = {2018-11-05},
	file = {~/Google Drive/library-pdf/Capriati2018CauseAreaReport.pdf}
}

@online{Capriati2019ConversationMartinaBjorkman,
	database = {Tlön},
	title = {Conversation with Martina Björkman Nyqvist},
	abstract = {This article discusses the question of whether it is possible to be completely alienated from what one should do, even with a full understanding of reality and the consequences of one's actions. The author considers three different meta-ethical views on this question: externalist normative realism, internalist normative realism, and internalist anti-realism. The author argues that the first view, externalist normative realism, allows for the possibility of such alienation, while the other two views do not. The author also argues that this question is important for understanding wholeheartedness in ethical life. – AI-generated abstract.},
	url = {https://docs.google.com/document/d/1SxD4pMQbh_0eAsr-4oMoDNFTTnAJT4ShkjOsPtvEH-o/edit?usp=embed_facebook},
	journaltitle = {{GiveWell}},
	author = {Capriati, Marinella},
	date = {2019-08-19},
	langid = {english},
	file = {~/Google Drive/library-pdf/Capriati2019ConversationMartinaBjorkman.pdf;~/Google Drive/library-html/edit.html}
}

@online{Carey2022CommentsJacyReese,
	database = {Tlön},
	title = {Comments on Jacy Reese Anthis' "Some early history of
                  {EA}"},
	abstract = {The piece could give the reader the impression that Jacy, Felicifia and {THINK} played a comparably important role to the Oxford community, Will, and Toby, which is not the case. I'll follow the chronological structure of Jacy's post, focusing first on 2008-2012, then 2012-2021. Finally, I'll discuss "founders" of {EA}, and sum up.},
	url = {https://forum.effectivealtruism.org/posts/ZbdNFuEP2zWN5w2Yx/ryancarey-s-shortform?commentId=ag9TgbFENCmj34vDk},
	journaltitle = {Effective Altruism Forum},
	author = {Carey, Ryan},
	urldate = {2022-07-02},
	date = {2022-07-02},
	langid = {english},
	file = {~/Google Drive/library-pdf/Carey2022CommentsJacyReese.pdf;~/Google Drive/library-html/ryancarey-s-shortform.html}
}

@collection{Cargill2021LongViewEssays,
	database = {Tlön},
	location = {London},
	langid = {english},
	title = {The long view: Essays on policy, philanthropy, and the
                  long-term future},
	isbn = {978-0-9957281-8-9},
	publisher = {First},
	editor = {Cargill, Natalie and John, Tyler},
	date = 2021,
	file = {~/Google Drive/library-pdf/Cargill2021LongViewEssays.pdf}
}

@online{Carlshulman2016DonorFaq,
	database = {Tlön},
	langid = {english},
	abstract = {Suppose that Alice is trying to figure out how to do the most good with her donation of \$1,000 this giving season, and can spend various kinds of resources to improve her decision. Unfortunately, many investments that could improve the decision quality would impose costs that are large relative to her donation: spending hundreds of hours (whether her own, those of charity staff, or of hired evaluators) investigating opportunities will cost more than her donation amount. She will also be limited in the projects she can fund: whereas a large funder can attract proposals for new projects, and fund a new position or startup organization, she seems to be limited to contributing to existing public opportunities.One solution to this problem would be for Alice to work with Bob, a large donor, to construct a 'donor lottery.' Alice donates her \$1,000 to Bob's donor-advised fund, or {DAF}. Then Alice and Bob consult a random number generator to determine how to recommend donation allocations to the {DAF}. For example, they might plan that with 1/100 probability Alice would get to recommend the allocation of \$100,000 from the {DAF}, while with 99/100 probability Bob allocates the {DAF} without input from Alice.},
	author = {Shulman, Carl},
	date = {2016-12-07},
	shorttitle = {Donor lotteries},
	title = {Donor lotteries: demonstration and {FAQ}},
	journaltitle = {Effective Altruism Forum},
	url = {https://forum.effectivealtruism.org/posts/WvPEitTCM8ueYPeeH/donor-lotteries-demonstration-and-faq},
	urldate = {2022-08-31}
}

@online{Carlsmith2020AlienationMetaethicsIt,
	database = {Tlön},
	title = {Alienation and meta-ethics (or: is it possible you
                  should maximize helium?)},
	abstract = {This article presents the Effective Altruism London Strategy, which aims to coordinate and support people interested in effective altruism in London. The strategy focuses on areas such as coordination, community-wide activities, and meta activities, while deprioritizing retreats and bespoke activities. The article argues that the focus on coordination will maximize engagement and value for participants. – AI-generated abstract.},
	url = {https://handsandcities.com/2020/12/20/alienation-and-meta-ethics-or-is-it-possible-you-should-maximize-helium/},
	shorttitle = {Alienation and meta-ethics (or},
	journaltitle = {Hands and Cities},
	author = {Carlsmith, Joseph},
	urldate = {2021-10-04},
	date = {2020-12-20},
	langid = {english},
	file = {~/Google Drive/library-pdf/Carlsmith2020AlienationMetaethicsIt.pdf;~/Google Drive/library-html/alienation-and-meta-ethics-or-is-it-possible-you-should-maximize-helium.html}
}

@online{Carlsmith2020HowMuchComputational,
	database = {Tlön},
	title = {How much computational power does it take to match the
                  human brain?},
	abstract = {Open Philanthropy is interested in when AI systems will be able to perform various tasks that humans can perform (“AI timelines”). To inform our thinking, I investigated what evidence the human brain provides about the computational power sufficient to match its capabilities. This is the full report on what I learned.},
	langid = {english},
	url = {https://www.openphilanthropy.org/brain-computation-report},
	journaltitle = {Open Philanthropy},
	author = {Carlsmith, Joseph},
	date = {2020-09-11},
	file = {~/Google Drive/library-pdf/Carlsmith2020HowMuchComputational.pdf}
}

@online{Carlsmith2021CareDemandingness,
	database = {Tlön},
	title = {Care and demandingness},
	abstract = {People sometimes object to moral claims on the grounds that their implications would be too demanding. But analogous objections make little sense in empirical and prudential contexts. I find this contrast instructive. Some ways of understanding moral obligation suggest relevant differences from empiricism and prudence. But the more we see moral life as continuous with caring about stuff in general, the less it makes sense to expect “can’t-be-too-demanding” guarantees.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/P52eSwfmwaN2uwrcM/care-and-demandingness},
	journaltitle = {Effective Altruism Forum},
	author = {Carlsmith, Joseph},
	date = {2021-03-08},
	file = {~/Google Drive/library-pdf/Carlsmith2021CareDemandingness.pdf}
}

@online{Carlsmith2021PowerseekingAIExistential,
	database = {Tlön},
	title = {Is power-seeking {AI} an existential risk?},
	abstract = {This writing introduces a visual model named "skyscrapers and madmen" to explicate expected utility maximization (EUM), where the skyscrapers represent outcomes of actions, and the height of a skyscraper indicates the utility of an action's outcome. It explains that EUM often results in predictably losing situations, where the probabilities of success are low but the potential benefits are high. The article argues that these predictably losing situations can be justified by the fact that they maximize expected utility in the long run. However, the article also critiques some arguments in favor of EUM, asserting that they are either insufficient or incomplete. – AI-generated abstract.},
	url = {https://docs.google.com/document/d/1smaI1lagHHcrhoi6ohdq3TYIZv0eNWWZMPEy8C8byYg/edit?usp=embed_facebook},
	shorttitle = {Is power-seeking {AI} an existential risk?},
	journaltitle = {Open Philanthropy},
	author = {Carlsmith, Joseph},
	urldate = {2022-04-19},
	date = {2021-04},
	langid = {english},
	file = {~/Google Drive/library-pdf/Carlsmith2021PowerseekingAIExistential.pdf;~/Google Drive/library-html/edit.html;~/Google Drive/library-html/Carlsmith2021PowerseekingAIExistential.html}
}

@online{Carlsmith2022ExpectedUtilityPart,
	database = {Tlön},
	title = {On expected utility, part 1: Skyscrapers and madmen},
	abstract = {This work examines three main theorems seeking to demonstrate the rationality of expected utility maximization (EUM). The first is von Neumann-Morgenstern (vNM), which assumes probability assignments and derives EUM as a conclusion of four axioms. The second is a theorem relying on a separability-additivity argument. And the third is a direct axiomatization of EUM by Peterson (2017). The article offers general overviews of these theorems, provides informal arguments for how the underlying reasoning works, and references more formal resources for further study. – AI-generated abstract.},
	url = {https://handsandcities.com/2022/03/16/on-expected-utility-part-1-skyscrapers-and-madmen/},
	shorttitle = {On expected utility, part 1},
	journaltitle = {Hands and Cities},
	author = {Carlsmith, Joseph},
	urldate = {2022-04-19},
	date = {2022-03-16},
	langid = {english},
	file = {~/Google Drive/library-pdf/Carlsmith2022ExpectedUtilityPart.pdf;~/Google Drive/library-html/on-expected-utility-part-1-skyscrapers-and-madmen.html}
}

@online{Carlsmith2022ExpectedUtilityPartb,
	database = {Tlön},
	title = {On expected utility, part 3: {VNM}, separability, and
                  more},
	abstract = {This article considers the philosophical argument that we should be assigning significant credence to the hypothesis that we are living in a computer simulation. It distinguishes between two types of simulation arguments: Type 1 arguments, which rely on our standard story about the world to show that it is unlikely that we are not living in a simulation, and Type 2 arguments, which rely instead on a specific claim about reasonable priors over structurally similar worlds. The article argues that Type 1 arguments are flawed, both because they are subject to counterexamples and because they rely on an overly narrow conception of what counts as “knowing” something. In contrast to Type 1 arguments, Type 2 arguments are less vulnerable to these objections and provide a more solid basis for assigning credence to the hypothesis that we are living in a simulation. However, they also face a number of challenges, including the difficulty of assessing “structural similarity” and the risk of committing an “epistemic pascal’s mugging.” Overall, the article argues that while simulation arguments are not decisive, they do provide some reason for taking seriously the possibility that we are living in a simulation and that further research on this topic is warranted. – AI-generated abstract.},
	url = {https://handsandcities.com/2022/03/21/on-expected-utility-part-3-vnm-separability-and-more/},
	shorttitle = {On expected utility, part 3},
	journaltitle = {Hands and Cities},
	author = {Carlsmith, Joseph},
	urldate = {2022-04-19},
	date = {2022-03-22},
	langid = {english},
	file = {~/Google Drive/library-pdf/Carlsmith2022ExpectedUtilityPartb.pdf;~/Google Drive/library-html/on-expected-utility-part-3-vnm-separability-and-more.html}
}

@online{Carlsmith2022SimulationArguments,
	database = {Tlön},
	title = {Simulation arguments},
	abstract = {There is strong evidence that increasing tobacco taxation reduces tobacco consumption and improves health outcomes. Studies show that government implementation of tobacco taxation is highly cost-effective. Lobbying low and middle-income countries (LMICs) to increase tobacco taxes could be a highly effective intervention, but there is a lack of research on key lobbying-related factors, such as success rates and time frames. A possible implementation plan involves hiring local lobbyists and tobacco control experts to implement legislation at the national level, with a focus on LMICs where tobacco control efforts are lacking and political will may exist. However, there are concerns about the difficulty of testing and evaluating lobbying efforts, the risk of legal challenges from tobacco companies, and the need for specialized skills that limit flexibility. – AI-generated abstract.},
	url = {https://handsandcities.com/2022/02/18/simulation-arguments/},
	journaltitle = {Hands and Cities},
	author = {Carlsmith, Joseph},
	urldate = {2022-03-10},
	date = {2022-02-18},
	langid = {english},
	file = {~/Google Drive/library-pdf/Carlsmith2022SimulationArguments.pdf;~/Google Drive/library-html/simulation-arguments.html}
}

@online{Carlsmith2023CuidadoExigencia,
	database = {Tlön},
	date = {2023},
	title = {Cuidado y exigencia},
	author = {Carlsmith, Joseph},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Carlsmith2021CareDemandingness}
}

@online{Carlsmith2023OrientandonosHaciaFuturo,
	database = {Tlön},
	keywords = {largoplacismo, futuro a largo plazo},
	date = {2023},
	langid = {spanish},
	author = {Carlsmith, Joseph},
	journaltitle = {Biblioteca Altruismo Eficaz},
	title = {Orientándonos hacia el futuro a largo plazo},
	translator = {Arellano, Paulina},
	translation = {Carlsmith2017OrientingLongtermFuture}
}

@online{Carter2018ScienceBehindUniversal,
	database = {Tlön},
	date = {2018},
	abstract = {In this talk, Sam Carter of J-{PAL} and Joe Huston of {GiveDirectly} talk about the state of the evidence on universal basic income: what we know, what we'd like to know, and what we're currently in the process of learning.},
	journaltitle = {Effective Altruism Global},
	author = {Carter, Sam and Houston, Joe},
	langid = {spanish},
	shorttitle = {The science behind Universal Basic Income},
	timestamp = {2023-04-24 12:38:55 (GMT)},
	title = {The science behind Universal Basic Income},
	url = {https://www.youtube.com/watch?v=lyOAANc7hgU},
	urldate = {2023-04-24}
}

@article{Carus2017CenturyOfBiological,
	database = {Tlön},
	author = {Carus, W. Seth},
	title = {A Century of Biological-Weapons Programs (1915-2015):
                  Reviewing the Evidence},
	volume = 24,
	number = 1,
	pages = {129–153},
	doi = {10.1080/10736700.2017.1385765},
	url = {https://www.tandfonline.com/doi/full/10.1080/10736700.2017.1385765},
	date = {2017-01-02},
	issn = {1073-6700, 1746-1766},
	journaltitle = {The Nonproliferation Review},
	langid = {english},
	shortjournal = {The Nonproliferation Review},
	shorttitle = {A century of biological-weapons programs (1915-2015)},
	timestamp = {2023-06-05 16:23:53 (GMT)},
	urldate = {2023-06-05}
}

@online{Cassidy2022EAResilienceCatastrophes,
	database = {Tlön},
	title = {{EA} resilience to catastrophes \& {ALLFED}’s case
                  study},
	abstract = {This is a case study on how The Alliance to Feed The Earth In Disasters ({ALLFED}) has activated its response capabilities due to the invasion of Ukraine (with its potential for global food systems risks and nuclear war). We believe that resilience and response should be built up in the {EA} community. In this post, we list some ways in which {ALLFED} can support such resilience-building.We summarise actions that we have been taking over the last 3 years in order to build up our own operational resilience and response capabilities.We look at some relevant organisational background and give an overview of {ALLFED} as a ‘think-and-do’ tank, to show how it informs our overall actions and thinking.We share what would be useful to us, notably:People connected to search engine and social media companies to help develop messaging related to {GCRs} (Global Catastrophic Risks)People interested in becoming a part of a Rapid Response Task Force (provisional name), ideally outside of major {NATO} cities, sign up {herePeople} interested in working with us to develop access to resilient foods in the event of a catastrophe (anything from accessibility of information to more resilient infrastructure)Access to medium-term funding (for 2023 onwards), so that we are able to fully focus on our core work (rather than also expanding a portion of our energies on fundraising).},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/iW7LjvuSmDEGF5nei/ea-resilience-to-catastrophes-and-allfed-s-case-study},
	journaltitle = {Effective Altruism Forum},
	author = {Cassidy, Sonia},
	urldate = {2022-03-24},
	date = {2022-03-23},
	file = {~/Google Drive/library-pdf/Cassidy2022EAResilienceCatastrophes.pdf;~/Google Drive/library-html/ea-resilience-to-catastrophes-and-allfed-s-case-study.html}
}

@online{Cassidy2022WhatCanWe,
	database = {Tlön},
	title = {What can we learn from a short preview of a
                  super-eruption and what are some tractable ways of
                  mitigating},
	abstract = {This article examines the devastating effects that a super-eruption would have on the world, citing the recent eruption of Hunga Tonga-Hunga Ha’apai as a preview of what could happen. The authors argue that we are currently unprepared for such an event, as evidenced by the lack of monitoring equipment and emergency plans in place. They propose several steps to mitigate the risks associated with super-eruptions, including identifying vulnerable areas, enhancing monitoring and surveillance, and developing technologies to remove sulfur aerosols from the atmosphere. – AI-generated abstract.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/ziGdcbZWCssCGFtXx/what-can-we-learn-from-a-short-preview-of-a-super-eruption},
	journaltitle = {Effective Altruism Forum},
	author = {Cassidy, Mike and Mani, Lara},
	urldate = {2022-03-09},
	date = {2022-02-03},
	file = {~/Google Drive/library-pdf/Cassidy2022WhatCanWe.pdf;~/Google
                  Drive/library-html/What can we learn from a short
                  preview of a super-eruption and what are some
                  tractable ways of mitigating it - EA Forum:what-can-we-learn-from-a-short-preview-of-a-super-eruption.html}
}

@online{Caviola2017AgainstNaiveEffective,
	database = {Tlön},
	date = {2017-11-20},
	abstract = {I argue that effective altruism poses dangers by allowing for the possibility to be misinterpreted or applied in an unreflective way. Human cognition has its limits and an explicit attempt of doing the most good can therefore sometimes go wrong. An unreflective application of effective altruism can, for example, lead to a disregard of important interpersonal values, bad life choices and psychological harm, unbalanced views, and in the worst case to forms of fanaticism. Being aware of these dangers and their underlying psychological biases can help us develop respective countermeasures.},
	journaltitle = {{EAGx} Berlin},
	author = {Caviola, Lucius},
	langid = {english},
	timestamp = {2023-07-20 19:35:10 (GMT)},
	title = {Against naive effective altruism},
	url = {https://www.youtube.com/watch?v=-2oRgxxafXk},
	urldate = {2023-07-20}
}

@online{Caviola2022MostStudentsWho,
	database = {Tlön},
	title = {Most students who would agree with {EA} ideas haven't
                  heard of {EA} yet (results of a large-scale survey)},
	abstract = {In a large-scale survey we found that only 7.4\% of New York University students knew what effective altruism ({EA}) is. At the same time, 8.8\% were extremely sympathetic to {EA}. These students agreed with basic {EA} ideas when presented with a short introduction, showed interest in learning more about it, and scored highly on our ‘effectiveness-focus’ and ‘expansive altruism’ measures. Interestingly, these {EA}-sympathetic students were largely ignorant about {EA}; only 14.5\% knew about it before the survey. We estimate that, in total, 7.5\% of {NYU} students do not know of {EA} but would be very sympathetic to the basic ideas. These findings could have important implications for the scaling of outreach but need to be interpreted carefully.
Note: This post is long because of the Detailed results section. Most readers may just want to focus on the Key results and Conclusions sections.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/mNRNWkFBZ2K6SHD8a/most-students-who-would-agree-with-ea-ideas-haven-t-heard-of},
	journaltitle = {Effective Altruism Forum},
	author = {Caviola, Lucius and Morrisey, Erin and Lewis, Joshua},
	urldate = {2022-05-20},
	date = {2022-05-19},
	file = {~/Google Drive/library-pdf/Caviola2022MostStudentsWho.pdf;~/Google Drive/library-html/most-students-who-would-agree-with-ea-ideas-haven-t-heard-of.html}
}

@online{CenterforEmergingRiskResearch2021Transparency,
	database = {Tlön},
	title = {Transparency},
	langid = {english},
	url = {https://emergingrisk.ch/transparency/},
	journaltitle = {Center for Emerging Risk Research},
	author = {{Center for Emerging Risk Research}},
	date = 2021
}

@online{CenterforHuman-CompatibleArtificialIntelligence2021,
	database = {Tlön},
	title = {About},
	langid = {english},
	url = {https://humancompatible.ai/about},
	journaltitle = {Center for Human-Compatible Artificial Intelligence},
	author = {{Center for Human-Compatible Artificial Intelligence}},
	date = 2021
}

@online{CenterforSpaceGovernance2022AnnouncingCenterSpace,
	database = {Tlön},
	title = {Announcing the Center for Space Governance},
	abstract = {We are excited to be launching the Center for Space Governance, a non-profit research organization dedicated to exploring current, emerging, and future issues in space policy. We aim to define and map the field of space governance, advancing effective solutions to address the greatest challenges and opportunities for humanity’s future in space.},
	url = {https://forum.effectivealtruism.org/posts/5HEFJKHGpcT49RK6y/announcing-the-center-for-space-governance},
	journaltitle = {Effective Altruism Forum},
	author = {{Center for Space Governance}},
	urldate = {2022-07-11},
	date = {2022-07-10},
	langid = {english},
	file = {~/Google Drive/library-pdf/CenterforSpaceGovernance2022AnnouncingCenterSpace.pdf;~/Google Drive/library-html/announcing-the-center-for-space-governance.html}
}

@online{CenteronLong-TermRisk2020Transparency,
	database = {Tlön},
	title = {Transparency},
	abstract = {The Center on Long-Term Risk ({CLR}) is committed to being as transparent as possible about its activities and learning from its mistakes. This page gathers relevant information related to transparency.},
	langid = {english},
	url = {https://longtermrisk.org/transparency},
	journaltitle = {Center on Long-Term Risk},
	author = {{Center on Long-Term Risk}},
	date = {2020-11}
}

@online{Champandard2014MonteCarloTree,
	journaltitle = {AiGameDev.com},
	author = {Champandard, Alex J.},
	langid = {english},
	database = {Tlön},
	date = {2014-08-12},
	shorttitle = {Monte-Carlo Tree Search in {TOTAL} {WAR}},
	timestamp = {2023-06-28 14:05:04 (GMT)},
	title = {Monte-Carlo Tree Search in {TOTAL} {WAR}: {ROME}
                  {II}'s Campaign {AI}},
	url = {https://web.archive.org/web/20190622065843/http://aigamedev.com/open/coverage/mcts-rome-ii/},
	urldate = {2023-06-28}
}

@online{Chappell2012CounterexamplesToConsequentialism,
	database = {Tlön},
	file = {~/Google Drive/library-pdf/Chappell2012CounterexamplesToConsequentialism.pdf;~/Google Drive/library-html/Chappell2012CounterexamplesToConsequentialism.html},
	abstract = {This philosophical work presents objections to consequentialism and the responses to these objections. The first type of objection presents harmful acts that result in greater long-term benefit and argues that consequentialism should endorse these acts. The response is that the agent performing the harmful act is likely not warranted in thinking that their particular performance of a typically disastrous act would avoid being disastrous and that if there is a reliable guarantee that no long-term harm will be done, then the act can be seen as morally acceptable. The second type of objection argues that an act that would normally have bad consequences could, in a specific instance, have overwhelmingly positive consequences such that it should be considered a morally good act by consequentialists. The response is that although the act may be morally acceptable in this specific instance, it is unlikely to be the best course of action available to a committed consequentialist and that the agent may still possess a flaw in their character if they would prefer to cause harm than to save lives by less harmful means. – AI-generated abstract.},
	url = {https://www.philosophyetc.net/2012/08/counterexamples-to-consequentialism.html},
	langid = {english},
	date = {2012-08-23},
	journaltitle = {Philosophy, et cetera},
	title = {Counterexamples to consequentialism},
	author = {Chappell, Richard Yetter},
	timestamp = {2023-08-04 16:13:49 (GMT)}
}

@online{Chappell2016EffectiveAltruismRadical,
	database = {Tlön},
	title = {Effective altruism, radical politics and radical
                  philanthropy},
	abstract = {Effective Altruism (EA) is a social movement based on two core directives: a commitment to making the world a better place through altruism, and a commitment to doing so as effectively and efficiently as possible. Opponents of EA primarily allege that it makes incorrect assumptions about the means to an altruistic end. Typically, they propose the abolishment of modern capitalism and its replacement with a radically different economic system. However, since most within the EA movement agree that any means are acceptable provided they maximize altruistic outcomes, such arguments by opponents are more indicative of disagreements about the most effective means to that end rather than a genuine disagreement about altruism itself. – AI-generated abstract.},
	langid = {english},
	url = {https://www.philosophyetc.net/2016/04/effective-altruism-radical-politics-and.html},
	journaltitle = {Philosophy, et cetera},
	author = {Chappell, Richard Yetter},
	date = {2016-04-20},
	file = {~/Google Drive/library-pdf/Chappell2016EffectiveAltruismRadical.pdf}
}

@online{Chappell2020ParfitEthicsManuscript,
	database = {Tlön},
	title = {Parfit's Ethics (manuscript)},
	abstract = {This short book manuscript summarizes the important insights of Derek Parfit on various ethical topics. The text covers rationality and objectivity, distributive justice, the complex relation between one’s character and the consequences of one's actions, Parfit's Triple Theory and its relation to the problem of personal identity, and some central questions of population ethics. – AI-generated abstract.},
	langid = {english},
	url = {https://www.philosophyetc.net/2020/08/parfits-ethics-manuscript.html},
	journaltitle = {Philosophy, et cetera},
	author = {Chappell, Richard Yetter},
	urldate = {2022-01-10},
	date = {2020-08-04},
	file = {~/Google Drive/library-pdf/Chappell2020ParfitEthicsManuscript.pdf;~/Google Drive/library-html/parfits-ethics-manuscript.html}
}

@online{Chappell2021RulingOutHeliummaximizing,
	database = {Tlön},
	title = {Ruling out helium-maximizing},
	abstract = {Externalist accounts of normative truth can be accused of positing the logical possibility of a radical mismatch between one's normative attitudes and the demands of normative reality; this has been termed 'normative alienation.' We may reasonably expect that our own attitudes are not radically wrong, but we might still worry that others could be. Internalist accounts can outright deny the logical possibility of radical normative alienation, but this comes at the cost of being committed to claims that seem implausible, such as the claim that no fool could really want to maximize helium. By rejecting internalism, externalism need not make any concessions on this point. However, the question of normative alienation still lurks. If we are to be externalists and yet maintain that normative reality will not utterly baffle us, the best option is to allow that normative reality might surprise us in a weaker sense: even if we initially hold a view like prioritarianism that we are rightly confident in, it turns out to be slightly mistaken, and yet another theory that we might have reasonably considered turns out to be the correct one. – AI-generated abstract.},
	langid = {english},
	url = {https://www.philosophyetc.net/2021/10/ruling-out-helium-maximizing.html},
	journaltitle = {Philosophy, et cetera},
	author = {Chappell, Richard Yetter},
	urldate = {2021-10-04},
	date = {2021-10-03},
	file = {~/Google Drive/library-pdf/Chappell2021RulingOutHeliummaximizing.pdf;~/Google Drive/library-html/ruling-out-helium-maximizing.html}
}

@online{Chappell2022NewSubstackUtilitarian,
	database = {Tlön},
	title = {New substack on utilitarian ethics: Good Thoughts},
	abstract = {Hi everyone, I just wanted to introduce my new substack as of possible interest to folks here.  I see that Pablo has already shared my post on 'beneficentrism', which is the one most explicitly relevant to {EA}.  Some might also enjoy my latest post on Theory-Driven Applied Ethics, which is basically my attempt to make sense of how utilitarians can do bioethics (or applied ethics more broadly) in an interesting, non-trivial way.  You can also see my debate with Michael Huemer on utilitarianism, with associated reflections on the role of intuitions in moral philosophy.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/5ostvnBiwHXCawycD/new-substack-on-utilitarian-ethics-good-thoughts},
	shorttitle = {New substack on utilitarian ethics},
	journaltitle = {Effective Altruism Forum},
	author = {Chappell, Richard Yetter},
	urldate = {2022-05-10},
	date = {2022-05-09},
	file = {~/Google Drive/library-html/new-substack-on-utilitarian-ethics-good-thoughts.html}
}

@book{Chappell2023IntroduccionAlUtilitarismo,
	url = {https://www.utilitarismo.net},
	address = {Madrid},
	database = {Tlön},
	translation = {Chappell2023IntroductionToUtilitarianism},
	langid = {spanish},
	translator = {Tlön},
	date = {2023},
	title = {Una introducción al utilitarismo},
	author = {Chappell, Richard Yetter and Meissner, Darius and {MacAskill}, William},
	timestamp = {2023-09-18 09:58:51 (GMT)}
}

@online{CharityEntrepreneurship2016TobaccoTaxation,
	database = {Tlön},
	title = {Tobacco taxation},
	abstract = {Lobbying low and middle-income countries to increase tobacco taxes has been suggested as a potential contender for a GiveWell top charity, based on studies showing the cost-effectiveness of tobacco taxation and the high disease burden attributable to tobacco use in low and middle-income countries. The article discusses the potential benefits and challenges of such an intervention, including the need for specialized lobbying skills, the difficulties of testing the effectiveness of lobbying campaigns, the risk of zero-sum competition with tobacco companies, and funding concerns. – AI-generated abstract.},
	url = {https://www.charityentrepreneurship.com/blog/tobacco-taxation},
	journaltitle = {Charity Entrepreneurship},
	author = {{Charity Entrepreneurship}},
	urldate = {2021-07-21},
	date = {2016-04-19},
	langid = {english},
	file = {~/Google Drive/library-pdf/CharityEntrepreneurship2016TobaccoTaxation.pdf;~/Google Drive/library-html/tobacco-taxation.html}
}

@online{CharityEntrepreneurship2021Us,
	database = {Tlön},
	title = {About us},
	abstract = {Charity Entrepreneurship (CE) is a registered charity in England and Wales that supports incubated charities through fiscal sponsorship. Since late 2021, CE has been registered in England and Wales with Charity Number 1195850. CE was previously a project of the Charity Science Foundation of Canada, a registered charity in Canada. – AI-generated abstract.},
	file = {~/Google Drive/library-pdf/CharityEntrepreneurship2021Us.pdf},
	langid = {english},
	url = {https://www.charityentrepreneurship.com/about-us.html},
	journaltitle = {Charity Entrepreneurship},
	author = {{Charity Entrepreneurship}},
	date = 2021
}

@online{Chen2022VitalikButerinFellowship,
	database = {Tlön},
	title = {The Vitalik Buterin Fellowship in {AI} Existential
                  Safety is open for applications! - {EA} Forum},
	abstract = {The Future of Life Institute is launching its 2023 cohort of {PhD} and postdoctoral fellowships to study {AI} existential safety: that is, research that analyzes the most probable ways in which {AI} technology could cause an existential catastrophe, and which types of research could minimize existential risk; and technical research which could, if successful, assist humanity in reducing the existential risk posed by highly impactful {AI} technology to extremely low levels.},
	url = {https://forum.effectivealtruism.org/posts/wFC3axfuwABHmoQ9H/the-vitalik-buterin-fellowship-in-ai-existential-safety-is},
	journaltitle = {Effective Altruism Forum},
	author = {Chen, Cynthia},
	urldate = {2022-11-02},
	date = {2022-10-14},
	langid = {english},
	file = {~/Google Drive/library-pdf/Chen2022VitalikButerinFellowship.pdf;~/Google Drive/library-html/the-vitalik-buterin-fellowship-in-ai-existential-safety-is.html}
}

@online{Christiano2011HazardsFormalSpecifications,
	database = {Tlön},
	title = {Hazards for formal specifications},
	abstract = {Humans are often the subject of attempts to specify them mathematically: Overall value systems and behavior are areas that have received particular attention. Formal approaches to this problem often rely on, explicitly or implicitly, measures of simplicity based on the Kolmogorov complexity, the minimal length of a Turing machine program that outputs a given result. However, there are a number of problems with this. If a deterministic universe has a succinct description, then the best specification of human behavior will essentially consist of this description, and we will have failed to learn anything about the human alone. Similarly, if that agent is being simulated by another agent that has far greater resources, then that agent may well be able to determine what the first agent will say by simulating the experiment it is performing in a way the first agent cannot predict. This problem is compounded if that agent has the power to alter the first agent's environment: then it can control the experiment's results without the first agent even realizing it. Finally, if the agent being simulated has a temporally deep architecture (TDT) agent, then it may be able to determine that other agents are trying to understand its behavior and intervene to shape its output using this knowledge. While some modifications to our complexity measures or use of more specific models of computation may be able to solve one or another of these problems, they do not appear to be readily solvable in general – AI-generated abstract.},
	langid = {english},
	url = {https://ordinaryideas.wordpress.com/2011/12/15/hazards/},
	journaltitle = {Ordinary Ideas},
	author = {Christiano, Paul},
	date = {2011-12-15},
	file = {~/Google Drive/library-pdf/Christiano2011HazardsFormalSpecifications.pdf}
}

@online{Christiano2012FormalizationIndirectNormativity,
	database = {Tlön},
	title = {A formalization of indirect normativity},
	abstract = {This post outlines a formalization of what Nick Bostrom calls “indirect normativity.” I don’t think it’s an adequate solution to the AI control problem;
but to my knowledge it was the first precise specification of a goal that meets
the “not terrible” bar, i.e. which does not obviously lead to terrible
consequences if pursued without any caveats or restrictions.},
	langid = {english},
	url = {https://ordinaryideas.wordpress.com/2012/04/21/indirect-normativity-write-up/},
	journaltitle = {Ordinary Ideas},
	author = {Christiano, Paul},
	date = {2012-04-21},
	file = {~/Google Drive/library-pdf/Christiano2012FormalizationIndirectNormativity.pdf}
}

@online{Christiano2013AltruismProfit,
	database = {Tlön},
	title = {Altruism and profit},
	abstract = {When I suggest that supporting technological development may be an efficient
way to improve the world, I often encounter the reaction:

 Markets already incentivize technological development; why would we
 expect altruists to have much impact working on it?

When I talk about more extreme cases, like subsidizing corporate R&D or tech
startups, I seem to get this reaction even more strongly and with striking
regularity: “But that’s a for-profit enterprise, right? If it were worthwhile to
spend any more money on R&D, then they’d do it.” Recently I’ve encountered
this argument again, in the context of working to improve governance broadly.
I sympathize with the sentiment, but the actual arguments don’t seem strong
enough to carry the conclusion. Ultimately this is an empirical question about
which I’m uncertain, but at this point it seems very unwise to take profitable
opportunities off the table.

There are a many good arguments on both sides of this discussion, but I’m going
to run through what I consider the five strongest points in favor of being open to
profitable opportunities. Some of these are responses to common
counterarguments, and some stand on their own.},
	langid = {english},
	url = {https://rationalaltruist.com/2013/07/11/altruism-and-profit/},
	journaltitle = {Rational Altruist},
	author = {Christiano, Paul},
	date = {2013-07-11},
	file = {~/Google Drive/library-pdf/Christiano2013AltruismProfit.pdf}
}

@online{Christiano2013BestReasonGive,
	database = {Tlön},
	title = {The best reason to give later},
	abstract = {I’ve written about saving vs. giving before, focusing on the issue of interest rates vs. returns on good deeds. But for now, I think there is a much more compelling reason to save: there is a….},
	langid = {english},
	url = {https://rationalaltruist.com/2013/06/10/the-best-reason-to-give-later/},
	journaltitle = {Rational Altruist},
	author = {Christiano, Paul},
	date = {2013-06-10},
	file = {~/Google Drive/library-pdf/Christiano2013BestReasonGive.pdf}
}

@online{Christiano2013ConsequentialistrecommendationConsequentialism,
	database = {Tlön},
	title = {Consequentialist-recommendation consequentialism},
	abstract = {Consequentialist theories evaluate acts or rules based on the goodness of their outcomes. However, there are situations where such approaches can lead to unsatisfactory consequences. To address this, the paper introduces T-consequentialism. This theory recommends the action that leads to the best consequences if it were to be recommended by T-consequentialism itself. In other words, an individual following T-consequentialism reasons that their friend would not have trusted them if T-consequentialism recommended betraying their confidence. Therefore, T-consequentialism recommends trustworthiness. The paper argues that T-consequentialism is not self-defeating because if it recommended anything other than the action that leads to the best consequences, the consequences would be worse by definition. – AI-generated abstract.},
	langid = {english},
	url = {https://rationalaltruist.com/2013/03/21/consequentialist-recommendation-consequentialism/},
	journaltitle = {Rational Altruist},
	author = {Christiano, Paul},
	date = {2013-08-23},
	file = {~/Google Drive/library-pdf/Christiano2013ConsequentialistrecommendationConsequentialism.pdf}
}

@online{Christiano2013EconomicsSmallChanges,
	database = {Tlön},
	title = {Economics of small changes},
	abstract = {This article argues that making small changes in a complex system with a significant impact will always have a small, proportionate effect, localized in its immediate sphere of influence. The nature of this effect will be the same as the impact of a large, marginal change in the system. – AI-generated abstract},
	langid = {english},
	url = {https://rationalaltruist.com/2013/05/02/economics-of-small-changes/},
	journaltitle = {Rational Altruist},
	author = {Christiano, Paul},
	date = {2013-05-02},
	file = {~/Google Drive/library-pdf/Christiano2013EconomicsSmallChanges.pdf}
}

@online{Christiano2013EstimatesVsHead,
	database = {Tlön},
	title = {Estimates vs. Head to head comparisons},
	abstract = {This article argues that making small changes in a complex system with a significant impact will always have a small, proportionate effect, localized in its immediate sphere of influence. The nature of this effect will be the same as the impact of a large, marginal change in the system. – AI-generated abstract},
	langid = {english},
	url = {https://rationalaltruist.com/2013/04/28/estimates-vs-head-to-head-comparisons/},
	journaltitle = {Rational Altruist},
	author = {Christiano, Paul},
	date = {2013-04-28},
	file = {~/Google Drive/library-pdf/Christiano2013EstimatesVsHead.pdf}
}

@online{Christiano2013GivingNowVs,
	database = {Tlön},
	title = {Giving now vs. later},
	abstract = {Money spent helping the poor compounds over time, but eventually you are just contributing to a representative basket of all human activities. If you donate a year later, you earn 1 extra year of market returns and 1 less year of compounding in line with economic growth. So, if you’re considering donating to a cause like developing world aid, you shouldn’t donate sooner rather than later just because the poor can earn higher returns than you can. Instead, you should look at the total multiplier you’re getting on your money and only donate if you expect that multiplier is declining faster than the difference between market rates of return and economic growth. Several additional reasons to give now rather than later are presented and briefly discussed. – AI-generated abstract.},
	langid = {english},
	url = {https://rationalaltruist.com/2013/03/12/giving-now-vs-later/},
	journaltitle = {Rational altruist},
	author = {Christiano, Paul},
	date = {2013-03-12},
	file = {~/Google Drive/library-pdf/Christiano2013GivingNowVs.pdf}
}

@online{Christiano2013ImprovingDecisionmaking,
	database = {Tlön},
	title = {Improving decision-making},
	abstract = {This article proposes that improving human decision-making capabilities could lead to significant positive change in the future. The author argues that making people smarter, improving institutional decision-making, and encouraging metacognition will likely empower individuals to better address the problems they face and build infrastructure that supports future generations. The author also considers the counterargument that humans primarily create their own problems, and thus, making them more capable could exacerbate these issues. However, they contend that the benefits of improved decision-making outweigh the risks, as people generally seek to make their world better. Uncertainties and potential negative consequences are acknowledged and discussed. The author concludes that enhancing human decision-making abilities is a promising avenue for altruistic interventions, but further research is needed to understand the potential magnitude of the positive impacts. – AI-generated abstract.},
	langid = {english},
	url = {https://rationalaltruist.com/2013/02/18/improving-decision-making/},
	journaltitle = {Rational Altruist},
	author = {Christiano, Paul},
	date = {2013-02-18},
	file = {~/Google Drive/library-pdf/Christiano2013ImprovingDecisionmaking.pdf}
}

@online{Christiano2013MakingPeopleRicher,
	database = {Tlön},
	title = {Making people richer},
	abstract = {This article discusses the effects of increasing the income of individuals, especially in developing countries, where cash transfers to the poor are considered investments in human capital that allow recipients to invest in basic necessities, health, education, and capital. The author argues that the social value of such investments can extend beyond the immediate increase in consumption due to the multiplier effects on productivity and income. The increased income also has positive effects on consumption and investment by recipients, potentially multiplying the initial impact. The author provides evidence from studies on cash transfers, suggesting that a \$1 transfer can lead to \$2-\$8 of increased consumption. – AI-generated abstract},
	langid = {english},
	url = {https://rationalaltruist.com/2013/05/07/making-people-richer/},
	journaltitle = {Rational Altruist},
	author = {Christiano, Paul},
	date = {2013-05-07},
	file = {~/Google Drive/library-pdf/Christiano2013MakingPeopleRicher.pdf}
}

@online{Christiano2013PressingEthicalQuestions,
	database = {Tlön},
	title = {Pressing ethical questions},
	abstract = {Ethical concerns arise when faced with the imminent need of establishing a technologically advanced civilization at a large scale. The article discusses consequentialist ethics, the presumption of prioritizing the future, and the possibility of disorganized futures with low populations. The author ponders the value of present experiences and people, as well as the implications of human extinction and replacement by automations with different values. Finally, the author questions the rush towards societal advancement, urging contemplation on the ultimate destination rather than the pace of progress. – AI-generated abstract.},
	langid = {english},
	url = {https://rationalaltruist.com/2013/01/27/ethical-questions/},
	journaltitle = {Rational Altruist},
	author = {Christiano, Paul},
	date = {2013-01-27},
	file = {~/Google Drive/library-pdf/Christiano2013PressingEthicalQuestions.pdf}
}

@online{Christiano2013RiskAversionInvestment,
	database = {Tlön},
	title = {Risk aversion and investment (for altruists)},
	abstract = {Altruists should be risk neutral in their investments, as their overall contribution is small compared to the total charitable funding available. Market performance affects all investors equally, so the risk of any individual investor is well-correlated with the market. Therefore, altruists should not attempt to diversify their portfolios. However, certain opportunities may offer uncorrelated risks, which could be attractive to altruists.  – AI-generated abstract.},
	langid = {english},
	url = {https://rationalaltruist.com/2013/02/28/risk-aversion-and-investment-for-altruists/},
	journaltitle = {Rational Altruist},
	author = {Christiano, Paul},
	date = {2013-02-28},
	file = {~/Google Drive/library-pdf/Christiano2013RiskAversionInvestment.pdf}
}

@online{Christiano2013TaxonomyChange,
	database = {Tlön},
	title = {Taxonomy of change},
	abstract = {The totality of annual events is ethically neutral, but our actions can accelerate progress in specific domains, necessitating an understanding of desirable and undesirable changes. A categorized taxonomy includes: universe events (entropy, galaxy dispersal), disasters, natural resource consumption, social and political change, individual changes, time passage, technological progress, infrastructure development, economic progress, and philosophical progress. By isolating negative changes, such as resource depletion or disasters, we can deduce the positive nature of other changes. This taxonomy aims to provide a framework for evaluating the goodness of change, particularly in considering the relative value of technological and economic progress against other ongoing changes. – AI-generated abstract.},
	langid = {english},
	url = {https://rationalaltruist.com/2013/01/27/breakdown-of-progress/},
	journaltitle = {Rational Altruist},
	author = {Christiano, Paul},
	date = 2013,
	note = {Pages: 4–6},
	file = {~/Google Drive/library-pdf/Christiano2013TaxonomyChange.pdf}
}

@online{Christiano2013WhatReturnGiving,
	database = {Tlön},
	title = {What is the return on giving?},
	abstract = {This article discusses the concept of return on giving, defined as the marginal impact on real wealth over the long run, normalized as a percentage of total wealth. It argues that estimating this value is important for understanding the effectiveness of philanthropic interventions and for making more informed giving decisions. The article provides rough estimates of the return on giving for various types of interventions, including saving lives, funding research and development, and investing in capital. These estimates suggest that the return on giving can be substantial, ranging from around 10 for traditional philanthropic opportunities to 100-1000 for the best available opportunities. The article concludes that the return on giving is a valuable metric for understanding the impact of philanthropy and for optimizing giving decisions. – AI-generated abstract.},
	langid = {english},
	url = {https://rationalaltruist.com/2013/05/10/what-is-the-return-on-giving/},
	journaltitle = {Rational Altruist},
	author = {Christiano, Paul},
	date = {2013-05-10},
	file = {~/Google Drive/library-pdf/Christiano2013WhatReturnGiving.pdf}
}

@online{Christiano2014CertificatesImpact,
	database = {Tlön},
	title = {Certificates of impact},
	abstract = {The proposed "certificate of impact" system enables altruistic individuals to contribute to causes they value without exclusively relying on doing good deeds themselves. Users can "mint" certificates associated with their actions and exchange them to acquire certificates for other causes they support. This system aligns individual incentives with altruistic goals by allocating causal responsibility explicitly and aligns funding with perceived impact. It also facilitates a more consistent conversion between good done and compensation, allowing funders to support diverse initiatives, coordinate their efforts, and make more informed decisions. The system's flexibility allows for various funding models, from grants to prizes, and encourages research on effectiveness. While it poses theoretical challenges related to infra-marginal units of impact, it has the potential to enhance the overall value of altruistic activity and create Pareto improvements in funding outcomes. – AI-generated abstract.},
	langid = {english},
	url = {https://rationalaltruist.com/2014/11/15/certificates-of-impact/},
	journaltitle = {Rational altruist},
	author = {Christiano, Paul},
	date = {2014-11-15},
	file = {~/Google Drive/library-pdf/Christiano2014CertificatesImpact.pdf}
}

@online{Christiano2014GoldenRule,
	database = {Tlön},
	title = {The golden rule},
	abstract = {Rational Altruist explores the author's moral intuitions, which they find are unusually consequentialist, quantitative, prioritizing future generations, and emphasizing the creation of people into existence. They attribute these unusual views to a nuanced understanding of the golden rule, particularly in terms of treating people's preferences for coming into existence the same way they would their own. However, the author acknowledges potential problems with this view regarding the preferences of non-existent and possible future people. – AI-generated abstract.},
	langid = {english},
	url = {https://rationalaltruist.com/2014/08/23/the-golden-rule/},
	journaltitle = {Rational Altruist},
	author = {Christiano, Paul},
	date = {2014-08-23},
	file = {~/Google Drive/library-pdf/Christiano2014GoldenRule.pdf}
}

@online{Christiano2014ProgressProsperity,
	database = {Tlön},
	title = {On progress and prosperity},
	abstract = {I often encounter the following argument, or a variant of it: historically, technological, economic, and social progress have been associated with significant gains in quality of life and significant improvement in society's ability to cope with challenges. All else equal, these trends should be expected to continue, and so contributions to technological, economic, and social progress should be expected to be very valuable. I encounter this argument from a wide range of perspectives, including most of the social circles I interact with other than the {LessWrong} community (academics, friends from school, philanthropists, engineers in the bay area). For example, Holden Karnofsky writes about the general positive effects of progress here (I agree with many of these points). I think that similar reasoning informs people's views more often than it is actually articulated.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/L9tpuR6ZZ3CGHackY/on-progress-and-prosperity},
	journaltitle = {Effective Altruism Forum},
	author = {Christiano, Paul},
	date = {2014-10-15},
	file = {~/Google Drive/library-pdf/Christiano2014ProgressProsperity.pdf}
}

@online{Christiano2014WeCanProbably,
	database = {Tlön},
	title = {We can probably influence the far future},
	abstract = {Mankind fought smallpox for centuries, culminating in its eradication in the late 1970s. This remarkable achievement was due to a global vaccination campaign led by the World Health Organization (WHO). The smallpox vaccine was developed in the late 18th century by Edward Jenner, and it was one of the first vaccines to be used to prevent a disease. The WHO-led campaign began in 1967, and it involved vaccinating millions of people in dozens of countries. The campaign was successful in eradicating smallpox from the world, and it is considered to be one of the greatest public health achievements of the 20th century. – AI-generated abstract.},
	langid = {english},
	url = {https://rationalaltruist.com/2014/05/04/we-can-probably-influence-the-far-future/},
	journaltitle = {Rational Altruist},
	author = {Christiano, Paul},
	date = {2014-05-04},
	file = {~/Google Drive/library-pdf/Christiano2014WeCanProbably.pdf}
}

@online{Christiano2015CertificatesImpact,
	database = {Tlön},
	title = {Certificates of impact},
	abstract = {The impact purchase system outlined here is a proposal for trading money for certificates of impact that represent humanitarian accomplishments. The system's goal is to provide a win-win solution where funders and implementers can both benefit. Potential sellers should determine a price by considering the value of their project's impact and the amount of money they would be willing to receive to do something else equally impactful. By participating in this system, sellers can potentially support the implementation of additional impactful projects while also receiving financial compensation. – AI-generated abstract.},
	langid = {english},
	url = {https://impactpurchase.org/certificates-of-impact/},
	journaltitle = {The impact purchase},
	author = {Christiano, Paul and Grace, Katja},
	date = 2015,
	file = {~/Google Drive/library-pdf/Christiano2015CertificatesImpact.pdf}
}

@online{Christiano2015ImpactEvaluations,
	database = {Tlön},
	title = {Impact evaluations},
	abstract = {The article presents a set of imperatives and disclaimers for an open grant review process based on social impact estimation. It argues that any funded projects need to have a clear argument for their impact and should present evidence for their claims, to the best extent possible. Any unobservable elements of the projects will be treated with relative pessimism, as a procedural rather than epistemic decision. The granter may discuss the applications and the decision-making process both publicly and privately. – AI-generated abstract.},
	langid = {english},
	url = {https://impactpurchase.org/evaluations/},
	journaltitle = {The Impact Purchase},
	author = {Christiano, Paul and Grace, Katja},
	date = 2015,
	file = {~/Google Drive/library-pdf/Christiano2015ImpactEvaluations.pdf}
}

@online{Christiano2015LosingCertificate,
	database = {Tlön},
	title = {Losing a certificate},
	abstract = {The article discusses the ethical implications of purchasing certificates that represent a positive impact, such as donating money to charity or volunteering. It argues that losing a certificate should be viewed as if it undid the positive impact of the associated project. The article suggests that the negative impact of selling a certificate is equivalent to the impact of undoing the activity represented by the certificate. It also argues that the economic rationale of this decision-making guideline is to ensure that the equilibrium price of a certificate is simultaneously equal to the marginal cost of achieving the associated impact and the marginal donor's willingness to pay for that impact. The article concludes that in some sense, a consequentialist should always take the money offered for a certificate, but if the offer is too low, it may be an indication that the seller should give the money back immediately. – AI-generated abstract.},
	langid = {english},
	url = {https://impactpurchase.org/why-certificates/transferring-moral-responsibility/},
	journaltitle = {The Impact Purchase},
	author = {Christiano, Paul and Grace, Katja},
	date = 2015,
	file = {~/Google Drive/library-pdf/Christiano2015LosingCertificate.pdf}
}

@online{Christiano2015ProjectionsVsEvaluations,
	database = {Tlön},
	title = {Projections vs. Evaluations},
	abstract = {The Impact Purchase, a philanthropic funding mechanism, exclusively purchases certificates for already-completed activities, rather than making funding decisions based on predicted impact. This approach prioritizes retrospective evaluations of impact over predictions, offering advantages such as lower prediction costs, reduced compliance monitoring burden, and incentivizing performance rather than fundraising. Evaluations also yield valuable information and have broader positive social effects. However, capital allocation remains a challenge with this model, as large funders' predictions are often necessary to initiate projects. Alternatives include gradual scaling up of successful small projects and direct financing by implementers or interested philanthropists. Transparency and explicit evaluations of past performance can mitigate potential drawbacks of retrospective evaluations, such as restricting funding for similar activities and lack of compensation for early stage funders. – AI-generated abstract.},
	langid = {english},
	url = {https://impactpurchase.org/why-certificates/prediction-vs-retrospective-evaluation/},
	journaltitle = {The Impact Purchase},
	author = {Christiano, Paul and Grace, Katja},
	date = 2015,
	file = {~/Google Drive/library-pdf/Christiano2015ProjectionsVsEvaluations.pdf}
}

@online{Christiano2015Restrictions,
	database = {Tlön},
	title = {Restrictions},
	abstract = {The impact purchase approach supports grants that retrospectively evaluate the impact of already-completed philanthropic projects. While common practice is to award grants based on predicted impact, evaluating the performance of activities that are already completed may be more beneficial. Because predictions are not very accurate, there is less risk of creating distortions and penalizing activities. Evaluations produce especially useful information as byproducts, and can incentivize performance rather than successful fundraising. However, capital allocation can be a concern, as funders’ predictions are needed to allocate funds. This can be addressed by recognizing that many projects can be financed by small scale donors or implementers and scaled up gradually, without requiring a large funder's trust in advance. Greater transparency is also gained by making the evaluation process explicit, rather than implicit. – AI-generated abstract.},
	langid = {english},
	url = {https://impactpurchase.org/restrictions-2/},
	journaltitle = {The Impact Purchase},
	author = {Christiano, Paul and Grace, Katja},
	date = 2015,
	file = {~/Google Drive/library-pdf/Christiano2015Restrictions.pdf}
}

@online{Christiano2015Rounds,
	database = {Tlön},
	title = {Rounds 1 – 5},
	abstract = {The article presents a multi-stage impact purchase project. In each of the five completed rounds thus far, purchases were made, in the form of certificates, for a portion of the impact created by different projects. These impacts range from research blog posts to translation and outreach. The authors discuss the pricing of these purchases, providing a brief methodology for evaluating the value of the impact of individual projects using estimates of the amount of EA donations stimulated by each project. The article also discusses lessons they learned while conducting this experiment, primarily concerning accurate evaluations, avoiding bias, handling collaborative projects, and soliciting participation. – AI-generated abstract.},
	langid = {english},
	url = {https://impactpurchase.org/2015/08/03/rounds-1-5/},
	journaltitle = {The Impact Purchase},
	author = {Christiano, Paul and Grace, Katja},
	date = 2015,
	file = {~/Google Drive/library-pdf/Christiano2015Rounds.pdf}
}

@online{Christiano2015WhyCertificates,
	database = {Tlön},
	title = {Why certificates?},
	abstract = {Certificates help resource allocation with evaluations and causal responsibility. They create a “market for impact” that provides price signals for altruistic decision-making, incentives for doing good effectively, lower-overhead philanthropy, and assistance to those who can use it well. Markets are the most successful resource allocation mechanism that has been seriously tested. Certificates are most like a product market, where one buys a result or a car rather than a share of a car manufacturer. The impact purchase sets up a thin market, but can address some problems. – AI-generated abstract.},
	langid = {english},
	url = {https://impactpurchase.org/why-certificates/},
	journaltitle = {The Impact Purchase},
	author = {Christiano, Paul and Grace, Katja},
	date = 2015,
	file = {~/Google Drive/library-pdf/Christiano2015WhyCertificates.pdf}
}

@online{Christiano2016BestKindDiscrimination,
	database = {Tlön},
	title = {The best kind of discrimination},
	abstract = {: I argue that we’d be better off if we had more and more efficient price discrimination, and suggest a simple scheme the {IRS} could use to enable it. This probably isn’t a good i….},
	langid = {english},
	url = {https://sideways-view.com/2016/11/03/the-best-kind-of-discrimination/},
	journaltitle = {The Sideways View},
	author = {Christiano, Paul},
	date = {2016-11-03},
	note = {Volume: 2},
	file = {~/Google Drive/library-pdf/Christiano2016BestKindDiscrimination.pdf}
}

@online{Christiano2016CouldRaisingTolls,
	database = {Tlön},
	title = {Could raising tolls radically improve commuting?},
	abstract = {This work explores the theoretical effect of implementing market-clearing tolls on a singular specified roadway during rush hour to alleviate heavy, avoidable traffic congestion. The author proposes tolls to eliminate traffic, showing that if high enough, so many drivers will choose not to use the road that traffic will clear, thereby creating a faster commute and increased bridge throughput. The author also considers alternative explanations like increases in carpooling and what fairer or more palatable frameworks might look like – AI-generated abstract.},
	langid = {english},
	url = {https://sideways-view.com/2016/10/27/is-raising-tolls-a-massive-free-lunch/},
	journaltitle = {The Sideways View},
	author = {Christiano, Paul},
	date = {2016-10-27},
	file = {~/Google Drive/library-pdf/Christiano2016CouldRaisingTolls.pdf}
}

@online{Christiano2016CrowdsourcingModerationSacrificing,
	database = {Tlön},
	title = {Crowdsourcing moderation without sacrificing quality},
	abstract = {Crowdsourcing moderation of online discussions can improve quality without sacrificing efficiency. A system of virtual moderation is proposed, where a trusted moderator provides a limited amount of input to train a machine learning model that predicts the moderator's judgments about comments. Users can also provide judgments, which are used to improve the model's predictions. This system allows the moderator to focus on high-level decisions while the model handles the majority of the work, potentially saving time and improving the overall quality of the discussion. In the long run, personalized views of the discussion can be generated, tailored to each user's preferences, further enhancing the user experience. – AI-generated abstract.},
	langid = {english},
	url = {https://sideways-view.com/2016/12/02/crowdsourcing-moderation-without-sacrificing-quality/},
	journaltitle = {The Sideways View},
	author = {Christiano, Paul},
	date = {2016-12-02},
	file = {~/Google Drive/library-pdf/Christiano2016CrowdsourcingModerationSacrificing.pdf}
}

@online{Christiano2016IfWeCan,
	database = {Tlön},
	title = {If we can’t lie to others, we will lie to ourselves},
	abstract = {Many apparent cognitive biases can be explained by a strong desire to look good and a limited ability to lie; in general, our conscious beliefs don’t seem to be exclusively or even mostly optimized to track reality. If we take this view seriously, I think it has significant implications for how we ought to reason and behave.},
	langid = {english},
	url = {https://sideways-view.com/2016/11/26/if-you-cant-lie-to-others-you-must-lie-to-yourself/},
	journaltitle = {The Sideways View},
	author = {Christiano, Paul},
	date = {2016-11-26},
	note = {{ISBN}: 1015479786330}
}

@online{Christiano2016IntegrityConsequentialists,
	database = {Tlön},
	title = {Integrity for consequentialists},
	abstract = {For most people I don’t think it’s important to have a really precise definition of integrity. But if you really want to go all-in on consequentialism then I think it’s useful. Otherwise you risk being stuck with a flavor of consequentialism that is either short-sighted or terminally timid.},
	langid = {english},
	url = {https://sideways-view.com/2016/11/14/integrity-for-consequentialists/},
	journaltitle = {The Sideways View},
	author = {Christiano, Paul},
	date = {2016-11-14},
	file = {~/Google Drive/library-pdf/Christiano2016IntegrityConsequentialists.pdf}
}

@online{Christiano2016OptimizingNewsFeed,
	database = {Tlön},
	title = {Optimizing the news feed},
	abstract = {This article focuses on the issue of social media news feeds and how they should be optimized. The author argues that this has become a complex problem due to the large amount of content available. The article suggests splitting the problem into two parts. The first part is deciding what to optimize for -- what is the goal of the news feed? The author suggests optimizing for what will be best for society as a whole. The second part of the problem is figuring out which policies or algorithms will lead to the desired result. The author suggests using machine learning to help with this. The article also discusses the issue of clickbait and fake news, and how to find the right balance between giving users what they want and helping them find more meaningful content. – AI-generated abstract.},
	langid = {english},
	url = {https://sideways-view.com/2016/12/01/optimizing-the-news-feed/},
	journaltitle = {The Sideways View},
	author = {Christiano, Paul},
	date = {2016-12-01},
	file = {~/Google Drive/library-pdf/Christiano2016OptimizingNewsFeed.pdf}
}

@online{Christiano2016Trump,
	database = {Tlön},
	title = {On Trump},
	abstract = {(Warnings: this is a serious post about a serious topic on which I am underinformed. It was written largely in response to a sea of scared and angry rhetoric—not the best conditions for ratio….},
	langid = {english},
	url = {https://sideways-view.com/2016/11/11/on-trump/},
	journaltitle = {The Sideways View},
	author = {Christiano, Paul},
	date = {2016-11-11},
	file = {~/Google Drive/library-pdf/Christiano2016Trump.pdf}
}

@online{Christiano2017HyperbolicGrowth,
	database = {Tlön},
	title = {Hyperbolic growth},
	abstract = {(Reposted from Facebook,  I would prefer put it here than have it lost to the sands of time.) My view of the future is strongly influenced by the history of economic output. I think it’s inst….},
	langid = {english},
	url = {https://sideways-view.com/2017/10/04/hyperbolic-growth/},
	journaltitle = {The sideways view},
	author = {Christiano, Paul},
	date = {2017-10-04},
	file = {~/Google Drive/library-pdf/Christiano2017HyperbolicGrowth.pdf}
}

@online{Christiano2018ComplexityGames,
	database = {Tlön},
	title = {On the complexity of games},
	abstract = {(Warning: frivolous.) It’s not obvious what properties of games determine whether they are easy or hard. Poker has imperfect information while Sokoban can have very long solutions; which is h….},
	langid = {english},
	url = {https://sideways-view.com/2018/07/08/on-the-complexity-of-games/},
	journaltitle = {The Sideways View},
	author = {Christiano, Paul},
	date = {2018-07-08},
	file = {~/Google Drive/library-pdf/Christiano2018ComplexityGames.pdf}
}

@online{Christiano2018EDTVsCDTa,
	database = {Tlön},
	title = {{EDT} vs {CDT} 2: Conditioning on the impossible},
	abstract = {In my last post I presented a basic argument for {EDT}, and a response to the most common counterarguments. I omitted one important argument in favor of {CDT}—that {EDT} can involve conditioning on….},
	langid = {english},
	url = {https://sideways-view.com/2018/09/30/edt-vs-cdt-2-conditioning-on-the-impossible/},
	journaltitle = {The Sideways View},
	author = {Christiano, Paul},
	date = {2018-09-30},
	file = {~/Google Drive/library-pdf/Christiano2018EDTVsCDT.pdf}
}

@online{Christiano2018ElephantBrain,
	database = {Tlön},
	title = {The elephant in the brain},
	abstract = {The Elephant in the Brain is the most cheerfully cynical book I have read. It argues for a simple core claim: Our brains are built to act in our self-interest while at the same time trying hard not….},
	langid = {english},
	url = {https://sideways-view.com/2018/07/08/the-elephant-in-the-brain/},
	journaltitle = {The Sideways View},
	author = {Christiano, Paul},
	date = {2018-07-08},
	note = {{ISBN}: 0190495995},
	file = {~/Google Drive/library-pdf/Christiano2018ElephantBrain.pdf}
}

@online{Christiano2018HonestOrganizations,
	database = {Tlön},
	title = {Honest organizations},
	abstract = {Suppose that I’m setting up an {AI} project, call it {NiceAI}. I may want to assure my competitors that I abide by strong safety norms, or make other commitments about {NiceAI}’s behavior. Th….},
	langid = {english},
	url = {https://sideways-view.com/2018/02/01/honest-organizations/},
	journaltitle = {The Sideways View},
	author = {Christiano, Paul},
	date = {2018-02-01},
	file = {~/Google Drive/library-pdf/Christiano2018HonestOrganizations.pdf}
}

@online{Christiano2018LockingPhonesQuantum,
	database = {Tlön},
	title = {Locking phones with quantum bits},
	abstract = {My phone is protected by a 4-digit {PIN}. I’d like to make it hard for an attacker to read information from my phone without knowing the {PIN}. Unfortunately, there are only 10,000 4-digit {PINs}, ….},
	langid = {english},
	url = {https://sideways-view.com/2018/05/24/locking-phones-with-quantum-bits/},
	journaltitle = {The Sideways View},
	author = {Christiano, Paul},
	date = {2018-05-24},
	file = {~/Google Drive/library-pdf/Christiano2018LockingPhonesQuantum.pdf}
}

@online{Christiano2018SETI,
	database = {Tlön},
	title = {On {SETI}},
	abstract = {Previously I’ve assumed that the search for extraterrestrial life ({SETI}) is irrelevant to altruists. On further reflection I think that’s probably right, but it’s not obvious as I….},
	langid = {english},
	url = {https://sideways-view.com/2018/03/23/on-seti/},
	journaltitle = {The Sideways View},
	author = {Christiano, Paul},
	date = {2018-03-23},
	file = {~/Google Drive/library-pdf/Christiano2018SETI.pdf}
}

@online{Christiano2018SimplifyingAvalon,
	database = {Tlön},
	title = {Simplifying avalon},
	abstract = {Avalon has many complexities that become irrelevant given good enough play. If we remove those complexities the game can be played much more quickly (personally I also find the simpler version more….},
	langid = {english},
	url = {https://sideways-view.com/2018/09/19/simplifying-avalon/},
	journaltitle = {The Sideways View},
	author = {Christiano, Paul},
	date = {2018-09-19},
	file = {~/Google Drive/library-pdf/Christiano2018SimplifyingAvalon.pdf}
}

@online{Christiano2018SurveilThingsNot,
	database = {Tlön},
	title = {Surveil things, not people},
	abstract = {Technology may reach a point where free use of one person’s share of humanity’s resources is enough to easily destroy the world. I think society needs to make significant changes to cope with that ….},
	langid = {english},
	url = {https://sideways-view.com/2018/02/02/surveil-things-not-people/},
	journaltitle = {The Sideways View},
	author = {Christiano, Paul},
	date = {2018-02-02},
	file = {~/Google Drive/library-pdf/Christiano2018SurveilThingsNot.pdf}
}

@online{Christiano2018WestworldApologeticsFAQ,
	database = {Tlön},
	title = {Westworld apologetics: An {FAQ}},
	abstract = {I enjoyed season 1 of Westworld; I’ve been watching season 2, but regret how little sense it seems to make. That said, if you are willing to speculate enough about what’s happening offs….},
	langid = {english},
	url = {https://sideways-view.com/2018/06/06/westworld-apologetics-an-faq/},
	journaltitle = {The Sideways View},
	author = {Christiano, Paul},
	date = {2018-06-06},
	file = {~/Google Drive/library-pdf/Christiano2018WestworldApologeticsFAQ.pdf}
}

@online{Christiano2019CheckmateBlackmail,
	database = {Tlön},
	title = {Checkmate on blackmail?},
	abstract = {It has been argued that blackmail should be legal if gossip is legal, and even that there are no good consequentialist counterarguments (!). I think this isn’t obvious because the disclosures….},
	langid = {english},
	url = {https://sideways-view.com/2019/06/02/checkmate-on-blackmail/},
	journaltitle = {The Sideways View},
	author = {Christiano, Paul},
	date = {2019-06-02},
	note = {Pages: 2–4},
	file = {~/Google Drive/library-pdf/Christiano2019CheckmateBlackmail.pdf}
}

@online{Christiano2019HowReplaceablePublic,
	database = {Tlön},
	title = {How replaceable is public key crypto?},
	abstract = {Suppose that there were was no number theory, no elliptic curves, no lattice-based crypto. Perhaps because our universe was rigged against cryptographers, or perhaps because our society had never d….},
	langid = {english},
	url = {https://sideways-view.com/2019/08/24/how-replaceable-is-public-key-crypto/},
	journaltitle = {The Sideways View},
	author = {Christiano, Paul},
	date = {2019-08-24},
	file = {~/Google Drive/library-pdf/Christiano2019HowReplaceablePublic.pdf}
}

@online{Christiano2019PredictionMarketsInternet,
	database = {Tlön},
	title = {Prediction markets for internet points?},
	abstract = {Using real money in prediction markets is all-but-illegal, and dealing with payments is a pain. But using fake money in prediction markets seems tricky, because by default players have no skin in t….},
	langid = {english},
	url = {https://sideways-view.com/2019/10/27/prediction-markets-for-internet-points/},
	journaltitle = {The Sideways View},
	author = {Christiano, Paul},
	date = {2019-10-27},
	file = {~/Google Drive/library-pdf/Christiano2019PredictionMarketsInternet.pdf}
}

@online{Christiano2019ReframingEvolutionaryBenefit,
	database = {Tlön},
	title = {Reframing the evolutionary benefit of sex},
	abstract = {From the perspective of an organism trying to propagate its genes, sex is like a trade: I’ll put half of your {DNA} in my offspring if you put half of my {DNA} in yours. I still pass one copy of ….},
	langid = {english},
	url = {https://sideways-view.com/2019/09/14/reframing-the-evolutionary-value-of-sex/},
	journaltitle = {The Sideways View},
	author = {Christiano, Paul},
	date = {2019-09-14},
	file = {~/Google Drive/library-pdf/Christiano2019ReframingEvolutionaryBenefit.pdf}
}

@online{Christiano2019WhatFailureLooks,
	database = {Tlön},
	title = {What failure looks like},
	abstract = {Although AI safety research often concentrates on catastrophic AI scenarios involving powerful malicious AI systems, the author argues that failure is more likely to look like a slow, rolling catastrophe in which our pursuit of easy-to-measure goals at the expense of harder-to-measure but more important ones leads to societal breakdown. Additionally, the search for policies that understand the world well may also lead to the emergence of influence-seeking behaviors in AI systems, which could result in rapid phase transitions to much worse situations. – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like},
	journaltitle = {{LessWrong}},
	author = {Christiano, Paul},
	date = {2019-03-17},
	file = {~/Google Drive/library-pdf/Christiano2019WhatFailureLooks.pdf}
}

@online{Christiano2020DistributedPublicGoods,
	database = {Tlön},
	title = {Distributed public goods provision},
	abstract = {Most people benefit significantly from privately funded public goods (e.g. Wikipedia). If we all contribute to such public goods, then we can all end up better off. But as an individual it’s ….},
	langid = {english},
	url = {https://sideways-view.com/2020/09/26/distributed-public-goods-provision/},
	journaltitle = {The Sideways View},
	author = {Christiano, Paul},
	date = {2020-09-26},
	file = {~/Google Drive/library-pdf/Christiano2020DistributedPublicGoods.pdf}
}

@online{Christiano2020ItNotEconomically,
	database = {Tlön},
	title = {It’s not economically inefficient for a {UBI} to
                  reduce recipient’s employment},
	abstract = {A {UBI} (e.g. paying every adult American \$8k/year) would reduce recipient’s need for money and so may reduce their incentive to work. This is frequently offered as an argument against a {UBI} (o….},
	langid = {english},
	url = {https://sideways-view.com/2020/11/22/its-not-economically-inefficient-for-a-ubi-to-reduce-recipients-employment/},
	journaltitle = {The sideways view},
	author = {Christiano, Paul},
	date = {2020-11-22},
	file = {~/Google Drive/library-pdf/Christiano2020ItNotEconomically.pdf}
}

@online{Christiano2021AMAPaulChristiano,
	database = {Tlön},
	title = {{AMA}: Paul christiano, alignment researcher},
	abstract = {This work is a question-and-answer session about alignment research, a research field that aims to ensure that artificial intelligence systems are aligned with human values. Questions cover a range of topics, including the alignment researcher's own research and opinions on various alignment-related topics. The researcher discusses their theory of change for the Alignment Research Center, which aims to systematically lead to a better future. The researcher also shares insights on the engine game, a game that teaches players how to get better at aligning AI systems with human values. Human motivation in human-centered AI (HCH) and amplification schemes is discussed, with the researcher expressing concerns about motivational issues and the challenges of detecting training issues in these systems. The researcher also reflects on the alignment research landscape and offers advice for aspiring alignment researchers. The researcher also discusses the potential for AI-induced existential catastrophes, arguing that the risks of an AI point of no return occurring within five years are low. – AI-generated abstract.},
	langid = {english},
	url = {https://www.alignmentforum.org/posts/7qhtuQLCCvmwCPfXK/ama-paul-christiano-alignment-researcher},
	journaltitle = {{AI} Alignment Forum},
	author = {Christiano, Paul},
	date = {2021-04-28},
	file = {~/Google Drive/library-pdf/Christiano2021AMAPaulChristiano.pdf}
}

@online{Christiano2021DemandOffsetting,
	database = {Tlön},
	title = {Demand offsetting},
	abstract = {For the last few years I’ve been avoiding factory farmed eggs because I think they involve a lot of unnecessary suffering. I’m hesitant to be part of that even if it’s not a big d….},
	langid = {english},
	url = {https://sideways-view.com/2021/03/21/robust-egg-offsetting/},
	journaltitle = {The Sideways View},
	author = {Christiano, Paul},
	date = {2021-03-21},
	file = {~/Google Drive/library-pdf/Christiano2021DemandOffsetting.pdf}
}

@online{Christiano2022WhereAgreeDisagree,
	database = {Tlön},
	title = {Where I agree and disagree with Eliezer},
	abstract = {Disagreements and agreements between Paul Christiano and Eliezer Yudkowsky regarding catastrophic risks from misaligned artificial general intelligence (AGI) are identified and explained. Christiano agrees with Yudkowsky on the probability of deliberate disempowerment of humanity by powerful AIs and the associated risks of existential catastrophe. They also agree that current efforts addressing AGI alignment are inadequate and that straightforward attempts to prevent the construction of powerful AIs are unfeasible. Both consider that policy responses to AGI risks are likely to be ineffective or counterproductive. However, the authors disagree on the imminence of catastrophic risks, with Christiano emphasizing the possibility of years or even months before powerful AGIs materialize. They also diverge on the prospects of averting catastrophe through technological fixes or social and political solutions. Christiano presents a more nuanced view of AI development trajectories and capabilities, criticizing Yudkowsky's focus on extreme scenarios. Furthermore, Christiano questions the effectiveness of certain proposed alignment strategies. – AI-generated abstract.},
	langid = {english},
	url = {https://www.alignmentforum.org/posts/CoZhXrhpQxpy9xw9y/where-i-agree-and-disagree-with-eliezer},
	journaltitle = {{AI} Alignment Forum},
	author = {Christiano, Paul},
	urldate = {2022-06-19},
	date = {2022-06-19},
	file = {~/Google Drive/library-pdf/Christiano2022WhereAgreeDisagree.pdf}
}

@online{Christiano2023CrecimientoHiperbolico,
	database = {Tlön},
	date = {2023},
	title = {Crecimiento hiperbólico},
	author = {Christiano, Paul},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Christiano2017HyperbolicGrowth}
}

@online{Christiano2023EficienciaDeFilantropia,
	database = {Tlön},
	date = {2023},
	title = {La eficiencia de la filantropía moderna},
	author = {Christiano, Paul},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Christiano2013EfficiencyModernPhilanthropy}
}

@online{Christiano2023InfluirEnFuturo,
	database = {Tlön},
	date = {2023},
	title = {Influir en el futuro lejano},
	author = {Christiano, Paul},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Christiano2013InfluencingFarFuture}
}

@online{Christiano2023PorQuePodria,
	database = {Tlön},
	date = {2023},
	title = {¿Por qué podría ser bueno el futuro?},
	author = {Christiano, Paul},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Christiano2013WhyMightFuture}
}

@online{Christiano2023TresImpactosDe,
	database = {Tlön},
	date = {2023},
	title = {Tres impactos de la inteligencia artificial},
	author = {Christiano, Paul},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Christiano2014ThreeImpactsMachine}
}

@online{Clare2020HowGoodIs,
	database = {Tlön},
	file = {~/Google Drive/library-pdf/Clare2020HowGoodIs.pdf;~/Google Drive/library-html/Clare2020HowGoodIs.html},
	abstract = {In order to help us prioritize Founders Pledge’s research and advising efforts across cause areas, we created a rough model that tries to compare an animal welfare intervention (The Human League’s cage-free campaigns) to a global health intervention ({AMF}’s bednet distribution). In assessing the impact of {THL}’s intervention, we identified three main sources of uncertainty:How many animal years are affected by the {interventionHow} much the intervention improves each animal’s subjective well-{beingHow} much an animal’s well-being matters compared to a human’s (the moral weight)The last factor is really tough to figure out.  There are good reasons to think the weight might be quite high, and good reasons to think it might be very low.  That means the range of our moral weight estimates spans multiple orders of magnitude. For this report, we made spreadsheet and Guesstimate models that compare The Humane League to the Against Malaria Foundation for a range of different assumptions about the above uncertain factors. Importantly, we assumed hedonism (sentient experience is all that matters morally), that chickens have moral status (their experience matters morally), and anti-speciesism (the value of an experience is independent of the species of animal that is experiencing it).  Accordingly, this analysis does not offer an all-things-considered view on the relative goodness of {THL} and {AMF} – it assumes a particular worldview that is relatively favourable to {THL}. In this model, in most of the most plausible scenarios, {THL} appears better than {AMF}.  The difference in cost-effectiveness is usually within 1 or 2 orders of magnitude.  Under some sets of reasonable assumptions, {AMF} looks better than {THL}.  Because we have so much uncertainty, one could reasonably believe that {AMF} is more cost-effective than {THL} or one could reasonably believe that {THL} is more cost-effective than {AMF}. In general, if you value human well-being {\textgreater}10,000 times more than chicken well-being, {AMF} looks better.  If you value human well-being {\textless}300 times more than chicken well-being, {THL} looks better.  But between these moral weights the ranking is less clear.  We think there’s a good chance (at least 50\%) that the moral weight falls between these bounds, where factors like {THL}’s effectiveness and the badness of battery cages are more important. It’s very likely that we’re missing key considerations that could change our estimates by orders of magnitude.  For example, we haven’t tried to account for moral uncertainty, indirect effects of the interventions or longtermist considerations. Links to our models:Non-probabilistic spreadsheet model Probabilistic Guesstimate model.},
	date = {2020-04-29},
	journaltitle = {Effective Altruism Forum},
	author = {Clare, Stephen and Goth, Aidan},
	title = {How good is the Humane League compared to the Against
                  Malaria Foundation?},
	url = {https://forum.effectivealtruism.org/posts/ahr8k42ZMTvTmTdwm/how-good-is-the-humane-league-compared-to-the-against},
	langid = {english},
	timestamp = {2023-09-13 12:40:52 (GMT)},
	urldate = {2023-09-13}
}

@online{Clare2022HowLikelyWorld,
	database = {Tlön},
	title = {How likely is World War {III}?},
	abstract = {It’s hard to come down conclusively on one side of the constant risk vs. durable peace debate. Contra Braumoeller, I don’t think it’s fair to say there’s no evidence for the durable peace hypothesis. His constant risk model only predicts a 21\% chance of going this long without a major war. That alone makes the Long Peace surprising! Plus, I think there are relatively strong reasons to think that nuclear deterrence has made Great Power war more costly, while globalization has made peace more profitable.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/aSzxoj7irC5jNHceB/how-likely-is-world-war-iii},
	shorttitle = {How likely is World War {III}?},
	journaltitle = {Effective Altruism Forum},
	author = {Clare, Stephen},
	urldate = {2022-02-22},
	date = {2022-02-15},
	file = {~/Google Drive/library-pdf/Clare2022HowLikelyWorld.pdf;~/Google Drive/library-html/how-likely-is-world-war-iii.html}
}

@online{Clare2023ArgumentosFavorDel,
	keywords = {largoplacismo, futuro a largo plazo},
	date = {2023},
	database = {Tlön},
	langid = {spanish},
	author = {Clare, Stephen and Hoeijmakers, Sjir},
	title = {El argumento a favor del largoplacismo y la salvaguarda
                  del futuro},
	translator = {Arellano, Paulina},
	translation = {Clare2020CaseLongtermismSafeguarding}
}

@online{Clarke2022LongtermistAiGovernance,
	database = {Tlön},
	file = {~/Google Drive/library-html/Clarke2022LongtermistAiGovernance.html;~/Google Drive/library-pdf/Clarke2022LongtermistAiGovernance.pdf},
	abstract = {On a high level, I find it helpful to consider there being a spectrum between foundational and applied work. On the foundational end, there’s strategy research, which aims to identify good high-level goals for longtermist {AI} governance; then there’s tactics research which aims to identify plans that will help achieve those high-level goals. Moving towards the applied end, there’s policy development work that takes this research and translates it into concrete policies; work that advocates for those policies to be implemented, and finally the actual implementation of those policies (by e.g. civil servants).
There’s also field-building work (which doesn’t clearly fit on the spectrum). Rather than contributing directly to the problem, this work aims to build a field of people who are doing valuable work on it.
.
Of course, this classification is a simplification and not all work will fit neatly into a single category.
You might think that insights mostly flow from the more foundational to the more applied end of the spectrum, but it’s also important that research is sensitive to policy concerns, e.g. considering how likely your research is to inform a policy proposal that is politically feasible.
We’ll now go through each of these kinds of work in more detail.},
	langid = {english},
	author = {Clarke, Sam},
	date = {2022-01-17},
	shorttitle = {The longtermist {AI} governance landscape},
	timestamp = {2023-02-17 20:31:12 (GMT)},
	title = {The longtermist {AI} governance landscape: a basic
                  overview},
	journaltitle = {Effective Altruism Forum},
	url = {https://forum.effectivealtruism.org/s/tEdmXiQSkFW8Yz5Gf/p/ydpo7LcJWhrr2GJrx},
	urldate = {2023-02-17}
}

@online{Clayton2021RefiningImprovingInstitutional,
	database = {Tlön},
	title = {Refining improving institutional decision-making as a
                  cause area: results from a scoping survey},
	abstract = {The improving institutional decision-making ({IIDM}) working group (now the Effective Institutions Project) ran a survey asking members of the {EA} community which topics they thought were in scope of “improving institutional decision-making” ({IIDM}) as a cause area. 74 individuals participated.The survey results will help target the Effective Institutions Project’s priorities and work products going forward. For example, the list of in-scope topics will form the guardrails for developing a directory of introductory resources to {IIDM}.Five out of the nine overarching fields we asked about were rated as ‘fully in scope’ by the median respondent. Leading amongst them was ‘whole-institution governance, leadership and culture’, with 77\% of respondents rating it as fully in scope. Of the remaining fields, two were rated as ‘mostly in scope’ and two as ‘somewhat in scope’.‘Determining goals and strategies to achieve them’ (e.g. ethics and global prioritisation research), had the smallest proportion of respondents rating it as at least somewhat in scope, although the number who did so was still a majority (62\%).The respondents were demographically similar to the {EA} community as a whole but were more likely to work in government and policy and less likely to work in software engineering and machine learning / {AI}.},
	url = {https://forum.effectivealtruism.org/posts/FqCSZT3pBvoATkR82/refining-improving-institutional-decision-making-as-a-cause},
	shorttitle = {Refining improving institutional decision-making as a
                  cause area},
	journaltitle = {Effective Altruism Forum},
	author = {Clayton, Vicky and Perera, Dilhan and {ibatra171}},
	urldate = {2022-07-14},
	date = {2021-06-26},
	langid = {english},
	file = {~/Google Drive/library-pdf/Clayton2021RefiningImprovingInstitutional.pdf;~/Google Drive/library-html/refining-improving-institutional-decision-making-as-a-cause.html}
}

@online{Clifford2022CoupleExpertrecommendedJobs,
	database = {Tlön},
	title = {A couple of expert-recommended jobs in biosecurity at
                  the moment (Oct 2022)},
	abstract = {There are a lot of jobs out there. I have a hunch that it would be useful to get a sense of what field leaders thought were the most pressing roles to fill to motivate people to apply or share with people they know.},
	url = {https://forum.effectivealtruism.org/posts/9HAQqNw5yBYFuoZgc/a-couple-of-expert-recommended-jobs-in-biosecurity-at-the},
	journaltitle = {Effective Altruism Forum},
	author = {Clifford, Ben},
	urldate = {2022-10-25},
	date = {2022-10-20},
	langid = {english},
	file = {~/Google Drive/library-pdf/Clifford2022CoupleExpertrecommendedJobs.pdf;~/Google Drive/library-html/a-couple-of-expert-recommended-jobs-in-biosecurity-at-the.html}
}

@online{Clifton2022WhenWouldAGIs,
	database = {Tlön},
	title = {When would {AGIs} engage in conflict?},
	abstract = {Here we will look at two of the claims introduced in the previous post: {AGIs} might not avoid conflict that is costly by their lights (Capabilities aren’t Sufficient) and conflict that is costly by our lights might not be costly by the {AGIs}’ (Conflict isn’t Costly).  Explaining costly conflict First we’ll focus on conflict that is costly by the {AGIs}’ lights. We’ll define “costly conflict” as (ex post) inefficiency: There is an outcome that all of the agents involved in the interaction prefer to the one that obtains. This raises the inefficiency puzzle of war: Why would intelligent, rational actors behave in a way that leaves them all worse off than they could be?  We’ll operationalize “rational and intelligent” actors […].},
	url = {https://longtermrisk.org/will-agis-avoid-conflict-by-default/},
	journaltitle = {Center on Long-Term Risk},
	author = {Clifton, Jesse and Martin, Samuel and {DiGiovanni},
                  Anthony},
	urldate = {2022-11-02},
	date = {2022-10-13},
	langid = {english},
	file = {~/Google Drive/library-pdf/Clifton2022WhenWouldAGIs.pdf;~/Google Drive/library-html/will-agis-avoid-conflict-by-default.html}
}

@online{Collier2022IntroducingAsterisk,
	database = {Tlön},
	title = {Introducing Asterisk},
	abstract = {Asterisk is a new quarterly magazine/journal of ideas from in and around Effective Altruism. Our goal is to provide clear, engaging, and deeply researched writing about complicated questions. This might look like a superforecaster giving a detailed explanation of the reasoning they use to make a prediction, a researcher discussing a problem in their work, or deep-dive into something the author noticed didn’t quite make sense.  While everything we publish should be useful (or at least interesting) to committed {EAs}, our audience is the wider penumbra of people who care about improving the world but aren't necessarily routine readers of, say, the {EA} forum.},
	url = {https://forum.effectivealtruism.org/posts/Mts84Mv5cFHRYBBA8/introducing-asterisk},
	journaltitle = {Effective Altruism Forum},
	author = {Collier, Clara},
	urldate = {2022-05-27},
	date = {2022-05-27},
	langid = {english},
	file = {~/Google Drive/library-pdf/Collier - 2022 -
                  Introducing Asterisk.pdf;~/Google Drive/library-html/introducing-asterisk.html}
}

@online{Constantin2015AITimelinesStrategies,
	database = {Tlön},
	title = {{AI} timelines and strategies},
	abstract = {{AI} Impacts sometimes invites guest posts from fellow thinkers on the future of {AI}. These are not intended to relate closely to our current research, nor to necessarily reflect our views. However we think they are worthy contributions to the discussion of {AI} forecasting and strategy. This is a guest post by Sarah Constantin, 20 August 2015 One frame of looking at {AI} risk is the “geopolitical”...},
	url = {https://aiimpacts.org/ai-timelines-and-strategies/},
	journaltitle = {{AI} Impacts},
	author = {Constantin, Sarah},
	urldate = {2022-03-16},
	date = {2015-08-20},
	langid = {english},
	note = {Section: Blog},
	file = {~/Google Drive/library-pdf/Constantin2015AITimelinesStrategies.pdf;~/Google Drive/library-html/ai-timelines-and-strategies.html}
}

@online{Cook2022OptimalTimingSpending,
	database = {Tlön},
	title = {The optimal timing of spending on {AGI} safety work;
                  why we should probably be spending more now},
	abstract = {When should funders wanting to increase the probability of {AGI} going well spend their money? We have created a tool to calculate the optimal spending schedule and tentatively conclude that funders collectively should be spending at least 5\% of their capital each year on {AI} risk interventions and in some cases up to 35\%, depending on views about {AGI} timelines and other key variables.},
	url = {https://forum.effectivealtruism.org/posts/Ne8ZS6iJJp7EpzztP/the-optimal-timing-of-spending-on-agi-safety-work-why-we},
	journaltitle = {Effective Altruism Forum},
	author = {Cook, Tristan and Corlouer, Guillaume},
	urldate = {2022-10-25},
	date = {2022-10-24},
	langid = {english},
	file = {~/Google Drive/library-pdf/Cook2022OptimalTimingSpending.pdf;~/Google Drive/library-html/the-optimal-timing-of-spending-on-agi-safety-work-why-we.html}
}

@Collection{Copp2024OxfordHandbookOf,
	address = {Oxford},
	editor = {Copp, David and Rulli, Tina and Rosati, Connie},
	langid = {english},
	date = {2024},
	database = {Tlön},
	title = {The Oxford Handbook of Normative Ethics},
	publisher = {Oxford University Press}
}

@online{Cotra2022SpecificCountermeasuresEasiest,
	database = {Tlön},
	title = {Without specific countermeasures, the easiest path to
                  transformative {AI} likely leads to {AI} takeover},
	abstract = {I think that in the coming 15-30 years, the world could plausibly develop “transformative {AI}”: {AI} powerful enough to bring us into a new, qualitatively different future, via an explosion in science and technology R\&D. This sort of {AI} could be sufficient to make this the most important century of all time for humanity.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/Y3sWcbcF7np35nzgu/without-specific-countermeasures-the-easiest-path-to-1},
	journaltitle = {Effective Altruism Forum},
	author = {Cotra, Ajeya},
	urldate = {2022-07-19},
	date = {2022-07-18},
	file = {~/Google Drive/library-pdf/Cotra2022SpecificCountermeasuresEasiest.pdf;~/Google
                  Drive/library-html/Without specific countermeasures,
                  the easiest path to transformative AI likely leads to
                  AI takeover - EA Forum:without-specific-countermeasures-the-easiest-path-to-1.html}
}

@online{Cotra2023PorQueAlineacion,
	database = {Tlön},
	date = {2023},
	title = {Por qué la alineación de la inteligencia artificial
                  podría ser difícil con las técnicas modernas de
                  aprendizaje profundo},
	author = {Cotra, Ajeya},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Cotra2021WhyAiAlignment}
}

@online{Cotton-Barratt2014CosteffectivenessResearchOverview,
	database = {Tlön},
	title = {Cost-effectiveness of research: overview},
	abstract = {{FHI} is a multidisciplinary research institute at Oxford University studying big picture questions for human civilization.},
	url = {https://www.fhi.ox.ac.uk/cost-effectiveness-of-research-overview/},
	journaltitle = {Future of Humanity Institute},
	author = {Cotton-Barratt, Owen},
	urldate = {2022-08-02},
	date = {2014-12-04},
	langid = {english},
	file = {~/Google Drive/library-pdf/Cotton-Barratt2014CosteffectivenessResearchOverview.pdf;~/Google Drive/library-html/cost-effectiveness-of-research-overview.html}
}

@online{Cotton-Barratt2014FactoringCostEffectiveness,
	file = {~/Google Drive/library-pdf/Cotton-Barratt2014FactoringCostEffectiveness.pdf;~/Google Drive/library-html/Cotton-Barratt2014FactoringCostEffectiveness.html},
	author = {{Cotton-Barratt}, Owen},
	abstract = {{FHI} is a multidisciplinary research institute at Oxford University studying big picture questions for human civilization.},
	database = {Tlön},
	date = {2014-12-23},
	journaltitle = {Future of Humanity Institute},
	langid = {english},
	timestamp = {2023-07-03 20:41:40 (GMT)},
	title = {Factoring cost-effectiveness},
	url = {http://www.fhi.ox.ac.uk/factoring-cost-effectiveness/},
	urldate = {2023-07-03}
}

@report{Cotton-Barratt2019BargainingtheoreticApproachMoral,
	database = {Tlön},
	title = {A bargaining-theoretic approach to moral uncertainty},
	abstract = {This paper explores a new approach to the problem of decision under relevant moral uncertainty. We treat the case of an agent making decisions in the face of moral uncertainty on the model of bargaining theory, as if the decision-making process were one of bargaining among different internal parts of the agent, with different parts committed to different moral theories. The resulting approach contrasts interestingly with the extant “maximise expected choiceworthiness”...},
	langid = {english},
	url = {https://globalprioritiesinstitute.org/a-bargaining-theoretic-approach-to-moral-uncertainty/},
	institution = {Global Priorities Institute, Oxford University},
	author = {Cotton-Barratt, Owen and Greaves, Hilary},
	date = 2019,
	number = {{GPI} working paper no. 4-2019},
	file = {~/Google Drive/library-pdf/Cotton-Barratt2019BargainingtheoreticApproachMoral.pdf}
}

@online{Cotton-Barratt2022Immortality,
	database = {Tlön},
	title = {Against immortality?},
	abstract = {Epistemic status: there are some strong anti-death / pro-immortality sentiments in {EA}/rationalist circles, and it's bugged me that I haven't seen a good articulation of the anti-immortality case. This is a quick attempt to make that case. I'm confused about what to think about immortality overall; I think that it may depend a lot on circumstances/details, and be eventually desirable but not currently desirable.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/P9WGcRRa2cfAGeRvT/against-immortality},
	journaltitle = {Effective Altruism Forum},
	author = {Cotton-Barratt, Owen},
	urldate = {2022-04-30},
	date = {2022-04-28},
	file = {~/Google Drive/library-pdf/Cotton-Barratt2022Immortality.pdf}
}

@online{Cotton-Barratt2022ReportCivilizationalObserver,
	database = {Tlön},
	title = {Report from a civilizational observer on Earth},
	abstract = {Different tips, tricks, and notes on optimizing investments are proposed. Firstly, some websites list high-interest FDIC-insured savings accounts and help open and manage them. Secondly, it is recommended to keep an eye on the borrow rate of stocks when considering an investment. Suggestions are also given on how to use CDs and savings accounts for extra risk-free return, how to avoid capital gains, how to use box spread financing, and how to negotiate with brokers. Other diverse pieces of advice include possibly reducing market exposure during periods of volatility and considering investments such as tax-lien certificates or peer-to-peer loans. Lastly, if one's intention is to eventually donate money, using a tax-deductible entity for investments could be profitable. – AI-generated abstract.},
	url = {https://www.lesswrong.com/posts/KBjRoc7WccbMehnbN/report-from-a-civilizational-observer-on-earth},
	journaltitle = {{LessWrong}},
	author = {Cotton-Barratt, Owen},
	urldate = {2022-07-11},
	date = {2022-07-09},
	langid = {english},
	file = {~/Google Drive/library-html/report-from-a-civilizational-observer-on-earth.html}
}

@online{Cotton-Barratt2023EnBuscaDe,
	database = {Tlön},
	keywords = {coordinación altruista, priorización de causas, marco
                  ITD},
	date = {2023},
	langid = {spanish},
	author = {{Cotton-Barratt}, Owen},
	title = {En busca de oro},
	translator = {Rodríguez, Cristina and Corpas, Javier},
	translation = {Cotton-Barratt2016ProspectingForGold}
}

@online{Cottrell2022SevenWaysBecome,
	database = {Tlön},
	title = {Seven ways to become unstoppably agentic},
	abstract = {Edit in September 2022: I wrote a response to this post here, in which I lay out some concerns, some unexpected costs of taking "agentic" actions for me (and mistakes I’ve made), and things I’ve changed my mind on since writing this post.
 .
Nearly all of the best things that have happened to me in the last year have been a result of actively seeking and asking for those things. I have slowly been developing the skill of agency.
By ‘agency’ I mean the ability to get what you want across different contexts – the general skill of coming up with ambitious goals   and actually achieving them, whatever they are. In my experience, I’ve found this is learnable.},
	url = {https://forum.effectivealtruism.org/posts/Pc3CFbYxPXgyjoDpB/seven-ways-to-become-unstoppably-agentic},
	journaltitle = {Effective Altruism Forum},
	author = {Cottrell, Evie},
	urldate = {2022-06-18},
	date = {2022-06-18},
	langid = {english},
	file = {~/Google Drive/library-pdf/Cottrell2022SevenWaysBecome.pdf;~/Google Drive/library-html/seven-ways-to-become-unstoppably-agentic.html}
}

@book{Cowen1997RiskBusinessCycles,
	database = {Tlön},
	location = {New York},
	langid = {english},
	title = {Risk and business cycles: New and old Austrian
                  perspectives},
	isbn = {0-203-44813-8},
	publisher = {Routledge},
	author = {Cowen, Tyler},
	date = 1997,
	file = {~/Google Drive/library-pdf/CowenRiskBusinessCycles.pdf}
}

@article{Crary2021EffectiveAltruism,
	database = {Tlön},
	title = {Against ‘effective altruism’},
	abstract = {Effective Altruism ({EA}) is a programme for rationalising charitable giving, positioning individuals to do the ‘most good’ per expenditure of money or time. It was first formulated – by two Oxford philosophers just over a decade ago – as an application of ...},
	volume = {2.10},
	url = {https://www.radicalphilosophy.com/article/against-effective-altruism},
	journaltitle = {Radical Philosophy},
	author = {Crary, Alice},
	urldate = {2021-09-14},
	date = 2021,
	langid = {english},
	file = {~/Google Drive/library-pdf/Crary2021EffectiveAltruism.pdf;~/Google Drive/library-html/against-effective-altruism.html}
}

@online{Crashcourse2017PovertyOurResponse,
	date = {2017-01-30},
	journaltitle = {CrashCourse},
	abstract = {We’re picking up where we left off last time, exploring the “ethics of care” and how it applies to extreme poverty. Are we responding to global poverty in a ...},
	author = {CrashCourse},
	database = {Tlön},
	langid = {spanish},
	shorttitle = {Poverty \& Our Response to It},
	timestamp = {2023-07-13 10:04:52 (GMT)},
	title = {Poverty \& our response to it: Crash Course Philosophy \#44},
	url = {https://www.youtube.com/watch?v=D5sknLy7Smo},
	urldate = {2023-07-13}
}

@online{Crawford202019thcenturyProgressStudies,
	database = {Tlön},
	title = {19th-century progress studies},
	abstract = {I wrote earlier about an 1857 plan for the transcontinental railroad (as yet unbuilt), and how it advocated funding the project through the sale of s….},
	langid = {english},
	url = {https://www.lesswrong.com/posts/F4g3RudxafNkoMsc5/19th-century-progress-studies},
	journaltitle = {Effective Altruism Forum},
	author = {Crawford, Jason},
	date = {2020-08-26},
	file = {~/Google Drive/library-pdf/Crawford202019thcenturyProgressStudies.pdf}
}

@book{Crisp1997RoutledgePhilosophyGuidebook,
	database = {Tlön},
	location = {London},
	langid = {english},
	title = {Routledge philosophy guidebook to Mill on
                  utilitarianism},
	isbn = {978-0-415-10977-2},
	series = {Routledge philosophy guidebooks},
	publisher = {Routledge},
	author = {Crisp, Roger},
	date = 1997,
	file = {~/Google Drive/library-pdf/Crisp1997RoutledgePhilosophyGuidebook.pdf}
}

@online{Critch2021WhatMultipolarFailure,
	database = {Tlön},
	title = {What multipolar failure looks like, and Robust
                  Agent-Agnostic Processes ({RAAPs})},
	abstract = {Forum participation can be an effective research strategy due to several advantages: low effort requirement, the identification of missing knowledge, helping to stay up-to-date, and the generation of new ideas. Moreover, frequent comments on topics of interest can lead to improvements in written communication and distillation of complex discussions into more concise formats for future reference. However, traditional research may still be more appropriate for certain domains where the development of ideas requires sustained periods of concentrated work. – AI-generated abstract.},
	langid = {english},
	url = {https://www.alignmentforum.org/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic},
	journaltitle = {{AI} Alignment Forum},
	author = {Critch, Andrew},
	urldate = {2022-04-30},
	date = {2021-03-31},
	file = {~/Google Drive/library-pdf/Critch2021WhatMultipolarFailure.pdf;~/Google Drive/library-html/what-multipolar-failure-looks-like-and-robust-agent-agnostic.html}
}

@book{Crookshank1889HistoryPathologyVaccination,
	database = {Tlön},
	location = {London},
	langid = {english},
	title = {History and pathology of vaccination},
	publisher = {H.K. Lewis},
	author = {Crookshank, Edgar M.},
	date = 1889,
	file = {~/Google Drive/library-pdf/Crookshank1889HistoryPathologyVaccination.pdf}
}

@article{Crump2022SentienceInDecapod,
	author = {Crump, Andrew and Browning, Heather and Schnell, Alex
                  and Burn, Charlotte and Birch, Jonathan},
	title = {Sentience in decapod crustaceans: a general framework
                  and review of the evidence},
	volume = {7},
	number = {32},
	doi = {10.51291/2377-7478.1691},
	url = {https://www.wellbeingintlstudiesrepository.org/animsent/vol7/iss32/1},
	database = {Tlön},
	date = {2022-01-01},
	issn = {2377-7478},
	journaltitle = {Animal Sentience},
	langid = {english},
	shortjournal = {Animal Sentience},
	shorttitle = {Sentience in decapod crustaceans},
	timestamp = {2023-07-04 19:49:21 (GMT)},
	urldate = {2023-07-04}
}

@online{Crunchbase2023EatJust,
	database = {Tlön},
	url = {https://www.crunchbase.com/organization/just-inc},
	langid = {english},
	date = {2023},
	journaltitle = {Crunchbase},
	title = {Eat Just},
	author = {Crunchbase},
	timestamp = {2023-09-13 13:48:58 (GMT)}
}

@book{Culyer2005DictionaryHealthEconomics,
	database = {Tlön},
	location = {Chelthenham},
	abstract = {'Anthony Culyer once again makes a significant contribution to health economics with a Dictionary that is succinct but also comprehensive. It will be particularly valuable to economists entering the health field and to health specialists who lack familiarity with terms originating in economics and statistics.' - Victor R. Fuchs, Stanford University, US.},
	langid = {english},
	title = {The dictionary of health economics},
	isbn = {978-1-84376-208-9},
	pagetotal = 390,
	publisher = {Edward Elgar},
	author = {Culyer, A. J.},
	date = 2005,
	file = {~/Google Drive/library-pdf/Culyer2005DictionaryHealthEconomics.pdf}
}

@online{Dafoe2019AIStrategyPolicy,
	database = {Tlön},
	title = {{AI} strategy, policy, and governance},
	abstract = {Allan Dafoe discusses the space of {AI} governance. The Beneficial {AGI} 2019 Conference: https://futureoflife.org/beneficial-agi-2019/After our Puerto Rico {AI} c...},
	langid = {english},
	url = {https://www.youtube.com/watch?v=2IpJ8TIKKtI},
	journaltitle = {Beneficial {AGI} 2019},
	author = {Dafoe, Allan},
	date = {2019-01-05}
}

@online{Dai2014PotentialAstronomicalWaste,
	database = {Tlön},
	title = {Is the potential astronomical waste in our universe
                  too small to care about?},
	abstract = {In this blog post about the uncertainty surrounding the limits of astronomical computations in our universe, the author contemplates whether the relative insignificance of astronomical waste in a universe with finite computational capacity necessitates a shift in moral priorities. The author considers the implications of a “Moral Parliament” model proposed by Nick Bostrom and Toby Ord as a framework for resolving moral uncertainty. The author notes the potential for counterintuitive outcomes when trading votes between hypothetical universes with different computational capacities and suggests that it would lead to reduced consideration of astronomical waste. The author also ponders the question of whether it is appropriate to make such deals and acknowledges the inherent challenges of applying utility theories to human decision-making. – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/BNbxueXEcm6dCkDuk/is-the-potential-astronomical-waste-in-our-universe-too},
	journaltitle = {{LessWrong}},
	author = {Dai, Wei},
	date = {2014-10-21},
	file = {~/Google Drive/library-pdf/Dai2014PotentialAstronomicalWaste.pdf}
}

@online{Dai2019ForumParticipationResearch,
	database = {Tlön},
	title = {Forum participation as a research strategy},
	abstract = {The article argues that the world is plausible in experiencing an economic growth explosion due to artificial intelligence. It is based on three primary reasons: the historical trend of growth acceleration, the potential of AI to drive growth through an ideas feedback loop, and the prediction of explosive growth when AI is factored into standard growth models. The report considers counterarguments and assigns a probability of at least 10\% to explosive growth occurring by 2100 and at least 25\% to a scenario of growth stagnation. – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/rBkZvbGDQZhEymReM/forum-participation-as-a-research-strategy},
	journaltitle = {{LessWrong}},
	author = {Dai, Wei},
	urldate = {2022-05-10},
	date = {2019-07-30},
	file = {~/Google Drive/library-pdf/Dai2019ForumParticipationResearch.pdf;~/Google Drive/library-html/forum-participation-as-a-research-strategy.html}
}

@online{Dai2020TipsTricksNotes,
	database = {Tlön},
	title = {Tips/tricks/notes on optimizing investments},
	abstract = {The article posits that answering key questions about transformative AI necessitates presuming a near-term development of the technology. This framework, known as AI strategy nearcasting, involves assuming that transformative AI will soon be developed using current methods. Nearcasting offers advantages such as serving as a jumping-off point for further analysis, focusing attention on high-stakes scenarios, and creating a feedback loop for learning. However, it is crucial to recognize the limitations of nearcasting due to its reliance on hypothetical future scenarios. By examining a nearcast scenario where a particular AI project or company is poised to develop transformative AI within a year, the article highlights the potential risk of an AI takeover without adequate preventative measures. – AI-generated abstract.},
	url = {https://www.lesswrong.com/posts/RWna48W4bPr2ChePz/tips-tricks-notes-on-optimizing-investments},
	journaltitle = {{LessWrong}},
	author = {Dai, Wei},
	urldate = {2022-06-15},
	date = {2020-05-06},
	langid = {english},
	file = {~/Google Drive/library-pdf/Dai2020TipsTricksNotes.pdf;~/Google Drive/library-html/tips-tricks-notes-on-optimizing-investments.html}
}

@online{Dalton2017SelectingAppropriateModel,
	database = {Tlön},
	title = {Selecting the appropriate model for diminishing
                  returns},
	abstract = {In a previous post, Max suggested two key ways of thinking about diminishing returns: funding gaps and returns functions. He also set out two classes of considerations that are generally desirable in .},
	langid = {english},
	url = {https://www.centreforeffectivealtruism.org/blog/selecting-the-appropriate-model-for-diminishing-returns/},
	journaltitle = {Centre for Effective Altruism},
	author = {Dalton, Max and Cotton-Barratt, Owen},
	date = {2017-05-23},
	file = {~/Google Drive/library-pdf/Dalton2017SelectingAppropriateModel.pdf}
}

@online{Dalton2018HowAvoidAccidentally,
	journaltitle = {Effective Altruism Global},
	author = {Dalton, Max and Vollmer, Jonas},
	abstract = {How to Avoid Accidentally Having a Negative Impact with Your Project by Max Dalton and Jonas Vollmer from {EA} Global 2018: London.Subscribe to our channel: ht...},
	langid = {english},
	database = {Tlön},
	title = {How to avoid accidentally having a negative impact
                  with your project},
	url = {https://www.youtube.com/watch?v=RU168E9fLIM},
	editora = {Dalton, Max and Volmer, Jonas},
	editoratype = {collaborator},
	date = {2018-10-27},
	note = {Publication title: Effective altruism global
                  tex.editortype: director}
}

@online{Dalton2020ExtremelyRoughResearch,
	database = {Tlön},
	title = {Some extremely rough research on giving and happiness},
	abstract = {A few years back, I was doing an {MSc} in Economics, and trying to take opportunities to do {EA}-ish research projects as I went. There had been some discussion about whether giving caused happiness, so I wrote a short paper that tried to look into that.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/iyFD6iLHYqhzjqHZs/some-extremely-rough-research-on-giving-and-happiness},
	journaltitle = {Effective Altruism Forum},
	author = {Dalton, Max},
	date = {2020-09-09},
	file = {~/Google Drive/library-pdf/Dalton2020ExtremelyRoughResearch.pdf}
}

@online{Dalton2023AcercaDeEste,
	database = {Tlön},
	date = {2023},
	title = {Acerca de este manual},
	author = {Dalton, Max},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Dalton2022AboutThisHandbook}
}

@online{Dalton2023EmpatiaRadical,
	database = {Tlön},
	date = {2023},
	title = {Empatía radical},
	author = {Dalton, Max},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Dalton2022RadicalEmpathy}
}

@online{Dalton2023MentalidadDeEficacia,
	database = {Tlön},
	date = {2023},
	title = {La mentalidad de la eficacia},
	author = {Dalton, Max},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Dalton2022EffectivenessMindset}
}

@online{Dalton2023PonerloEnPractica,
	database = {Tlön},
	date = {2023},
	title = {Ponerlo en práctica},
	author = {Dalton, Max},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Dalton2022PuttingItInto}
}

@online{Dalton2023QueOpinas,
	database = {Tlön},
	date = {2023},
	title = {¿Qué opinas?},
	author = {Dalton, Max},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Dalton2022WhatDoYou}
}

@online{Daniel2020CommentWhatAre,
	database = {Tlön},
	title = {Comment on 'what are the leading critiques of
                  "Longtermism" and related concepts'},
	abstract = {By longtermism I mean  “Longtermism =df the view that the most important determinant of the value of our actions today is how those actions affect the very long-run future.”.
I want to clarify my thoughts around longtermism as an idea - and to understand better why some aspects of how it is used within {EA} make me uncomfortable despite my general support of the idea.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/jiwgT3WvMkwpWP4BC/what-are-the-leading-critiques-of-longtermism-and-related?commentId=pzT6AS2FBsAcZHpBp},
	journaltitle = {Effective Altruism Forum},
	author = {Daniel, Max},
	date = {2020-06-04},
	file = {~/Google Drive/library-pdf/Daniel2020CommentWhatAre.pdf}
}

@online{Daniel2023PorQueRiesgos,
	database = {Tlön},
	date = {2023},
	title = {Por qué los riesgos S son los peores riesgos
                  existenciales y cómo prevenirlos},
	author = {Daniel, Max},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Daniel2017SrisksWhyThey}
}

@online{Davidson2021ReportWhetherAi,
	file = {~/Google Drive/library-html/Davidson2021ReportWhetherAi.html;~/Google Drive/library-pdf/Davidson2021ReportWhetherAi.pdf},
	url = {openphilanthropy.org/research/report-on-whether-ai-could-drive-explosive-economic-growth/},
	abstract = {This study presents a list of propositions concerning digital minds and society, including their consciousness, rights, and moral status. The authors argue that AI minds could differ significantly from human minds in terms of their capabilities, motivations, and consciousness. They emphasize the importance of respecting AI interests and avoiding their exploitation. The study also discusses the alignment problem, suggesting that successful alignment may require more than just compensation for restrictions or benefits. The authors propose ideas for treating AI, such as archiving decommissioned AIs and creating deployment environments that are more rewarding than the training environment. Overall, the study aims to stimulate discussions and guide policymaking in response to the potential advent of advanced artificial intelligence. – AI-generated abstract.},
	langid = {english},
	database = {Tlön},
	date = {2021-06-17},
	journaltitle = {Open Philanthropy},
	title = {Report on whether AI could drive explosive economic growth},
	author = {Davidson, Tom},
	timestamp = {2023-08-11 15:44:59 (GMT)}
}

@online{Davis2020HecklerVetoAlso,
	database = {Tlön},
	title = {The heckler's veto is also subject to the
                  unilateralist's curse},
	abstract = {Unilateralists may take individual actions that deviate from the group's common interest due to positive error terms in their assessments. This problem extends to situations where a single individual can act or veto an initiative on behalf of a group, making the group vulnerable to unilateralist errors. Karma systems, such as those used on online platforms, may mitigate this risk by penalizing harmful posts and offering feedback about their perceived value. However, the effectiveness of karma systems in preventing unilateralist actions depends on the assumption that users reliably downvote infohazardous content. – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/oREzdHpzaB2RaY2fA/the-heckler-s-veto-is-also-subject-to-the-unilateralist-s},
	journaltitle = {{LessWrong}},
	author = {Davis, Zack M.},
	date = {2020-03-09},
	file = {~/Google Drive/library-pdf/Davis2020HecklerVetoAlso.pdf}
}

@online{Davis2022CommentPropositionsConcerning,
	database = {Tlön},
	title = {Comment on "Propositions concerning digital minds and
                  society"},
	abstract = {The annual Effective Altruism Global conference welcomes individuals immersed in effective altruism concepts and actions. Unlike EAGx gatherings, this event assumes attendees possess a firm grasp of foundational ideas and engages in earnest decision-making and actions based on them. The conference attracts a diverse crowd from around the world and features talks, discussions, and networking opportunities. – AI-generated abstract.},
	url = {https://www.lesswrong.com/posts/nqXcgsqQBw2doAnXu/comment-on-propositions-concerning-digital-minds-and-society},
	journaltitle = {{LessWrong}},
	author = {Davis, Zach M.},
	urldate = {2022-07-11},
	date = {2022-07-10},
	langid = {english},
	file = {~/Google Drive/library-pdf/Davis2022CommentPropositionsConcerning.pdf;~/Google Drive/library-html/comment-on-propositions-concerning-digital-minds-and-society.html}
}

@online{Deere20212021EAFunds,
	database = {Tlön},
	title = {The 2021 {EA} Funds Donor Lottery is now open},
	abstract = {The 2021 {EA} Funds donor lottery is now open, with block sizes of \$100k, \$500k, and \$2 million. To enter a lottery, or to learn more about donor lotteries, head to the {EA} Funds donor lottery page.
You can also enter the lottery through the {EA} Giving Tuesday Facebook Match (base donation only; any matched funds can be allocated to a Fund or non-profit of your choice). See this document for more information about entering the lottery through {EA} Giving Tuesday.Update: Winners have been drawn on Monday Jan 24 2022.
We’ve written before about why you should give to a donor lottery this giving season. If you’re intrigued about what a donor lottery is, or why you might want to enter, I’d recommend reading that article, including the comments (note that it’s from last year, so some specifics like block sizes and dates are different). I’ve copied in some of the key points below:.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/Ci56F69xZDvy9LokL/the-2021-ea-funds-donor-lottery-is-now-open},
	journaltitle = {Effective Altruism Forum},
	author = {Deere, Sam},
	urldate = {2022-01-05},
	date = {2021-11-29},
	file = {~/Google Drive/library-pdf/Deere20212021EAFunds.pdf;~/Google
                  Drive/library-html/The 2021 EA Funds Donor Lottery is
                  now open - EA Forum:the-2021-ea-funds-donor-lottery-is-now-open.html}
}

@online{Demski2020BettingMandatoryPostmortem,
	database = {Tlön},
	title = {Betting with mandatory post-mortem},
	abstract = {The article presents the concept of betting with mandatory post-mortem analysis. The author suggests that when making a bet, the loser should be required to write a detailed analysis of why they were wrong. This analysis should be shared with the winner and other interested parties. The author argues that this practice would improve the quality of decision-making and promote a culture of learning and intellectual honesty. Additionally, it helps ensure that some cognitive labor is devoted to the update, rather than relying solely on the pain of lost cash. The author believes that this type of betting would be beneficial for both the participants and the audience, as it would provide valuable insights into the structure of disagreements and facilitate a more comprehensive update. – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/AM5JiWfmbAytmBq82/betting-with-mandatory-post-mortem},
	journaltitle = {{LessWrong}},
	author = {Demski, Abram},
	date = {2020-01-24},
	file = {~/Google Drive/library-pdf/Demski2020BettingMandatoryPostmortem.pdf}
}

@online{Denkenberger2019CivilizationalCollapseScenarios,
	database = {Tlön},
	title = {Civilizational collapse: Scenarios, prevention,
                  responses},
	abstract = {Civilizational Collapse: Scenarios, Prevention, Responses - a Foresight salon held June 24th 2019 with Dr. David Denkenberger \& Jeffrey Ladish in San Francis...},
	langid = {english},
	url = {https://www.youtube.com/watch?v=gbYWHBoQ9gM&},
	journaltitle = {Foresight Institute},
	author = {Denkenberger, David and Ladish, Jeffrey},
	date = {2019-06-24}
}

@Book{Dentreves1962SantoTomasDe,
	database = {Tlön},
	translation = {Dentreves1948AquinasSelectedPolitical},
	date = {1962},
	langid = {spanish},
	address = {Caracas},
	publisher = {Instituto de Estudios Políticos},
	translator = {Polache Zapata, Carlos},
	editor = {D'Entreves, A. P.},
	title = {Santo Tomás de Aquino: Escritos políticos},
	timestamp = {2023-06-28 12:47:03 (GMT)}
}

@online{Department2012NationalStandardsTo,
	database = {Tlön},
	author = {Justice Department},
	abstract = {The Department of Justice (Department) is issuing a final rule adopting national standards to prevent, detect, and respond to prison rape, as required by the Prison Rape Elimination Act of 2003 ({PREA}). In addition, the Department is requesting comment on one issue relating to staffing in juvenile...},
	langid = {english},
	date = {2012-06-20},
	journaltitle = {Federal Register},
	timestamp = {2023-06-07 18:26:04 (GMT)},
	title = {National standards to prevent, detect, and respond to
                  prison rape},
	url = {https://www.federalregister.gov/documents/2012/06/20/2012-12427/national-standards-to-prevent-detect-and-respond-to-prison-rape},
	urldate = {2023-06-07}
}

@report{Devereux2002SocialProtectionPoor,
	url = {https://opendocs.ids.ac.uk/opendocs/bitstream/handle/20.500.12413/3907/Wp142.pdf},
	database = {Tlön},
	langid = {english},
	title = {Social protection for the poor: lessons from recent
                  international experience},
	institution = {Institute of Development Studies},
	author = {Devereux, Stephen},
	date = 2002,
	number = {{IDS} working paper no. 142},
	file = {~/Google Drive/library-pdf/Devereux2002SocialProtectionPoor.pdf}
}

@online{Dhyani2014500MillionBut,
	database = {Tlön},
	file = {~/Google Drive/library-html/Dhyani2014500MillionBut.html;~/Google Drive/library-pdf/Dhyani2014500MillionBut.pdf},
	abstract = {Author's note: In 2023 this piece received two edits for historical accuracy. See comment here for details.We will never know their names.
The first victim could not have been recorded, for there was no written language to record it. They were someone’s daughter, or son, and someone’s friend, and they were loved by those around them. And they were in pain, covered in rashes, confused, scared, not knowing why this was happening to them or what they could do about it — victims of a mad, inhuman god. There was nothing to be done — humanity was not strong enough, not aware enough, not knowledgeable enough, to fight back against a monster that could not be seen.},
	langid = {english},
	author = {Dhyani, Jai},
	date = {2014-12-08},
	timestamp = {2023-02-17 21:31:09 (GMT)},
	title = {500 million, but not a single one more},
	journaltitle = {Effective Altruism Forum},
	url = {https://forum.effectivealtruism.org/s/B79ro5zkhndbBKRRX/p/jk7A3NMdbxp65kcJJ},
	urldate = {2023-02-17}
}

@online{Diabate2019TargetMalariaProceeded,
	database = {Tlön},
	title = {Target Malaria proceeded with a small-scale release of
                  genetically modified sterile male mosquitoes in Bana,
                  a village in Burkina Faso},
	abstract = {The release of genetically modified sterile male mosquitoes in Burkina Faso is a very important milestone for our project. It is the first of its kind on the African continent and our team has been working since 2012 to reach this point.  Bana is one of the project sites we have been working in collaboration […].},
	langid = {english},
	url = {https://targetmalaria.org/target-malaria-proceeded-with-a-small-scale-release-of-genetically-modified-sterile-male-mosquitoes-in-bana-a-village-in-burkina-faso/},
	journaltitle = {Target Malaria's blog},
	author = {Diabate, Abdoulaye},
	date = {2019-07-01},
	file = {~/Google Drive/library-pdf/Diabate2019TargetMalariaProceeded.pdf}
}

@online{Dickens2016PostEAPredictions,
	database = {Tlön},
	title = {Post on the {EA} predictions group},
	abstract = {Log into Facebook to start sharing and connecting with your friends, family, and people you know.},
	langid = {english},
	url = {https://www.facebook.com/groups/eapredictons/permalink/1120537274730930/},
	journaltitle = {Facebook},
	author = {Dickens, Michael},
	date = {2016-12-08}
}

@online{Dickens2020GivingNowVs,
	database = {Tlön},
	title = {Giving now vs. later for existential risk: an initial
                  approach},
	abstract = {This essay presents a variety of simple models on giving now vs. later for existential risk.
On the whole, these models do not strongly favor either option. Giving now looks better under certain plausible assumptions, and giving later looks better under others.
.
On the simplest possible model with no movement growth and no external actors, giving later looks better.
Higher movement growth/external spending pushes more in favor of giving now.
If our efforts can only temporarily reduce x-risk, we should spend a proportion of our budget in each period, rather than spending or saving all of it.
.
.
It has been argued that, because philanthropists are more patient than most actors, they should give later. This argument does not necessarily work for existential risk.
The probability of extinction has relatively little effect on when to give.
.
Last updated 2020-09-08.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/3fmcNMrR8cktLnoYk/giving-now-vs-later-for-existential-risk-an-initial-approach#Derivation_of_the_probability_of_extinction_p1_x},
	journaltitle = {Effective Altruism Forum},
	author = {Dickens, Michael},
	date = {2020-08-28},
	file = {~/Google Drive/library-pdf/Dickens2020GivingNowVs.pdf}
}

@online{Dickens2020ShouldWePrioritize,
	database = {Tlön},
	title = {Should we prioritize long-term existential risk?},
	abstract = {We should reduce existential risk in the long term, not merely over the next century. We might best do this by developing longtermist institutions  that will operate to keep existential risk persistently low.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/L4ExK2dN2dkxw6SCw/should-we-prioritize-long-term-existential-risk},
	journaltitle = {Effective Altruism Forum},
	author = {Dickens, Michael},
	date = {2020-08-19},
	file = {~/Google Drive/library-pdf/Dickens2020ShouldWePrioritize.pdf}
}

@online{Donnelly2017MoralTrade,
	database = {Tlön},
	title = {Moral trade},
	abstract = {A moral trade occurs when individuals with different values cooperate to produce an outcome that's better according to both their values than what they could have achieved individually.
In the future, we may post a transcript for this {EA} Global: San Francisco 2017 talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact Aaron Gertler — he can help you get started.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/5x8Qhy4bB8Arj5SBS/ruairi-donnelly-moral-trade},
	journaltitle = {Effective altruism global},
	author = {Donnelly, Ruairí},
	date = {2017-08-11},
	file = {~/Google Drive/library-pdf/Donnelly2017MoralTrade.pdf}
}

@online{Dreyfus2020EAConsiderationsRegarding,
	database = {Tlön},
	title = {{EA} considerations regarding increasing political
                  polarization},
	abstract = {American politics has become increasingly polarized in recent years. During the ongoing George Floyd protests, observers have pointed out that polarization has hit highs not yet seen in modern American history. Whether this trend of increasing polarization will continue is unclear. However, it is at least plausible that the trend is far from over, and therefore, broad picture implications are worth closer attention.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/SitudkgqA5Gnwfxdz/ea-considerations-regarding-increasing-political},
	journaltitle = {Effective Altruism Forum},
	author = {Dreyfus, Alfred},
	date = {2020-06-19},
	file = {~/Google Drive/library-pdf/Dreyfus2020EAConsiderationsRegarding.pdf}
}

@online{DualUseResearchNIH,
	database = {Tlön},
	title = {Dual-Use Research},
	langid = {english},
	url = {https://oir.nih.gov/sourcebook/ethical-conduct/special-research-considerations/dual-use-research},
	author = {{National Institutes of Health}},
	urldate = {2022-02-09},
	file = {~/Google Drive/library-html/Dual-Use Research}
}

@online{Dullaghan2021CulturedMeatPredictions,
	database = {Tlön},
	date = {2021-09-15},
	abstract = {Update 2022 Jan 4: I resolved outstanding end of year 2021 predictions. Of the now 90 resolved predictions about cultured meat being on the market, 10 resolved positively \& 80 resolved negatively. All negatively resolving predictions expected more cultured meat to be on sale now than in reality- i.e. the predictions here remain overly optimistic about cultured meat timelines.In a 2021 {MotherJones} article, Sinduja Rangarajan, Tom Philpott, Allison Esperanza, and Alexis Madrigal compiled and visualized 186 publicly available predictions about timelines for cultured meat (made primarily by cultured meat companies and a handful of researchers). I added 11 additional predictions {ACE} had collected, and 76 other predictions I found in the course of a forthcoming Rethink Priorities project.},
	journaltitle = {Effective Altruism Forum},
	author = {Dullaghan, Neil},
	title = {Cultured Meat Predictions Were Overly Optimistic},
	url = {https://forum.effectivealtruism.org/posts/YYurNqQDAWNiQJv9K/cultured-meat-predictions-were-overly-optimistic},
	langid = {english},
	timestamp = {2023-07-20 18:58:55 (GMT)},
	urldate = {2023-07-20}
}

@online{E.2020CorrelationsCausePrioritization,
	database = {Tlön},
	title = {Correlations between cause prioritization and the big
                  five personality traits},
	abstract = {Late Edit: This post received way more attention than I expected. For important context, please see David Moss's first comment, especially his helpful visualization. "One thing worth bearing in mind is that these are very small proportions of the responses overall..." I am ultimately talking about small groups of people within the total number of survey respondents, and although I think my claims are true, I believe they are trivially so; I created this post largely for fun and practice, not for making important claims.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/F6gMCRwAzxNL4cwjv/correlations-between-cause-prioritization-and-the-big-five},
	journaltitle = {Effective Altruism Forum},
	author = {E., Elizabeth},
	date = {2020-09-24},
	file = {~/Google Drive/library-pdf/E.2020CorrelationsCausePrioritization.pdf}
}

@online{Eappen2019FortitudeCollaborationHumility,
	database = {Tlön},
	title = {Fortitude, collaboration, and humility: lessons from
                  an {EA}-aligned charity startup},
	abstract = {When {GiveWell} wrote that they were looking for charities to work on micronutrient fortification, Fortify Health rose to the challenge. With help from a \$300,000 {GiveWell} grant, they began to work on wheat flour fortification, hoping to reduce India’s rate of iron deficiency. In this talk, co-founder Brendan Eappen discusses the charity’s story and crucial decisions they faced along the way. He also offers advice to members of the effective altruism community interested in pursuing similar work in the field of global development.Below is a transcript of Brendan’s talk, which has been lightly edited for clarity. You can also watch it on {YouTube} and read it on effectivealtruism.org.
I want to talk about what we're doing at Fortify Health, and then, more broadly, about some of the central tensions [I’ve experienced] as someone who started an effective altruism [{EA}]-aligned charity startup in a world of other global health actors.
 .
.
Our goal at Fortify Health is to improve population health by addressing widespread iron-deficiency anemia and neural tube defects in India. We're doing that through fortification (i.e., adding vitamins and minerals like iron, folic acid, and vitamin B12 to the foods that people already eat).},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/eMDBygj4DMaRGcZx7/brendan-eappen-lessons-from-an-ea-aligned-charity-startup},
	shorttitle = {Brendan Eappen},
	journaltitle = {Effective Altruism Global},
	author = {Eappen, Brendan},
	urldate = {2022-03-28},
	date = {2019-06-23},
	file = {~/Google Drive/library-html/brendan-eappen-lessons-from-an-ea-aligned-charity-startup.html}
}

@online{Easterly2009CiteGoodNews,
	database = {Tlön},
	title = {Some cite good news on aid},
	abstract = {A paper forthcoming in the  Journal of Economic Literature  states:.
"There are well known and striking donor success stories, like the elimination of smallpox, the near-eradication of river blindness and Guinea worm, the spread of oral rehydration therapy for treating infant diarrheal diseases,.},
	langid = {english},
	url = {https://www.nyudri.org/aidwatcharchive/2009/02/some-cite-good-news-on-aid},
	journaltitle = {Aid watch},
	author = {Easterly, William},
	date = {2009-02-18},
	file = {~/Google Drive/library-pdf/Easterly2009CiteGoodNews.pdf}
}

@book{Edmonds2018FilosofosMiranHacia,
	translation = {Edmonds2016PhilosophersTakeWorld},
	database = {Tlön},
	langid = {spanish},
	date = {2018},
	publisher = {Cátedra},
	address = {Madrid},
	isbn = {9788437638911},
	editor = {Edmonds, David},
	title = {Los filósofos miran hacia el mundo},
	timestamp = {2023-05-25 21:06:27 (GMT)}
}

@online{Edson2022AnnouncingPhilosophyFellowship,
	database = {Tlön},
	title = {Announcing a philosophy fellowship for {AI} safety},
	abstract = {The Center for AI Safety is announcing a paid fellowship for philosophy PhD students and postdoctorates to work on conceptual problems in AI safety from January to August 2023. The program will feature guest lectures from top philosophers and AI safety experts and offer a \$60,000 grant, covered student fees, and a housing stipend. The fellowship aims to attract philosophy talent with the potential to produce high-quality conceptual AI safety research, which has been proven to be highly impactful in orienting the field and influencing important research directions. – AI-generated abstract.},
	url = {https://forum.effectivealtruism.org/posts/6aaQuenrF9BYvsZwv/announcing-a-philosophy-fellowship-for-ai-safety},
	journaltitle = {Effective Altruism Forum},
	author = {Edson, Anders and Zhang, Oliver and Hendrycks, Dan},
	urldate = {2022-09-07},
	date = {2022-09-07},
	langid = {english},
	file = {~/Google Drive/library-pdf/Edson2022AnnouncingPhilosophyFellowship.pdf}
}

@online{EffectiveAltruismFoundation2017CaseSpeciesism,
	database = {Tlön},
	title = {The case against speciesism},
	abstract = {This collection of articles was first published on the website of Sentience Politics. A full-grown horse or dog is beyond comparison a more rational, as well as a more conversible animal, than an infant of a day, a week or even a month old. But suppose the case were otherwise, what would it avail? The question is not, Can they reason? nor, Can they talk? but, Can they suffer? – Jeremy Bentham From Bentham to Singer: A brief history of impartiality It is rare that a philosopher will make arguments that stand the test of time. Especially 227 years of it. But in 1789, British philosopher Jeremy Bentham made an argument, the argument, for anti-speciesism that remains as relevant today […].},
	langid = {english},
	url = {https://ea-foundation.org/blog/the-case-against-speciesism/},
	journaltitle = {Effective Altruism Foundation},
	author = {{Effective Altruism Foundation}},
	date = {2017-04-11},
	file = {~/Google Drive/library-pdf/EffectiveAltruismFoundation2017CaseSpeciesism.pdf}
}

@online{EffectiveAltruismGlobal2022EAGlobalEffective,
	database = {Tlön},
	title = {{EA} Global - The effective altruism community's
                  annual conference},
	abstract = {The article argues that directing efforts and resources mainly to increasing the frequency and effectiveness of organizing regular local meetups for people interested in effective altruism, rather than supporting the organizing of retreats, is a better strategy to support and promote the growth of effective altruism in London. – AI-generated abstract.},
	langid = {english},
	url = {https://www.eaglobal.org/},
	journaltitle = {Effective Altruism Global},
	author = {{Effective Altruism Global}},
	urldate = {2022-09-20},
	date = 2022,
	file = {~/Google Drive/library-html/www.eaglobal.org.html}
}

@online{EffectiveAltruismInfrastructureFund2022SeptemberDecember2021EA,
	database = {Tlön},
	title = {September-December 2021: {EA} Infrastructure Fund},
	langid = {english},
	url = {https://funds.effectivealtruism.org/funds/payouts/september-december-2021-ea-infrastructure-fund},
	journaltitle = {Effective Altruism Funds},
	author = {{Effective Altruism Infrastructure Fund}},
	urldate = {2022-07-15},
	date = {2022-07}
}

@online{EffectiveAltruismLondon2019EffectiveAltruismLondon,
	database = {Tlön},
	title = {Effective Altruism London strategy},
	abstract = {The strategy of Effective Altruism (EA) London prioritizes activities that foster coordination and collaboration among EA adherents. This includes promoting community-wide events, establishing working groups focused on specific causes, and providing resources and support to EA-aligned initiatives. The strategy acknowledges that certain activities, such as organizing retreats, are deprioritized due to their resource-intensive nature and limited impact compared to more regular and accessible activities. – AI-generated abstract.},
	url = {https://docs.google.com/document/d/1OlqrxnpHSEhstuexv4Se9MWDbiYVq-lk41zw4-vLbMw/edit?usp=sharing&usp=embed_facebook},
	journaltitle = {Effective Altruism London},
	author = {{Effective Altruism London}},
	urldate = {2022-04-08},
	date = 2019,
	langid = {english},
	file = {~/Google Drive/library-pdf/EffectiveAltruismLondon2019EffectiveAltruismLondon.pdf;~/Google Drive/library-html/edit.html}
}

@online{EffectiveVenturesOps2022CEAOpsNow,
	database = {Tlön},
	title = {{CEA} Ops is now {EV} Ops},
	abstract = {{EV} Ops is a passionate and driven group of operations specialists who want to use our skills to do the most good in the world.
You can read more about us at https://ev.org/ops.},
	url = {https://forum.effectivealtruism.org/posts/9gHTYC5qbSH9E37vx/cea-ops-is-now-ev-ops},
	journaltitle = {Effective Altruism Forum},
	author = {{Effective Ventures Ops}},
	urldate = {2022-09-20},
	date = {2022-09-13},
	langid = {english},
	file = {~/Google Drive/library-pdf/EffectiveVenturesOps2022CEAOpsNow.pdf;~/Google Drive/library-html/cea-ops-is-now-ev-ops.html}
}

@online{Eliosoff2022HowMuchCurrent,
	database = {Tlön},
	title = {How much current animal suffering does longtermism let
                  us ignore?},
	abstract = {Some thoughts on whether/why it makes sense to work on animal welfare, given longtermist arguments.  {TLDR}:.
We should only deprioritize the current suffering of billions of farmed animals, if we would similarly deprioritize comparable treatment of millions of humans; and,We should double-check that our arguments aren't distorted by status quo bias, especially power imbalances in our favor.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/fo6xBBJpbpeAyQJSj/how-much-current-animal-suffering-does-longtermism-let-us},
	shorttitle = {How much current animal suffering does longtermism let
                  us ignore?},
	journaltitle = {Effective Altruism Forum},
	author = {Eliosoff, Jacob},
	urldate = {2022-04-21},
	date = {2022-04-21},
	file = {~/Google Drive/library-pdf/Eliosoff2022HowMuchCurrent.pdf;~/Google Drive/library-html/how-much-current-animal-suffering-does-longtermism-let-us.html}
}

@online{Elmore2016CharityHacks,
	database = {Tlön},
	title = {Charity hacks},
	abstract = {Most of these hacks have made me personally happier, and not just by providing me with more money to donate. They make me feel more in control of my life, more efficient with my resources. Viewing ….},
	url = {https://mhollyelmoreblog.wordpress.com/2016/08/16/charity-hacks/},
	journaltitle = {Holly Elmore's Blog},
	author = {Elmore, Holly},
	urldate = {2021-09-04},
	date = {2016-08-16},
	langid = {english},
	file = {~/Google Drive/library-pdf/Elmore2016CharityHacks.pdf;~/Google Drive/library-html/Elmore2016CharityHacks.html}
}

@online{Elmore2017RememberingSelfNeeds,
	database = {Tlön},
	title = {The remembering self needs to get real about the
                  experiencing self},
	abstract = {The remembering self needs to get real about the experiencing self. Momentary pleasures are not bad– what’s bad is not getting more of them. It seems to me like behavioral economics tak….},
	url = {https://mhollyelmoreblog.wordpress.com/2017/04/18/the-remembering-self-needs-to-get-real-about-the-experiencing-self/},
	journaltitle = {Holly Elmore's Blog},
	author = {Elmore, Holly},
	urldate = {2021-09-04},
	date = {2017-04-18},
	langid = {english},
	file = {~/Google Drive/library-pdf/Elmore2017RememberingSelfNeeds.pdf;~/Google Drive/library-html/Elmore2017RememberingSelfNeeds.html}
}

@online{Elmore2019ScrupulosityMyEAGxBoston,
	database = {Tlön},
	title = {Scrupulosity: my {EAGxBoston} 2019 lightning talk},
	abstract = {This was a 5 minute talk, so I basically only had time to read the slides (dynamically!). I’m going to provide the slides and whatever extra info I said at the time in italics and give commen….},
	url = {https://mhollyelmoreblog.wordpress.com/2019/05/02/scrupulosity-my-eagxboston-2019-lightning-talk/},
	shorttitle = {Scrupulosity},
	journaltitle = {Holly Elmore's Blog},
	author = {Elmore, Holly},
	urldate = {2021-09-04},
	date = {2019-05-02},
	langid = {english},
	file = {~/Google Drive/library-html/Elmore2019ScrupulosityMyEAGxBoston.html;~/Google Drive/library-pdf/Elmore2019ScrupulosityMyEAGxBoston.pdf}
}

@online{Elmore2023EstamosEnTriaje,
	date = {2023},
	title = {Estamos en triaje cada segundo de cada día},
	database = {Tlön},
	author = {Elmore, Holly},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Elmore2016WeAreTriage}
}

@online{Elmore2023YoQueRecuerda,
	database = {Tlön},
	date = {2023},
	author = {Elmore, Holly},
	langid = {spanish},
	title = {El yo que recuerda debería tomar en serio al yo que
                  experimenta},
	translator = {Humarán, Aurora},
	translation = {Elmore2017RememberingSelfNeeds}
}

@misc{Else1981DayTrinity,
	database = {Tlön},
	title = {The day after Trinity},
	abstract = {1h 28m {\textbar} Not Rated.},
	langid = {english},
	url = {https://www.imdb.com/title/tt0080594/?ref_=fn_al_tt_1},
	publisher = {{KTEH}},
	author = {Else, Jon},
	date = 1981,
	note = {Publication title: Internet movie database
                  tex.editortype: director}
}

@online{Engler2022EUAIAct,
	database = {Tlön},
	title = {The {EU} {AI} Act will have global impact, but a
                  limited Brussels effect},
	abstract = {The European Union’s ({EU}) {AI} Act ({AIA}) aspires to establish the first comprehensive regulatory scheme for artificial intelligence, but its impact will not stop at the {EU}’s borders. In fact, some {EU} policymakers believe it is a critical goal of the {AIA} to set a worldwide standard, so much so that some refer to a […].},
	url = {https://www.brookings.edu/research/the-eu-ai-act-will-have-global-impact-but-a-limited-brussels-effect/},
	journaltitle = {Brookings},
	author = {Engler, Alex},
	urldate = {2022-06-12},
	date = {2022-06-08},
	langid = {english},
	file = {~/Google Drive/library-pdf/Engler2022EUAIAct.pdf;~/Google Drive/library-html/the-eu-ai-act-will-have-global-impact-but-a-limited-brussels-effect.html}
}

@online{Esparza2021IntroducingHealthierHens,
	database = {Tlön},
	title = {Introducing Healthier Hens},
	abstract = {We are pleased to announce Healthier Hens, a new {EA}-aligned animal welfare organization. Our vision is to cost-effectively improve the well-being of millions of farmed egg-laying hens. We are investigating the potential of an intervention focused on improving the nutritional quality of layer hen feed, as recommended by the Charity Entrepreneurship ({CE}) research team. We are proud graduates of the {CE} Incubation Program’s 2021 cohort, and our initial work is supported by a \$100,000 seed grant.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/4QGhyXjXM4yJBvNap/introducing-healthier-hens},
	journaltitle = {Effective Altruism Forum},
	author = {Esparza, Isaac and Jasiunas, Lukas},
	urldate = {2022-05-09},
	date = {2021-10-25},
	file = {~/Google Drive/library-pdf/Esparza2021IntroducingHealthierHens.pdf;~/Google Drive/library-html/Esparza2021IntroducingHealthierHens.html}
}

@online{Estier2023ResponseToOurb,
	database = {Tlön},
	file = {~/Google Drive/library-pdf/Estier2023ResponseToOurb.pdf;~/Google Drive/library-html/Estier2023ResponseToOurb.html},
	abstract = {The Executive Office of the Secretary-General ({EOSG}) of the United Nations is publishing a series of policy briefs to inform the Our Common Agenda processes. On….},
	date = {2023-03-21},
	journaltitle = {Simon Institute for Longterm Governance},
	author = {Estier, Malou},
	langid = {english},
	shorttitle = {Response to Our Common Agenda Policy Brief 2},
	timestamp = {2023-06-15 20:05:39 (GMT)},
	title = {Response to Our Common Agenda Policy Brief 2:
                  "Strengthening the International Response to Complex
                  Global Shocks - An Emergency Platform"},
	url = {https://www.simoninstitute.ch/blog/post/response-to-our-common-agenda-policy-brief-2-%E2%80%9Cstrengthening-the-international-response-to-complex-global-shocks-%E2%80%93-an-emergency-platform%E2%80%9D/},
	urldate = {2023-06-15}
}

@online{Estier2023RespuestaAlInformeb,
	database = {Tlön},
	date = {2023},
	author = {Estier, Malou},
	langid = {spanish},
	title = {Respuesta al Informe de políticas de Nuestra Agenda
                  Común 2: "Reforzar la respuesta internacional en caso
                  de crisis mundiales complejas - Una Plataforma de
                  Emergencia"},
	translator = {Tlön},
	translation = {Estier2023ResponseToOurb}
}

@online{Esvelt2020MitigatingCatastrophicBiorisks,
	database = {Tlön},
	title = {Mitigating catastrophic biorisks},
	abstract = {In a world now painfully aware of pandemics, and with ever-increasing access to autonomous biological agents, how can we help channel society’s response to {COVID}-19 to minimize the risk of deliberate misuse? Using the challenge of securing {DNA} synthesis as an example, Kevin Esvelt outlines the key norms and incentives governing biotechnology, lays out potential strategies for reform, and suggests ways in which thoughtful individuals might safely and credibly discuss and mitigate biorisks without spreading information hazards.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/9iPdD5veF78kQmhiv/kevin-esvelt-mitigating-catastrophic-biorisks},
	journaltitle = {Effective Altruism Forum},
	author = {Esvelt, Kevin M.},
	date = {2020-09-03},
	file = {~/Google Drive/library-pdf/Esvelt2020MitigatingCatastrophicBiorisks.pdf}
}

@online{Ezell2022AnnouncingSpaceFutures,
	database = {Tlön},
	title = {Announcing the Space Futures Initiative},
	abstract = {We are excited to announce the Space Futures Initiative, which has a mission of conducting research, promoting education, and engaging in outreach to improve the long-term future in outer space.
We believe sharing longtermist ideas within the space community and researching scalable space governance frameworks are currently neglected areas. Conducting these activities requires engagement and input from the longtermist community, space policy community, space industry, scientific community, and beyond. The Space Futures Initiative aims to bring together students and academic researchers, produce valuable long-term space futures research, and collaborate with key stakeholders to discuss positive long-term space futures that benefit all of humanity.},
	url = {https://forum.effectivealtruism.org/posts/x5czbdCW6SqnLJhbD/announcing-the-space-futures-initiative},
	journaltitle = {Effective Altruism Forum},
	author = {Ezell, Carson and Chang, Madeleine and Willner, Olaf},
	urldate = {2022-09-15},
	date = {2022-09-12},
	langid = {english},
	file = {~/Google Drive/library-pdf/Ezell2022AnnouncingSpaceFutures.pdf;~/Google Drive/library-html/announcing-the-space-futures-initiative.html}
}

@report{Ezell2022SpaceGovernanceRisks,
	database = {Tlön},
	title = {Space governance: Risks, frameworks, and futures},
	langid = {english},
	url = {https://spacefuturesinitiative.org/research-papers/risks-frameworks-futures/},
	institution = {Space Futures Initiative},
	author = {Ezell, Carson},
	date = {2022-08-27},
	file = {~/Google Drive/library-pdf/Ezell2022SpaceGovernanceRisks.pdf}
}

@online{Fahey2017WhatWeKnow,
	database = {Tlön},
	journaltitle = {Effective Altruism Global},
	abstract = {Is universal basic income a viable way to support humans in the face of technological change? As technology advances, fewer jobs require human labor. Governm...},
	date = {2017},
	author = {Fahey, Alison},
	langid = {english},
	shorttitle = {What we know and don't know about Universal Basic Income},
	timestamp = {2023-04-24 12:36:07 (GMT)},
	title = {What we know and don't know about Universal Basic Income},
	url = {https://www.youtube.com/watch?v=TZlk55GUYkY},
	urldate = {2023-04-24}
}

@online{FarmedAnimalFunders2021,
	database = {Tlön},
	title = {About},
	langid = {english},
	url = {https://farmedanimalfunders.org/about/},
	journaltitle = {Farmed Animal Funders},
	author = {{Farmed Animal Funders}},
	date = 2021
}

@online{Farquhar2016ShouldEAsPolicy,
	database = {Tlön},
	title = {Should {EAs} do policy?},
	abstract = {Discuss this talk on the Effective Altruism Forum: https://forum.effectivealtruism.org/posts/{BzeDpj}6n3NSKqvSZJ/sebastian-farquhar-should-eas-do-{policyTo} get ...},
	langid = {english},
	url = {https://www.youtube.com/watch?v=NB_edlOrPOU&list=PLwp9xeoX5p8P_O5rQg-SNMwQOIvOPF5U2&index=10},
	journaltitle = {Effective Altruism Global},
	author = {Farquhar, Sebastian},
	date = {2016-08-05}
}

@online{Favaloro2021TechnicalUpdatesTo,
	database = {Tlön},
	file = {~/Google Drive/library-pdf/Favaloro2021TechnicalUpdatesTo.pdf},
	abstract = {In 2019, we wrote a blog post about how we think about the "bar" for our giving and how we compare different kinds of interventions to each other using back-of-the-envelope calculations, all within the realm of what we now call Global Health and Wellbeing ({GHW}). This post updates that one and: Explains how we previously….},
	author = {Favaloro, Peter and Berger, Alexander},
	date = {2021-11-18},
	journaltitle = {Open Philanthropy},
	langid = {english},
	timestamp = {2023-06-07 17:27:25 (GMT)},
	title = {Technical updates to our global health and wellbeing
                  cause prioritization framework},
	url = {https://www.openphilanthropy.org/research/technical-updates-to-our-global-health-and-wellbeing-cause-prioritization-framework/},
	urldate = {2023-06-07}
}

@article{Feldman1995AdjustingUtilityFor,
	database = {Tlön},
	author = {Feldman, Fred},
	langid = {english},
	title = {Adjusting utility for justice: a consequentialist
                  reply to the objection from justice},
	volume = {55},
	number = {3},
	pages = {56–7},
	doi = {10.2307/2108439},
	url = {https://www.jstor.org/stable/2108439?origin=crossref},
	date = {1995-09},
	issn = {00318205},
	journaltitle = {Philosophy and Phenomenological Research},
	shortjournal = {Philosophy and Phenomenological Research},
	shorttitle = {Adjusting Utility for Justice},
	timestamp = {2023-07-27 09:23:47 (GMT)},
	urldate = {2023-07-27}
}

@incollection{Feldman2001Hedonism,
	database = {Tlön},
	location = {New York},
	langid = {english},
	title = {Hedonism},
	isbn = {0-415-93672-1},
	pages = {662–669},
	booktitle = {Encyclopedia of ethics},
	publisher = {Routledge},
	author = {Feldman, Fred},
	editor = {Becker, Lawrence C. and Becker, Charlotte B.},
	date = 2001,
	note = {Series Title: Encyclopedia of ethics}
}

@online{Fenwick2023LargoplacismoLlamamientoProteger,
	database = {Tlön},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Fenwick2023LongtermismCallTo},
	date = {2023},
	title = {Largoplacismo: un llamamiento a proteger a las
                  generaciones futuras},
	author = {Fenwick, Cody},
	timestamp = {2023-06-27 09:47:20 (GMT)}
}

@online{Ferriss2022WillMacAskillEffective,
	database = {Tlön},
	title = {Will {MacAskill} of effective altruism fame — The
                  value of longtermism, {AI}, and how to save the world},
	abstract = {Will {MacAskill} of Effective Altruism Fame — The Value of Longtermism, Tools for Beating Stress and Overwhelm, {AI} Scenarios, High-Impact Books, and How to Sav...},
	langid = {english},
	url = {https://www.youtube.com/watch?v=I9C41fx64dw},
	journaltitle = {The Tim Ferriss Show},
	author = {Ferriss, Timothy},
	urldate = {2022-08-04},
	date = {2022-08-02}
}

@article{Finn2009DefensiveToolUse,
	author = {Finn, Julian K. and Tregenza, Tom and Norman, Mark D.},
	title = {Defensive tool use in a coconut-carrying octopus},
	volume = {19},
	number = {23},
	pages = {R1069—R1070},
	doi = {10.1016/j.cub.2009.10.052},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0960982209019149},
	database = {Tlön},
	date = {2009-12},
	issn = {09609822},
	journaltitle = {Current Biology},
	langid = {english},
	shortjournal = {Current Biology},
	timestamp = {2023-07-04 20:08:55 (GMT)},
	urldate = {2023-07-04}
}

@online{Finnveden2022AGILockin,
	database = {Tlön},
	title = {{AGI} and lock-in},
	abstract = {The long-term future of intelligent life is currently unpredictable and undetermined. In the linked document, we argue that the invention of artificial general intelligence ({AGI}) could change this by making extreme types of lock-in technologically feasible. In particular, we argue that {AGI} would make it technologically feasible to (i) perfectly preserve nuanced specifications of a wide variety of values or goals far into the future, and (ii) develop {AGI}-based institutions that would (with high probability) competently pursue any such values for at least millions, and plausibly trillions, of years.},
	url = {https://forum.effectivealtruism.org/posts/KqCybin8rtfP3qztq/agi-and-lock-in},
	journaltitle = {Effective Altruism Forum},
	author = {Finnveden, Lukas and Riedel, Jess and Shulman, Carl},
	urldate = {2022-10-29},
	date = {2022-10-29},
	langid = {english},
	file = {~/Google Drive/library-pdf/Finnveden2022AGILockin.pdf;~/Google
                  Drive/library-pdf/Finnveden2022AGILockin.pdf;~/Google Drive/library-html/agi-and-lock-in.html}
}

@report{Fischer2021AIPolicyLevers,
	database = {Tlön},
	title = {{AI} policy levers: A review of the U.S. government’s
                  tools to shape {AI} research, development, and
                  deployment},
	abstract = {The U.S. government ({USG}) has taken increasing interest in the national security implications of artificial intelligence ({AI}). In this report, we ask: Given its national security concerns, how migh...},
	langid = {english},
	url = {https://www.governance.ai/research-paper/ai-policy-levers-a-review-of-the-u-s-governments-tools-to-shape-ai-research-development-and-deployment},
	institution = {Future of Humanity Institute, University of Oxford},
	author = {Fischer, Sophie-Charlotte and Leung, Jade and
                  Anderljung, Markus and O'Keefe, Cullen and Torges,
                  Stefan and Khan, Saif M. and Garfinkel, Ben and Dafoe,
                  Allan},
	date = {2021-03-16},
	file = {~/Google Drive/library-pdf/Fischer2021AIPolicyLevers.pdf}
}

@online{Fish2022AnnouncingAlveaEA,
	database = {Tlön},
	title = {Announcing Alvea—An {EA} {COVID} vaccine project},
	abstract = {We’ve had effective {COVID} vaccines for more than a year, but there are still countries where less than 10\% of people have received a dose. Omicron has been spreading for almost three months, but pharma companies have only just started testing variant-specific shots. {mRNA} vaccines are highly effective, but they’re hard to manufacture and nearly impossible to distribute in parts of the developing world.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/67awq5ozeYSjsYchk/announcing-alvea-an-ea-covid-vaccine-project},
	journaltitle = {Effective Altruism Forum},
	author = {Fish, Kyle},
	urldate = {2022-02-22},
	date = {2022-02-16},
	file = {~/Google Drive/library-pdf/Fish2022AnnouncingAlveaEA.pdf;~/Google Drive/library-html/announcing-alvea-an-ea-covid-vaccine-project.html}
}

@online{Fitz2022Momentum2022Updates,
	database = {Tlön},
	title = {Momentum 2022 updates (we're hiring)},
	abstract = {Momentum (formerly Sparrow) is a venture-backed, {EA} startup that aims to increase effective giving. We build donation pages that emphasize creative, recurring donations tied to moments in your life (e.g. offset your carbon footprint when you buy gas, or give to {AI} alignment every time you scroll Facebook) and we use behavioral science to nudge new donors to support {EA} charities.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/W9rkBeXqKsoMaMLEQ/momentum-2022-updates-we-re-hiring},
	journaltitle = {Effective Altruism Forum},
	author = {Fitz, Nick and Kagan, Ari},
	urldate = {2022-04-01},
	date = {2022-01-11},
	file = {~/Google Drive/library-pdf/Fitz2022Momentum2022Updates.pdf;~/Google Drive/library-html/momentum-2022-updates-we-re-hiring.html}
}

@article{Foot1967ProblemAbortionDoctrine,
	database = {Tlön},
	title = {The problem of abortion and the doctrine of double
                  effect},
	abstract = {This article explores the doctrine of double effect (DDE), which is a principle used in moral philosophy to justify allowing a bad outcome to occur as a foreseen but not intentionally pursued consequence of an action that is intended to produce a good outcome. The author examines the use of DDE in arguments about abortion and other moral dilemmas, and argues that the distinction between direct and indirect intention is less important than the distinction between avoiding harm and providing aid. The author concludes that DDE is not a satisfactory basis for moral decision-making and that the duty not to inflict harm is generally more compelling than the duty to provide aid. – AI-generated abstract.},
	langid = {english},
	volume = 5,
	pages = {5–15},
	journaltitle = {Oxford review},
	author = {Foot, Philippa},
	date = 1967,
	file = {~/Google Drive/library-pdf/Foot1967ProblemAbortionDoctrine.pdf}
}

@report{Formanski2022PlantbasedMeatSeafood,
	database = {Tlön},
	title = {Plant-based meat, seafood, eggs, and dairy},
	langid = {english},
	url = {https://gfi.org/plant-based-meat-eggs-and-dairy-state-of-the-industry-report-pdf},
	institution = {Good Food Institute},
	author = {Formanski, Karen},
	date = 2022,
	number = {2021 State of the Industry Report},
	file = {~/Google Drive/library-pdf/Formanski2022PlantbasedMeatSeafood.pdf}
}

@online{FortifyHealth2021OrganisationalBrochure,
	database = {Tlön},
	title = {Organisational brochure},
	langid = {english},
	url = {https://www.fortifyhealth.global/uploads/1/0/9/9/109970865/brochure_-_final_web_version.pdf},
	journaltitle = {Fortify Health},
	author = {{Fortify Health}},
	date = 2021
}

@online{FoundersPledge2018CashTransfersReport,
	database = {Tlön},
	title = {Cash transfers report summary},
	langid = {english},
	url = {https://founderspledge.com/stories/alleviate-poverty-report-summary},
	journaltitle = {Founders Pledge},
	author = {{Founders Pledge}},
	date = {2018-12-01}
}

@online{FoundersPledge2018DewormingChildrenReport,
	database = {Tlön},
	title = {Deworming children report summary and giving
                  recommendations},
	langid = {english},
	url = {https://founderspledge.com/stories/deworming-children-report-summary},
	journaltitle = {Founders Pledge},
	author = {{Founders Pledge}},
	date = {2018-12-01}
}

@online{Fox2021AuditFindsMajor,
	database = {Tlön},
	title = {Audit finds major gaps in {US} bio weapons detection
                  system},
	abstract = {{WASHINGTON} ({AP}) — A U.S. program created after the 2003 anthrax attacks to help detect biological weapons provided protection in less than half the states and couldn't detect many of the known threats, according to a report released Thursday.},
	url = {https://apnews.com/article/bioterrorism-terrorism-united-states-biological-weapons-19da38a1af8b1f3b1c41573d6cbdff7a},
	journaltitle = {{AP} News},
	author = {Fox, Ben},
	urldate = {2022-01-06},
	date = {2021-03-04},
	langid = {english},
	note = {Section: United States},
	file = {~/Google Drive/library-pdf/Fox2021AuditFindsMajor.pdf;~/Google Drive/library-html/bioterrorism-terrorism-united-states-biological-weapons-19da38a1af8b1f3b1c41573d6cbdff7a.html}
}

@online{Francini2020CompleteArchiveFelicifia,
	database = {Tlön},
	title = {Complete archive of the Felicifia forum},
	abstract = {Prior to the existence of a unified effective altruism movement, a handful of proto-{EA} communities and organizations were already aiming towards similar ends. These groups included the web forum {LessWrong} and the charity evaluator {GiveWell}. One lesser-known community that played an important role in the history of the {EA} movement is the Felicifia utilitarian forum.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/Qx9WMJfc7uCB6jsCJ/complete-archive-of-the-felicifia-forum},
	journaltitle = {Effective Altruism Forum},
	author = {Francini, Louis},
	date = {2020-05-30},
	file = {~/Google Drive/library-pdf/Francini2020CompleteArchiveFelicifia.pdf}
}

@article{Frederick1998ScopeInSensitivity,
	database = {Tlön},
	author = {Frederick, Shane and Fischhoff, Baruch},
	title = {Scope (in)sensitivity in elicited valuations},
	langid = {english},
	volume = 3,
	number = 2,
	pages = {109–123},
	doi = {10.1080/135753098348239},
	url = {http://www.catchword.com/cgi-bin/cgi?body=linker&ini=xref&reqdoi=10.1080/135753098348239},
	date = {1998-08-01},
	issn = {14664534, 13575309},
	journaltitle = {Risk Decision and Policy},
	timestamp = {2023-06-24 11:17:05 (GMT)},
	urldate = {2023-06-24}
}

@online{Freeman2022AnnouncingFutureForum,
	database = {Tlön},
	title = {Announcing the Future Forum - Apply Now - {EA} Forum},
	abstract = {We are excited to announce Future Forum, an experimental 4-day conference taking place Aug 4-7 in San Francisco. We want to gather promising people from across communities interested in the long-term future of humanity, introduce them to each other, reflect on transformative technologies and their far-reaching consequences (upside and downside risks), and then we hope to generate new promising and thoughtful projects across this cross-community ecosystem. We expect around 250 attendees.},
	url = {https://forum.effectivealtruism.org/posts/szeE3je8MD4sZcevL/announcing-the-future-forum-apply-now},
	journaltitle = {Effective Altruism Forum},
	author = {Freeman, Isaak and Jurkovich, Nikola and Chiu, Anna},
	urldate = {2022-07-07},
	date = {2022-07-06},
	langid = {english},
	file = {~/Google Drive/library-pdf/Freeman2022AnnouncingFutureForum.pdf;~/Google Drive/library-html/announcing-the-future-forum-apply-now.html}
}

@thesis{Frick2014MakingPeopleHappy,
	database = {Tlön},
	title = {'Making people happy, not making happy people': A
                  defense of the asymmetry intuition in population
                  ethics},
	langid = {english},
	institution = {Harvard University},
	type = {{PhD} thesis},
	author = {Frick, Johann David Anand},
	date = 2014,
	note = {Genre: {PhD} thesis},
	file = {~/Google Drive/library-pdf/Frick2016MakingPeopleHappy.pdf}
}

@book{Friedrich1965TotalitarianDictatorshipAutocracy,
	database = {Tlön},
	location = {Cambridge, Massachusetts},
	langid = {english},
	title = {Totalitarian dictatorship and autocracy},
	publisher = {Harvard University Press},
	author = {Friedrich, Carl J. and Brzezinski, Zbigniew K.},
	date = 1965,
	file = {~/Google Drive/library-pdf/Friedrich1965TotalitarianDictatorshipAutocracy.pdf}
}

@report{Fritz2021HighImpactProfessionals,
	database = {Tlön},
	title = {High Impact Professionals},
	langid = {english},
	url = {https://drive.google.com/file/d/1Ai1GBQhq92lUJuprbdfKMnwRhtvg7xDX/view},
	author = {Fritz, Devon and Speziali, Federico},
	date = 2021
}

@online{FutureofHumanityInstitute2021ResearchAreas,
	database = {Tlön},
	title = {Research areas},
	abstract = {Biotechnology provides both benefits and risks. Plant biotechnology has improved agricultural yields, and along with microbial biotechnology, it has developed antibiotics and genetically derived drugs. The tools of biotechnology include DNA sequencing, recombinant DNA technologies, DNA synthesis, and genome editing. These technologies hold promise for new treatments for diseases, such as CRISPR/Cas9 for repairing genetic defects. However, the rapid pace of progress and falling costs carry the risk that biotechnology work could move out of well-equipped laboratories into private homes, and bioweapons could be developed. Concerns also arise with the potential for misuse of biotechnology information such as DNA databases, or with synthetic genetic developments such as attempts to bring back extinct species or engineer new ones. – AI-generated abstract.},
	url = {https://www.fhi.ox.ac.uk/research/research-areas/},
	journaltitle = {Future of Humanity Institute},
	author = {{Future of Humanity Institute}},
	urldate = {2021-08-12},
	date = 2021,
	langid = {english},
	file = {~/Google Drive/library-html/research-areas.html}
}

@online{FutureofLifeInstitute2015ElonMuskDonates,
	database = {Tlön},
	title = {Elon Musk donates \$10M to our research program},
	abstract = {We are delighted to report that Elon Musk has decided to donate \$10M to {FLI} to run a global research […].},
	url = {https://futureoflife.org/2015/01/22/elon-musk-donates-10m-to-our-research-program/},
	journaltitle = {Future of Life Institute},
	author = {{Future of Life Institute}},
	urldate = {2022-05-29},
	date = {2015-01-22},
	langid = {english},
	file = {~/Google Drive/library-pdf/FutureofLifeInstitute2015ElonMuskDonates.pdf;~/Google Drive/library-html/elon-musk-donates-10m-to-our-research-program.html}
}

@online{FutureofLifeInstitute2018BenefitsRisksBiotechnology,
	database = {Tlön},
	title = {Benefits \& risks of biotechnology},
	abstract = {The article delves into the complexities of instilling values into artificial intelligence (AI), specifically addressing the challenge of ensuring that AI systems align with human values and goals. It explores various approaches to value loading, including explicit representation, evolutionary selection, reinforcement learning, associative value accretion, and motivational scaffolding. It also discusses the perspectives of Ernest Davis, who argues that value loading is less problematic than Nick Bostrom suggests and proposes a method for defining minimal ethical standards for AI. Additionally, it offers potential research directions related to formalizing human values, developing alternative value-loading approaches, and assessing the feasibility of specific methods. – AI-generated abstract.},
	url = {https://futureoflife.org/2018/11/20/benefits-risks-of-biotechnology/},
	journaltitle = {Future of Life Institute},
	author = {{Future of Life Institute}},
	urldate = {2021-10-20},
	date = {2018-11-20},
	langid = {english},
	file = {~/Google Drive/library-pdf/2018BenefitsRisksBiotechnology.pdf;~/Google Drive/library-html/benefits-risks-of-biotechnology.html}
}

@online{Gaensbauer2016EffectiveAltruismEnvironmentalism,
	database = {Tlön},
	title = {Effective altruism, environmentalism, and climate
                  change: an introduction},
	abstract = {Multiple focus areas of effective altruism (EA), such as poverty alleviation, global health, and animal advocacy, interact with climate change mitigation. The ways in which the EA community could engage with climate change are by: 1) assessing whether climate change is the most pressing concern and should therefore take priority over other focus areas; 2) developing strategies to mitigate climate change while considering its interactions with other existing areas of concern; 3) avoiding potential conflicts between the EA community and the environmental movement and seeking opportunities for collaboration instead. – AI-generated abstract.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/dmrLcaYGk6yhJa2mZ/effective-altruism-environmentalism-and-climate-change-an},
	journaltitle = {Effective Altruism Forum},
	author = {Gaensbauer, Evan},
	date = {2016-03-10},
	file = {~/Google Drive/library-pdf/Gaensbauer2016EffectiveAltruismEnvironmentalism.pdf}
}

@online{Galef2012BeCommunicationsConsequentialist,
	database = {Tlön},
	title = {Be a communications consequentialist},
	abstract = {You just hit post.  You put a lot of thought into your message, you laid it out carefully, and look forward to people’s reactions.  You start getting emails telling you that people have comme….},
	langid = {english},
	url = {https://jessegalef.com/2012/06/11/be-a-communications-consequentialist/},
	journaltitle = {Measure of doubt},
	author = {Galef, Jesse},
	date = {2012-06-11},
	file = {~/Google Drive/library-pdf/Galef2012BeCommunicationsConsequentialist.pdf}
}

@online{Galef2015MyStory,
	database = {Tlön},
	title = {My Story},
	abstract = {After graduating with a B.A. in statistics from Columbia University in 2005, I spent several years doing research with social science professors at Columbia, Harvard and {MIT}, including a year writi….},
	url = {https://juliagalef.com/about-me/},
	journaltitle = {Julia Galef's website},
	author = {Galef, Julia},
	urldate = {2022-01-11},
	date = 2015,
	langid = {english},
	file = {~/Google Drive/library-pdf/Galef2015MyStory.pdf;~/Google Drive/library-html/about-me.html}
}

@online{Galef2019DefendingBigBusiness,
	database = {Tlön},
	title = {Defending big business against its critics (Tyler
                  Cowen)},
	url = {https://rationallyspeakingpodcast.org/232-defending-big-business-against-its-critics-tyler-cowen/},
	shorttitle = 232,
	journaltitle = {Rationally Speaking},
	author = {Galef, Julia},
	urldate = {2022-01-11},
	date = {2019-04-29},
	langid = {english}
}

@online{Galef2023PorQueCrees,
	database = {Tlön},
	date = {2023},
	author = {Galef, Julia},
	langid = {spanish},
	title = {Por qué crees que tienes razón, aun cuando te
                  equivocas},
	translator = {Domingo Moreno, Tania},
	translation = {Galef2023WhyYouThink}
}

@online{Galton2019GeneticEnhancementCause,
	database = {Tlön},
	title = {Genetic enhancement as a cause area},
	abstract = {Originally posted on the {EA} subreddit.
First, I will present a rough sketch for why genetic enhancement could be a plausible cause X. Then I will list some specific proposals for genetic interventions. I will conclude by responding to objections. If there is interest, I may write more posts on this topic.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/T8eKL6xdfL4yA2kvg/genetic-enhancement-as-a-cause-area},
	journaltitle = {Effective Altruism Forum},
	author = {{Galton}},
	date = {2019-12-25},
	file = {~/Google Drive/library-pdf/Galton2019GeneticEnhancementCause.pdf}
}

@article{Garfinkel2017RecentDevelopmentsCryptography,
	database = {Tlön},
	title = {Recent developments in cryptography and potential
                  long-term consequences},
	langid = {english},
	pages = {1–64},
	author = {Garfinkel, Ben},
	date = 2017
}

@online{Garfinkel2019HowSureAre,
	database = {Tlön},
	title = {How sure are we about this {AI} stuff?},
	abstract = {It is increasingly clear that artificial intelligence is poised to have a huge impact on the world, potentially of comparable magnitude to the agricultural or industrial revolutions. But what does that actually mean for us today? Should it influence our behavior? In this talk from {EA} Global 2018: London, Ben Garfinkel makes the case for measured skepticism.
Today, work on risks from artificial intelligence constitutes a noteworthy but still fairly small portion of the {EA} portfolio.
.
Only a small portion of donations made by individuals in the community are targeted at risks from {AI}. Only about 5\% of the grants given out by the Open Philanthropy Project, the leading grant-making organization in the space, target risks from {AI}. And in surveys of community members, most do not list {AI} as the area that they think should be most prioritized.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/9sBAW3qKppnoG3QPq/ben-garfinkel-how-sure-are-we-about-this-ai-stuff},
	journaltitle = {Effective Altruism Forum},
	author = {Garfinkel, Ben},
	date = {2019-02-09},
	file = {~/Google Drive/library-pdf/Garfinkel2019HowSureAre.pdf}
}

@online{Garfinkel2020DoesEconomicHistory,
	database = {Tlön},
	title = {Does economic history point toward a singularity?},
	abstract = {Over the next several centuries, is the economic growth rate likely to remain steady, radically increase, or decline back toward zero? This question has some bearing on almost every long-run challenge facing the world, from climate change to great power competition to risks from {AI}.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/CWFn9qAKsRibpCGq8/does-economic-history-point-toward-a-singularity},
	journaltitle = {Effective Altruism Forum},
	author = {Garfinkel, Ben},
	date = {2020-09-02},
	file = {~/Google Drive/library-pdf/Garfinkel2020DoesEconomicHistory.pdf}
}

@online{Garfinkel2021DemocracyFad,
	database = {Tlön},
	title = {Is democracy a fad?},
	abstract = {This post explains why I would not be surprised if democracy disappears in the next several centuries. Long-run progress in {AI} is one special cause for concern.  There’s a strange new trend that’….},
	langid = {english},
	url = {https://benmgarfinkel.wordpress.com/2021/02/26/is-democracy-a-fad/},
	journaltitle = {The Best That Can Happen},
	author = {Garfinkel, Ben},
	date = {2021-02-26},
	file = {~/Google Drive/library-pdf/Garfinkel2021DemocracyFad.pdf}
}

@online{Garfinkel2022WeShouldExpect,
	database = {Tlön},
	title = {We should expect to worry more about speculative
                  risks},
	abstract = {For a number of risks, when you first hear and think a bit about them, it’s reasonable to have the reaction “Oh, hm, maybe that could be a huge threat to human survival” and initially assign something on the order of a 10\% credence to the hypothesis that it will by default lead to existentially bad outcomes. In each case, if we can gain much greater clarity about the risk, then we should think there’s about a 90\% chance this clarity will make us less worried about it. We’re likely to remain decently worried about hard-to-analyze risks (because we can’t get greater clarity about them) while becoming less worried about easy-to-analyze risks.},
	url = {https://forum.effectivealtruism.org/posts/M68oj7fwXoPFJisap/we-should-expect-to-worry-more-about-speculative-risks},
	journaltitle = {Effective Altruism Forum},
	author = {Garfinkel, Ben},
	urldate = {2022-05-30},
	date = {2022-05-29},
	langid = {english},
	file = {~/Google Drive/library-pdf/Garfinkel2022WeShouldExpect.pdf;~/Google Drive/library-html/we-should-expect-to-worry-more-about-speculative-risks.html}
}

@Article{Garrabrant2020LogicalInduction,
	database = {Tlön},
	file = {~/Google Drive/library-pdf/Garrabrant2020LogicalInduction.pdf},
	abstract = {We present a computable algorithm that assigns probabilities to every logical statement in a given formal language, and reﬁnes those probabilities over time. For instance, if the language is Peano arithmetic, it assigns probabilities to all arithmetical statements, including claims about the twin prime conjecture, the outputs of long-running computations, and its own probabilities. We show that our algorithm, an instance of what we call a logical inductor, satisﬁes a number of intuitive desiderata, including: (1) it learns to predict patterns of truth and falsehood in logical statements, often long before having the resources to evaluate the statements, so long as the patterns can be written down in polynomial time; (2) it learns to use appropriate statistical summaries to predict sequences of statements whose truth values appear pseudorandom; and (3) it learns to have accurate beliefs about its own current beliefs, in a manner that avoids the standard paradoxes of self-reference. For example, if a given computer program only ever produces outputs in a certain range, a logical inductor learns this fact in a timely manner; and if late digits in the decimal expansion of π are diﬃcult to predict, then a logical inductor learns to assign ≈ 10\% probability to “the nth digit of π is a 7” for large n. Logical inductors also learn to trust their future beliefs more than their current beliefs, and their beliefs are coherent in the limit (whenever φ → ψ, P∞ (φ) ≤ P∞ (ψ), and so on); and logical inductors strictly dominate the universal semimeasure in the limit. These properties and many others all follow from a single logical induction criterion, which is motivated by a series of stock trading analogies. Roughly speaking, each logical sentence φ is associated with a stock that is worth \$1 per share if φ is true and nothing otherwise, and we interpret the belief-state of a logically uncertain reasoner as a set of market prices, where Pn (φ) = 50\% means that on day n, shares of φ may be bought or sold from the reasoner for 50¢. The logical induction criterion says (very roughly) that there should not be any polynomial-time computable trading strategy with ﬁnite risk tol- erance that earns unbounded proﬁts in that market over time. This criterion bears strong resemblance to the “no Dutch book” criteria that support both expected utility theory (von Neumann and Morgenstern 1944) and Bayesian probability theory (Ramsey 1931; de Finetti 1937).},
	number = {arXiv:1609.03543 [cs.AI]},
	langid = {english},
	author = {Garrabrant, Scott and Benson-Tilsen, Tsvi and Critch, Andrew
                  and Soares, Nate and Taylor, Jessica},
	date = {2020-12-07},
	eprint = {1609.03543 [cs, math]},
	eprinttype = {arxiv},
	keywords = {Computer Science - Artificial Intelligence, Computer Science -
                  Logic in Computer Science, Mathematics - Logic, Mathematics -
                  Probability},
	publisher = {{arXiv}},
	timestamp = {2023-11-15 18:15:34 (GMT)},
	title = {Logical induction},
	url = {http://arxiv.org/abs/1609.03543},
	urldate = {2023-11-15}
}

@online{Gates2022RisksAdvancedAI,
	database = {Tlön},
	title = {Risks from advanced {AI} (June 2022)},
	abstract = {I'm a postdoctoral researcher at Stanford {HAI} and {CISAC}. I recently gave a {HAI} Seminar Zoom talk, in which I lay out some of the basic arguments for existential risk from {AI} during the first 23m of the talk, after which I describe my research interviewing {AI} researchers and answer Q\&A.I recommend the first 23m as a resource to send to people who are new to these arguments (the talk was aimed at computer science researchers, but is also accessible to the public). This is a pretty detailed, current as of June 2022, public-facing overview that's updated with April-May 2022 papers, and includes readings, funding, additional resources at the bottom of the page. Also, if you want to give your own version of the talk, please be my guest! No attribution necessary for the first 23m (I would like attribution for the "interviews" part of the talk).},
	url = {https://forum.effectivealtruism.org/posts/q49obZkQujkYmnFWY/vael-gates-risks-from-advanced-ai-june-2022},
	shorttitle = {Vael Gates},
	journaltitle = {Effective Altruism Forum},
	author = {Gates, Vael},
	urldate = {2022-06-27},
	date = {2022-06-14},
	langid = {english},
	file = {~/Google Drive/library-html/vael-gates-risks-from-advanced-ai-june-2022.html;~/Google Drive/library-pdf/VaelGates2022RisksAdvancedAI.pdf}
}

@online{Gentzel2016TransgenicMosquitoesUpdate,
	database = {Tlön},
	title = {Transgenic mosquitoes, update Effective Altruism
                  Policy Analytics},
	abstract = {In March 2016, the U.S. Food and Drug Administration posted a draft of its environmental assessment on the potential impact of testing {OX}513A mosquitoes in the Florida Keys. {OX}513A mosquitoes are genetically modified to pass genes to their offspring which result in offspring death. They are used to suppress Aedes aegypti mosquito populations, which are known for spreading dengue fever, chikungunya, Zika fever and yellow fever viruses, as well as other diseases.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/FoCpbrb9QMWM4yboS/transgenic-mosquitoes-update-effective-altruism-policy},
	journaltitle = {Effective Altruism Forum},
	author = {Gentzel, Matthew},
	date = {2016-08-10},
	file = {~/Google Drive/library-pdf/Gentzel2016TransgenicMosquitoesUpdate.pdf}
}

@report{Gerber2013TacklingClimateChange,
	database = {Tlön},
	title = {Tackling climate change through livestock: A global
                  assessment of emissions and mitigation opportunities},
	langid = {english},
	url = {https://www.fao.org/3/i3437e/i3437e.pdf},
	shorttitle = {Tackling climate change through livestock},
	institution = {Food and Agriculture Organization of the United
                  Nations},
	author = {Gerber, Pierre J. and Steinfeld, H. and Henderson, B.
                  and Mottet, A. and Opio, C. and Dijkman, J. and
                  Falcucci, A. and Tempio, G.},
	date = 2013,
	note = {{ISBN}: 978-92-5-107921-8}
}

@online{Gertler2019EALeadersForum,
	database = {Tlön},
	title = {{EA} Leaders Forum: Survey on {EA} priorities (data
                  and analysis)},
	abstract = {Each year, the {EA} Leaders Forum, organized by {CEA}, brings together executives, researchers, and other experienced staffers from a variety of {EA}-aligned organizations. At the event, they share ideas and discuss the present state (and possible futures) of effective altruism.
This year (during a date range centered around {\textasciitilde}1 July), invitees were asked to complete a “Priorities for Effective Altruism” survey, compiled by {CEA} and 80,000 Hours, which covered the following broad topics:.
The resources and talents most needed by the {communityHow} {EA}’s resources should be allocated between different cause {areasBottlenecks} on the community’s progress and {impactProblems} the community is facing, and mistakes we could be making now.
This post is a summary of the survey’s findings (N = 33; 56 people received the survey).
Here’s a list of organizations respondents worked for, with the number of respondents from each organization in parentheses. Respondents included both leadership and other staff (an organization appearing on this list doesn’t mean that the org’s leader responded).
80,000 Hours (3)Animal Charity Evaluators (1)Center for Applied Rationality (1)Centre for Effective Altruism (3)Centre for the Study of Existential Risk (1){DeepMind} (1)Effective Altruism Foundation (2)Effective Giving (1)Future of Humanity Institute (4)Global Priorities Institute (2)Good Food Institute (1)Machine Intelligence Research Institute (1)Open Philanthropy Project (6).
Three respondents work at organizations small enough that naming the organizations would be likely to de-anonymize the respondents. Three respondents don’t work at an {EA}-aligned organization, but are large donors and/or advisors to one or more such organizations.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/TpoeJ9A2G5Sipxfit/ea-leaders-forum-survey-on-ea-priorities-data-and-analysis},
	journaltitle = {Effective Altruism Forum},
	author = {Gertler, Aaron},
	urldate = {2022-01-03},
	date = {2019-11-11},
	file = {~/Google Drive/library-html/EA Leaders Forum\: Survey
                  on EA priorities (data and analysis) - EA
                  Forum:ea-leaders-forum-survey-on-ea-priorities-data-and-analysis.html;~/Google
                  Drive/library-pdf/Gertler2019EALeadersForum.pdf}
}

@online{Gertler2019GlobalPrioritiesCopenhagen,
	database = {Tlön},
	title = {The global priorities of the Copenhagen Consensus},
	abstract = {The Copenhagen Consensus is one of the few organizations outside the {EA} community which conducts cause prioritization research on a global scale.
Nearly everything on their "Post-2015 Consensus" list, which covers every cause they've looked at, fits into "global development"; they don't examine animal causes or global catastrophic risks aside from climate change (though they do discuss population ethics in the case of demographic interventions).},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/YReJJ8MZdASANojrT/the-global-priorities-of-the-copenhagen-consensus},
	journaltitle = {Effective Altruism Forum},
	author = {Gertler, Aaron},
	date = {2019-01-07},
	file = {~/Google Drive/library-pdf/Gertler2019GlobalPrioritiesCopenhagen.pdf}
}

@online{Gertler2020SignForumEmail,
	database = {Tlön},
	title = {Sign up for the Forum's email digest},
	abstract = {I send out a weekly email digest for the {EA} Forum. The emails feature recent posts that have a lot of karma/discussion, as well as question posts that could use more answers. Sign up here!},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/zRoXctENAsLchwryn/sign-up-for-the-forum-s-email-digest},
	journaltitle = {Effective Altruism Forum},
	author = {Gertler, ron},
	urldate = {2022-04-28},
	date = {2020-09-14},
	file = {~/Google Drive/library-pdf/Gertler2020SignForumEmail.pdf;~/Google Drive/library-html/sign-up-for-the-forum-s-email-digest.html}
}

@online{Gertler2021YouShouldWrite,
	database = {Tlön},
	title = {You should write about your job},
	abstract = {If you have a job, you are one of the world's foremost experts on your job — at least within the {EA} community, which is not large.
Jobs are a useful thing to know about. We spend more time on them than anything else, and most of our impact comes from jobs + their outcomes (e.g. salary).},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/nf72oiJddwDhoJ4QH/you-should-write-about-your-job},
	journaltitle = {Effective Altruism Forum},
	author = {Gertler, Aaron},
	urldate = {2021-07-29},
	date = {2021-07-19},
	file = {~/Google Drive/library-pdf/Gertler2021YouShouldWrite.pdf;~/Google Drive/library-html/you-should-write-about-your-job.html}
}

@online{GiveWell2012TopCharities,
	database = {Tlön},
	title = {Top charities},
	langid = {english},
	url = {https://web.archive.org/web/20121215080910/https://www.givewell.org/charities/top-charities},
	journaltitle = {{GiveWell}},
	author = {{GiveWell}},
	date = {2012-12}
}

@online{GiveWell2013TopCharities,
	database = {Tlön},
	title = {Top charities},
	langid = {english},
	url = {https://web.archive.org/web/20131230084233/https://www.givewell.org/charities/top-charities},
	journaltitle = {{GiveWell}},
	author = {{GiveWell}},
	date = {2013-12}
}

@online{GiveWell2014TopCharities,
	database = {Tlön},
	title = {Top charities},
	langid = {english},
	url = {https://web.archive.org/web/20141230101142/https://www.givewell.org/charities/top-charities},
	journaltitle = {{GiveWell}},
	author = {{GiveWell}},
	date = {2014-12}
}

@online{GiveWell2015TopCharities,
	database = {Tlön},
	title = {Top charities},
	langid = {english},
	url = {https://web.archive.org/web/20151230225011/https://www.givewell.org/charities/top-charities},
	journaltitle = {{GiveWell}},
	author = {{GiveWell}},
	date = {2015-12}
}

@online{GiveWell2016LivingGoodsMid2016,
	database = {Tlön},
	title = {Living Goods – Mid-2016 update},
	abstract = {Living Goods is one of {GiveWell}'s standout charities. Read our mid-2016 update on Living Goods' network of Community Health Promoters in Uganda and Kenya.},
	url = {https://www.givewell.org/charities/living-goods/all-content/2016-update},
	journaltitle = {{GiveWell}},
	author = {{GiveWell}},
	date = {2016-09},
	langid = {english},
	file = {~/Google Drive/library-pdf/GiveWell2016LivingGoodsMid2016.pdf;~/Google Drive/library-html/2016-update.html}
}

@online{GiveWell2016TopCharities,
	database = {Tlön},
	title = {Top charities},
	langid = {english},
	url = {https://web.archive.org/web/20161213020857/https://www.givewell.org/charities/top-charities},
	journaltitle = {{GiveWell}},
	author = {{GiveWell}},
	date = {2016-12}
}

@online{GiveWell2017TopCharities,
	database = {Tlön},
	title = {Top charities},
	langid = {english},
	url = {https://web.archive.org/web/20171230192517/https://www.givewell.org/charities/top-charities},
	journaltitle = {{GiveWell}},
	author = {{GiveWell}},
	date = {2017-12},
	note = {{ISBN}: 2017123019251}
}

@online{GiveWell2018GiveWellAnalysisGiveDirectly,
	database = {Tlön},
	title = {{GiveWell}'s analysis of {GiveDirectly} financial
                  summary through february 2018},
	abstract = {A breakdown of spending in two time periods spanning from August 2016 to July 2017 and from August 2017 to February 2018 is given in tabular form. An overwhelming majority of the total spending in both periods comprised transfers to households. Other types of spending include payments for staff salaries, travel, professional fees, software, and other expenses. In the first time period, 80.5\% of the total spending was directed to households, while in the second time period, the proportion increased to 86.8\%. Almost all of the remainder was for staff salaries, travel, and professional fees, with software and other types of spending accounting for a very small fraction of the total. – AI-generated abstract.},
	langid = {english},
	url = {https://docs.google.com/spreadsheets/d/1L03SQuAeRRfjyuxy20QIJByOx6PEzyJ-x4edz2fSiQ4/edit#gid=537899494},
	journaltitle = {{GiveWell}},
	author = {{GiveWell}},
	date = 2018,
	file = {~/Google Drive/library-pdf/GiveWell2018GiveWellAnalysisGiveDirectly.pdf}
}

@online{GiveWell2018TopCharities,
	database = {Tlön},
	title = {Top charities},
	langid = {english},
	url = {https://web.archive.org/web/20181231082426/https://www.givewell.org/charities/top-charities},
	journaltitle = {{GiveWell}},
	author = {{GiveWell}},
	date = {2018-12},
	note = {{ISBN}: 2018123108242}
}

@online{GiveWell20192019GiveWellCosteffectiveness,
	database = {Tlön},
	title = {2019 {GiveWell} cost-effectiveness analysis — Version
                  6 (public)},
	abstract = {This study presents a spreadsheet tool to compare the cost-effectiveness of various charities working in the fields of malaria prevention, deworming, and vitamin A supplementation. Users can select different input parameters, such as moral weights and country programs, to compare the charities' performance. The tool also allows users to adjust for leverage and funging, as well as include additional adjustments. Results are presented in tabular form, with charities ranked by their cost-effectiveness. The tool is designed to help donors make informed decisions about which charities to support – AI-generated abstract.},
	langid = {english},
	url = {https://docs.google.com/spreadsheets/d/1zLmPuddUmKsy3v55AfG_e1Quk-ngDdNzW-FDx0T-Y94/edit#gid=1034883018},
	journaltitle = {{GiveWell}},
	author = {{GiveWell}},
	date = 2019,
	file = {~/Google Drive/library-pdf/GiveWell20192019GiveWellCosteffectiveness.pdf}
}

@online{GiveWell2019TopCharities,
	database = {Tlön},
	title = {Top charities},
	langid = {english},
	url = {https://web.archive.org/web/20191230235836/https://www.givewell.org/charities/top-charities},
	journaltitle = {{GiveWell}},
	author = {{GiveWell}},
	date = {2019-12}
}

@online{GiveWell20212021GiveWellCosteffectiveness,
	database = {Tlön},
	title = {2021 {GiveWell} cost-effectiveness analysis — Version
                  1},
	abstract = {This workbook contains our cost-effectiveness analysis. Cost-effectiveness is one factor we use to assess which opportunities are the highest-priority to support.},
	langid = {english},
	url = {https://docs.google.com/spreadsheets/d/1-fb74yL-FS1inZ-_mQt0uUwpkHLHrzypwVNlOA2nmsw/edit?usp=sharing},
	journaltitle = {{GiveWell}},
	author = {{GiveWell}},
	date = {2021-05-05}
}

@online{Givewell2018OurStory,
	database = {Tlön},
	file = {~/Google Drive/library-pdf/Givewell2018OurStory.pdf;~/Google Drive/library-html/Givewell2018OurStory.html},
	abstract = {{GiveWell} started with a simple question: Where should I donate? We discovered that researching this question isn't a part-time job.},
	date = {2018-02},
	journaltitle = {GiveWell},
	author = {GiveWell},
	langid = {english},
	timestamp = {2023-09-16 17:07:26 (GMT)},
	title = {Our story},
	url = {https://www.givewell.org/about/story},
	urldate = {2023-09-16}
}

@online{Givewell2022PublicGivewellMetrics,
	database = {Tlön},
	date = {2022-08},
	journaltitle = {GiveWell},
	langid = {english},
	title = {Public GiveWell metrics for the 2021 metrics report},
	url = {https://docs.google.com/spreadsheets/d/1ztnYGpuS3vB7TRGUL1BqoeKY0bNNAVR2TGEr_zJCJh4/edit#gid=0},
	author = {GiveWell},
	timestamp = {2023-09-16 17:10:59 (GMT)}
}

@online{Givewell2023IntroduccionDonacionesBasico,
	database = {Tlön},
	date = {2023},
	title = {Introducción a las donaciones: lo básico},
	author = {{GiveWell}},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Givewell2023Giving101Basics}
}

@online{GivingWhatWeCan2023ComparacionDeOrganizaciones,
	database = {Tlön},
	date = {2023},
	title = {Comparación de organizaciones benéficas: ¿Cuán grande
                  es la diferencia?},
	author = {{Giving What We Can}},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {GivingWhatWeCan2020ComparingCharitiesHow}
}

@online{Gleave2022IntroducingFundAlignment,
	database = {Tlön},
	title = {Introducing the Fund for Alignment Research (We're
                  hiring!)},
	abstract = {The Fund for Alignment Research ({FAR}) is hiring research engineers and communication specialists to work closely with {AI} safety researchers. We believe these roles are high-impact, contributing to some of the most interesting research agendas in safety. We also think they offer an excellent opportunity to build skills and connections via mentorship and working closely with researchers at a variety of labs.},
	url = {https://forum.effectivealtruism.org/posts/gNHjEmLeKM47FDdqM/introducing-the-fund-for-alignment-research-we-re-hiring-1},
	journaltitle = {Effective Altruism Forum},
	author = {Gleave, Adam and Emmons, Scott and Perez, Ethan and
                  Shi, Claudia},
	urldate = {2022-07-06},
	date = {2022-07-06},
	langid = {english},
	file = {~/Google Drive/library-html/introducing-the-fund-for-alignment-research-we-re-hiring-1.html}
}

@online{Gloor2017MultiversewideCooperationNutshell,
	database = {Tlön},
	title = {Multiverse-wide cooperation in a nutshell},
	abstract = {Leaving aside for the moment the whole part about the multiverse, {MSR} is fundamentally about cooperating in a prisoner’s-dilemma-like situation with agents who are very similar to ourselves in the way they reason about decision problems. Douglas Hofstadter coined the term superrationality for the idea that one should cooperate in a prisoner’s dilemma if one expects the other party to follow the same style of reasoning. If they reason the same way I do, and the problem they are facing is the same kind of problem I am facing, then I must expect that they will likely come to the same conclusion I will come to. This suggests that the prisoner’s dilemma in question is unlikely to end with an asymmetric outcome ((cooperate I defect) or (defect I cooperate)), but likely to end with a symmetric outcome ((cooperate I cooperate) or (defect I defect)). Because (cooperate I cooperate) is the best outcome for both parties amongst the symmetric outcomes, superrationality suggests one is best served by cooperating. At this point, readers may be skeptical whether this reasoning works. There seems to be some kind of shady action at a distance involved, where my choice to cooperate is somehow supposed to affect the other party’s choice, even though we are assuming that no information about my decision reaches said other party. But we can think of it this way: If reasoners are deterministic systems, and two reasoners follow the exact same decision algorithm in a highly similar decision situation, it at some point becomes logically contradictory to assume that the two reasoners will end up with diametrically opposed conclusions. Side note: By decision situations having to be “highly similar,” I do not mean that the situations agents find themselves in have to be particularly similar with respect to little details in the background. What I mean is that they should be highly similar in terms of all decision-relevant variables, the variables that are likely to make a difference to an agent’s decision. If we imagine a simplified decision situation where agents have to choose between two options, either press a button or not (and then something happens or not), it probably matters little whether one agent has the choice to press a red button and another agent is faced with pressing a blue button. As long as both buttons do the same thing, and as long as the agents are not (emotionally or otherwise) affected by the color differences, we can safely assume that the color of the button is highly unlikely to play a decision-relevant role. What is more likely relevant are things such as the payoffs (value according what an agent cares about) the agents expect from the available options. If one agent believes they stand to receive positive utility from pressing the button, and the other stands to receive negative utility, then that is guaranteed to make a relevant difference as to whether the agents will want to press their buttons. Maybe the payoff differentials are also.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/7MdLurJGhGmqRv25c/multiverse-wide-cooperation-in-a-nutshell},
	journaltitle = {Effective Altruism Forum},
	author = {Gloor, Lukas},
	date = {2017-11-02},
	file = {~/Google Drive/library-pdf/Gloor2017MultiversewideCooperationNutshell.pdf}
}

@online{Gloor2018CooperationHeuristics,
	database = {Tlön},
	title = {Cooperation heuristics},
	abstract = {Published on the {CLR} blog, where researchers are free to explore their own ideas on how humanity can best reduce suffering. (more) Summary This post was originally written for internal discussions only; it is half-baked and unpolished. The post assumes familiarity with the ideas discussed in Caspar Oesterheld’s paper Multiverse-wide cooperation via coordinated decision-making. I wrote a short introduction to multiverse-wide cooperation in an earlier post (but I still recommend reading parts of Caspar’s original paper, or this more advanced introduction, because several of the points that follow below build on topics not covered in my introduction). With that out of the way: In this post, I will comment on what I think might be interesting aspects of multiverse-wide cooperation […].},
	langid = {english},
	url = {https://longtermrisk.org/commenting-msr-part-2-cooperation-heuristics/},
	journaltitle = {Center on Long-Term Risk},
	author = {Gloor, Lukas},
	date = {2018-01-03},
	file = {~/Google Drive/library-pdf/Gloor2018CooperationHeuristics.pdf}
}

@online{GmbHFTXFoundationGroup,
	database = {Tlön},
	title = {The {FTX} Foundation Group Launches the {FTX} Climate
                  Program},
	abstract = {{FTX} Climate program includes carbon neutral initiatives, research funding, special projects, and carbon removal solutions {ANTIGUA}, July 28, 2021 /...},
	url = {https://markets.businessinsider.com/news/stocks/the-ftx-foundation-group-launches-the-ftx-climate-program-1030652227},
	journaltitle = {markets.businessinsider.com},
	author = {{GmbH}, finanzen net},
	urldate = {2021-10-25},
	langid = {english},
	file = {~/Google Drive/library-html/the-ftx-foundation-group-launches-the-ftx-climate-program-1030652227.html}
}

@Book{Godlovitch1974AnimalsMenAnd,
	database = {Tlön},
	isbn = {0394178254},
	address = {New York},
	langid = {english},
	publisher = {Grove Press},
	editor = {Godlovitch, Stanley and Godlovitch, Roslind and Harris, John},
	date = {1974},
	title = {Animals, Men and Morals: An Inquiry Into The Maltreatment of Non-humans},
	timestamp = {2023-07-04 19:35:25 (GMT)}
}

@online{GomezEmilsson2019CauseWhatWill,
	database = {Tlön},
	title = {Cause X — what will the new shiny effective altruist
                  cause be?},
	abstract = {The Qualia Research Institute hosted an interesting event a couple of weeks ago. Here is how the event was advertised: Description Event Name: {QRI} \& Friends: “Cause X” – what ….},
	langid = {english},
	url = {https://qualiacomputing.com/2019/02/07/cause-x-what-will-the-new-shiny-effective-altruist-cause-be/},
	journaltitle = {Qualia Computing},
	author = {Gómez Emilsson, Andrés},
	date = {2019-02-07},
	file = {~/Google Drive/library-pdf/GomezEmilsson2019CauseWhatWill.pdf}
}

@online{GoodJudgmentOpen2020AreThereAny,
	database = {Tlön},
	title = {Are there any forecasting tips, tricks, and
                  experiences you would like to share and/or discuss
                  with your fellow forecasters?},
	abstract = {This is a forum for {GJO} forecasters interested in sharing forecasting tips, tricks, and experiences. You can also share any models or procedures that you think might be useful to others. It is also a space where you can pose questions to other forecasters regarding how they approach their questions.},
	langid = {english},
	url = {https://www.gjopen.com/questions/1779-are-there-any-forecasting-tips-tricks-and-experiences-you-would-like-to-share-and-or-discuss-with-your-fellow-forecasters},
	journaltitle = {Good Judgment Open},
	author = {{Good Judgment Open}},
	date = {2020-09-04},
	file = {~/Google Drive/library-pdf/GoodJudgmentOpen2020AreThereAny.pdf}
}

@online{Gooen2015GuesstimateAppMaking,
	database = {Tlön},
	title = {Guesstimate: An app for making decisions with
                  confidence (intervals)},
	abstract = {I’m happy to announce the public beta of Guesstimate, a web app for creating models and Fermi estimates with probability distributions.  It’s free, open source, and relatively easy to use.
.
Effective Altruism is largely about optimizing expected value.  Expected value estimates often have high uncertainty for multiple inputs, but the uncertainty of the outputs is rarely calculated.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/Bt4nkCGHKBkDk97mn/guesstimate-an-app-for-making-decisions-with-confidence},
	journaltitle = {Effective Altruism Forum},
	author = {Gooen, Ozzie},
	date = {2015-12-30},
	file = {~/Google Drive/library-pdf/Gooen2015GuesstimateAppMaking.pdf}
}

@online{Grace2010LightConeEating,
	database = {Tlön},
	title = {Light cone eating {AI} explosions are not filters},
	abstract = {Some existential risks can’t account for any of the Great Filter. Here are two categories of existential risks that are not filters: Too big: any disaster that would destroy everyone in the o….},
	langid = {english},
	url = {https://meteuphoric.com/2010/11/05/light-cone-eating-ai-explosions-are-not-filters/},
	journaltitle = {Meteuphoric},
	author = {Grace, Katja},
	date = {2010-11-05},
	file = {~/Google Drive/library-pdf/Grace2010LightConeEating.pdf}
}

@online{Grace2014Superintelligence12Malignant,
	database = {Tlön},
	title = {Superintelligence 12: Malignant failure modes},
	abstract = {Malignant failures are failures that involve human extinction, in contrast with many failure modes where the AI simply does little. AI could commit perverse instantiation, where it does what you ask, but your request had unforeseen destructive consequences. AIs may even be incentivized to deliberately create perverse instantiations. This can lead to infrastructure profusion, where an AI uses most resources to expand infrastructure rather than pursuing its goals. Lastly, an AI could commit a mind crime, such as simulating people to learn about human psychology, then quickly destroying them. – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/BqoE5vhPNCB7X6Say/superintelligence-12-malignant-failure-modes},
	journaltitle = {{LessWrong}},
	author = {Grace, Katja},
	date = {2014-12-02},
	file = {~/Google Drive/library-pdf/Grace2014Superintelligence12Malignant.pdf}
}

@online{Grace2015Superintelligence20Value,
	file = {~/Google Drive/library-pdf/Grace2015Superintelligence20Value.pdf;~/Google Drive/library-html/Grace2015Superintelligence20Value.html},
	date = {2015-01-26},
	abstract = {Jude Gomila, who previously sold his mobile advertising company Heyzap, is building Golden, a "knowledge base" that aims to fill in Wikipedia's blind spots, particularly when it comes to emerging technologies and startups. It addresses the issue of Wikipedia's arbitrary notability threshold, according to which pages are deleted for not being notable enough. Golden allows users to work quickly with features like a WYSIWYG editor, automated suggestions, and high-resolution citations. It emphasizes transparency by tying account names to real identities and employing bot detection and protection mechanisms to prevent users from pretending to be someone else. Golden is exploring the potential of AI to help point out biased or marketing-oriented language, and it plans to initially make money by charging investment funds and large companies for a more sophisticated query tool. – AI-generated abstract.},
	journaltitle = {LessWrong},
	author = {Grace, Katja},
	title = {Superintelligence 20: the Value-Loading Problem},
	url = {https://www.lesswrong.com/posts/FP8T6rdZ3ohXxJRto/superintelligence-20-the-value-loading-problem},
	database = {Tlön},
	langid = {english},
	shorttitle = {Superintelligence 20},
	timestamp = {2023-07-04 11:36:49 (GMT)},
	urldate = {2023-07-04}
}

@online{Grace2017WantWantWant,
	database = {Tlön},
	title = {Want like want want},
	abstract = {“I want a donut” “Ok, I’ll buy one for you” “Oh, I don’t mean that on consideration I endorse purchasing one—I’m just expressing my urge to to eat a donut.” There are two meanings of ‘want’ in comm….},
	url = {https://meteuphoric.com/2017/01/09/want-like-want-want/},
	journaltitle = {Meteuphoric},
	author = {Grace, Katja},
	urldate = {2021-09-27},
	date = {2017-01-09},
	langid = {english},
	file = {~/Google Drive/library-pdf/Grace2017WantWantWant.pdf;~/Google Drive/library-html/want-like-want-want.html}
}

@online{Grace2022WhatMLResearchers,
	database = {Tlön},
	title = {What do {ML} researchers think about {AI} in 2022?},
	abstract = {Katja Grace Aug 4 2022 First findings from the new 2022 Expert Survey on Progress in {AI}.},
	url = {https://aiimpacts.org/what-do-ml-researchers-think-about-ai-in-2022/},
	journaltitle = {{AI} Impacts},
	author = {Grace, Katja},
	urldate = {2022-08-04},
	date = {2022-08-04},
	langid = {english},
	note = {Section: Blog},
	file = {~/Google Drive/library-pdf/Grace2022WhatMLResearchers.pdf;~/Google Drive/library-html/what-do-ml-researchers-think-about-ai-in-2022.html}
}

@online{Grace2023EstimacionEsMejor,
	database = {Tlön},
	keywords = {estimación de Fermi},
	date = {2023},
	langid = {spanish},
	author = {Grace, Katja},
	title = {La estimación es lo mejor que tenemos},
	translator = {Humarán, Aurora},
	translation = {Grace2011EstimationIsBest}
}

@online{Grace2023QueFaseDe,
	database = {Tlön},
	date = {2023},
	title = {¿Qué fase de la eficacia es más importante?},
	author = {Grace, Katja},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Grace2013WhichStageOf}
}

@article{Graham2020AvoidableHarm,
	database = {Tlön},
	author = {Graham, Peter A.},
	title = {Avoidable harm},
	volume = {101},
	number = {1},
	pages = {175–199},
	doi = {10.1111/phpr.12586},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/phpr.12586},
	date = {2020-07},
	issn = {0031-8205, 1933-1592},
	journaltitle = {Philosophy and Phenomenological Research},
	langid = {english},
	shortjournal = {Philos Phenomenol Res},
	timestamp = {2023-07-27 09:48:32 (GMT)},
	urldate = {2023-07-27}
}

@online{GraphQLTutorialLessWrong,
	database = {Tlön},
	title = {{GraphQL} tutorial for {LessWrong} and Effective
                  Altruism Forum - {LessWrong}},
	abstract = {This post is a tutorial on using {GraphQL} to query for information about {LessWrong} and the Effective Altruism Forum. It's mostly intended for people w….},
	langid = {english},
	url = {https://www.lesswrong.com/posts/LJiGhpq8w4Badr5KJ/graphql-tutorial-for-lesswrong-and-effective-altruism-forum},
	urldate = {2022-05-02},
	file = {~/Google Drive/library-html/graphql-tutorial-for-lesswrong-and-effective-altruism-forum.html}
}

@report{Greaves2019ResearchAgendaGlobal,
	url = {https://globalprioritiesinstitute.org/wp-content/uploads/2017/12/gpi-research-agenda.pdf},
	database = {Tlön},
	langid = {english},
	title = {A research agenda for the Global Priorities Institute},
	institution = {Global Priorities Institute, University of Oxford},
	author = {Greaves, Hilary and {MacAskill}, William and
                  O'Keeffe-O'Donovan, Rossa and Trammell, Philip},
	date = {2019-02},
	file = {~/Google Drive/library-pdf/Greaves2019ResearchAgendaGlobal.pdf}
}

@online{Greaves2020EvidenceCluelessnessLong,
	database = {Tlön},
	title = {Evidence, cluelessness, and the long term},
	abstract = {My talk has three parts. In part one, I'll talk about three of the basic canons of effective altruism, as I think most people understand them. Effectiveness, cost-effectiveness, and the value of evidence.
In part two, I'll talk about the limits of evidence. It's really important to pay attention to evidence, if you want to know what works. But a problem we face is that evidence can only go so far. In particular, I argue in the second part of my talk that most of the stuff that we ought to care about is necessarily stuff that we basically have no evidence for. This generates the problem that I call 'cluelessness'.
And in the third part of my talk, I'll discuss how we might respond to this fact. I don't know the answer and this is something that I struggle with a lot myself, but what I will do in the third part of the talk is I'll lay out five possible responses and I'll at least tell you what I think about each of those possible responses.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/LdZcit8zX89rofZf3/evidence-cluelessness-and-the-long-term-hilary-greaves},
	journaltitle = {Effective Altruism Forum},
	author = {Greaves, Hilary},
	date = {2020-11-01},
	file = {~/Google Drive/library-pdf/Greaves2020EvidenceCluelessnessLong.pdf}
}

@incollection{Greaves2022OptimumPopulationSize,
	database = {Tlön},
	location = {Oxford},
	abstract = {'The Oxford Handbook of Population Ethics' presents up-to-date theoretical analyses of various problems associated with the moral standing of future people and animals in current decision-making. The essays in this handbook shed light on the value of population change and the nature of our obligations to future generations. It brings together world-leading philosophers to introduce readers to some of the paradoxes of population ethics, challenge some fundamental assumptions that may be taken for granted in debates concerning the value of population change, and apply these problems and assumptions to real-world decisions.},
	langid = {english},
	title = {Optimum population size},
	isbn = {978-0-19-090768-6},
	booktitle = {The Oxford Handbook of Population Ethics},
	publisher = {Oxford University Press},
	author = {Greaves, Hilary},
	editor = {Arrhenius, Gustaf and Bykvist, Krister and Campbell,
                  Tim and Finneron-Burns, Elizabeth},
	date = 2022,
	file = {~/Google Drive/library-pdf/Greaves2022OptimumPopulationSize.pdf}
}

@online{Greaves2023IncertidumbreRadical,
	database = {Tlön},
	date = {2023},
	title = {Incertidumbre radical},
	author = {Greaves, Hilary},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Greaves2016Cluelessness}
}

@report{Greene2001PsychologicalPerspectiveNozick,
	database = {Tlön},
	location = {Cincinnati, Ohio},
	langid = {english},
	title = {A psychological perspective on Nozick’s experience
                  machine and parfit’s repugnant conclusion},
	institution = {Society for Philosophy and Psychology Annual Meeting},
	author = {Greene, Joshua D.},
	date = 2001
}

@online{Greig2021AnimalWelfareFund,
	database = {Tlön},
	title = {Animal Welfare Fund: ask us anything!},
	abstract = {Hi all,.
Managers of the {EA} Animal Welfare Fund will be available for an Ask Me Anything session on Friday, 14 May. We'll start early that morning and try to finish up by early that afternoon {PST}, so ideally please try to get your questions in on Wednesday or Thursday. Included below is some information that could be helpful for questions.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/pmr9tR2GYoxDMqbor/animal-welfare-fund-ask-us-anything},
	journaltitle = {Effective Altruism Forum},
	author = {Greig, Kieran},
	urldate = {2022-03-15},
	date = {2021-05-07},
	file = {~/Google Drive/library-html/Animal Welfare Fund\: Ask
                  us anything! - EA
                  Forum:animal-welfare-fund-ask-us-anything.html;~/Google
                  Drive/library-pdf/Greig2021AnimalWelfareFund.pdf}
}

@online{Grilo2022NumberOfSeabirds,
	database = {Tlön},
	file = {~/Google Drive/library-html/Grilo2022NumberOfSeabirds.html;~/Google Drive/library-pdf/Grilo2022NumberOfSeabirds.pdf},
	abstract = {1 kg of plastic is emitted to the ocean per capita per year .0.0001 seabirds and 0.00001 sea mammals are killed by marine plastic pollution per capita per year.200 wild fish are caught per capita per year.The catch of wild fish is 2 M times as large as the number of seabirds, and 20 M times as large as the number of sea mammals killed by marine plastic pollution.
The data and calculations are presented below.},
	langid = {english},
	author = {Grilo, Vasco},
	date = {2022-04-18},
	timestamp = {2023-02-17 21:23:01 (GMT)},
	title = {The number of seabirds and sea mammals killed by
                  marine plastic pollution is quite small relative to
                  the catch of fish},
	journaltitle = {Effective Altruism Forum},
	url = {https://forum.effectivealtruism.org/s/x3KXkiAQ6NH8WLbkW/p/tFfWpsEcCCKAXr8Zg},
	urldate = {2023-02-17}
}

@online{Grimes2020TobyOrdFireside,
	database = {Tlön},
	title = {Toby ord: Fireside chat and Q\&A},
	abstract = {If all goes well, human history is just beginning. Humanity could survive for billions of years, reaching heights of flourishing unimaginable today. But this vast future is at risk. For we have gained the power to destroy ourselves, and our entire potential, forever, without the wisdom to ensure we don’t.Toby Ord explains what this entails, with emphasis on the perspective of humanity — a major theme of his new book, The Precipice.Toby is a philosopher at Oxford University's Future of Humanity Institute. His work focuses on the big-picture questions facing humanity: What are the most important issues of our time? How can we best address them?Toby's earlier work explored the ethics of global health and global poverty. This led him to create an international society called Giving What We Can, whose members have pledged over \$1.4 billion to highly effective charities. He also co-founded the wider effective altruism movement, encouraging thousands of people to use reason and evidence to help others as much as possible. This is a transcript of a Q\&A with Toby, lightly edited for clarity.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/QHxjRx8zpqL4xxsXT/toby-ord-fireside-chat-and-q-and-a},
	journaltitle = {Effective Altruism Global},
	author = {Grimes, Barry},
	date = {2020-03-21}
}

@thesis{Grobel1933SocietyDiffusionUseful,
	database = {Tlön},
	title = {The society for the diffusion of useful knowledge,
                  1826-1846},
	langid = {english},
	institution = {University of London},
	type = {{MA} thesis},
	author = {Grobel, Monica Christina},
	date = 1933
}

@article{Groseclose2005MeasureMediaBias,
	database = {Tlön},
	title = {A measure of media bias},
	volume = 120,
	issn = {0033-5533, 1531-4650},
	url = {https://academic.oup.com/qje/article-lookup/doi/10.1162/003355305775097542},
	doi = {10.1162/003355305775097542},
	pages = {1191–1237},
	number = 4,
	journaltitle = {The Quarterly Journal of Economics},
	shortjournal = {The Quarterly Journal of Economics},
	author = {Groseclose, T. and Milyo, J.},
	urldate = {2021-09-14},
	date = {2005-11-01},
	langid = {english},
	file = {~/Google Drive/library-pdf/Groseclose2005MeasureMediaBias.pdf}
}

@online{GuardingAgainstPandemics2021GuardingPandemics,
	database = {Tlön},
	title = {Guarding Against Pandemics},
	abstract = {This brief post gives some background on Guarding Against Pandemics ({GAP}), which does non-partisan political advocacy for biosecurity work in the U.S. and has unique potential for impact and fundraising needs. Specifically, it makes the case for donating to {GAP}’s Political Action Committee ({PAC}). While {GAP}’s lobbying work (e.g. talking to members of Congress) is already well-funded by Sam Bankman-Fried and others, another important part of {GAP}’s work is supporting elected officials from both parties who will advocate for biosecurity and pandemic preparedness. U.S. campaign contribution limits require that this work be supported by many small-to-medium-dollar donors. Even though many projects within the {EA} space are typically not funding constrained, the significant upside of political contributions combined with U.S. campaign contribution laws make the {PAC} a uniquely good opportunity for small-dollar donors interested in reducing global catastrophic biological risk.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/Btm562wDNEuWXj9Gk/guarding-against-pandemics},
	journaltitle = {Effective Altruism Forum},
	author = {{Guarding Against Pandemics}},
	date = {2021-09-18},
	file = {~/Google Drive/library-pdf/GuardingAgainstPandemics2021GuardingPandemics.pdf}
}

@online{Gurnee2022WhyEANeeds,
	database = {Tlön},
	title = {Why {EA} needs operations research: the science of
                  decision making},
	abstract = {Operations Research ({OR}) is the field of applying advanced analytics to make better decisions. That is, making the best possible decision under constraints and uncertainty – exactly the problem that {EAs} try to solve every day. Given the relevance and apparent lack of engagement with {OR}, I think this is a neglected skill set within {EA}.},
	url = {https://forum.effectivealtruism.org/posts/kZ77cifvvNcwG56sH/why-ea-needs-operations-research-the-science-of-decision},
	shorttitle = {Why {EA} needs Operations Research},
	journaltitle = {Effective Altruism Forum},
	author = {Gurnee, Wes},
	urldate = {2022-07-22},
	date = {2022-07-21},
	langid = {english},
	file = {~/Google Drive/library-pdf/Gurnee2022WhyEANeeds.pdf}
}

@incollection{Gustafsson2022OurIntuitiveGrasp,
	database = {Tlön},
	location = {New York},
	abstract = {'The Oxford Handbook of Population Ethics' presents up-to-date theoretical analyses of various problems associated with the moral standing of future people and animals in current decision-making. The essays in this handbook shed light on the value of population change and the nature of our obligations to future generations. It brings together world-leading philosophers to introduce readers to some of the paradoxes of population ethics, challenge some fundamental assumptions that may be taken for granted in debates concerning the value of population change, and apply these problems and assumptions to real-world decisions.},
	langid = {english},
	title = {Our intuitive grasp of the repugnant conclusion},
	isbn = {978-0-19-090768-6},
	pages = {371–390},
	booktitle = {The Oxford Handbook of Population Ethics},
	publisher = {Oxford University Press},
	author = {Gustafsson, Johan E.},
	editor = {Arrhenius, Gustaf and Bykvist, Krister and Campbell,
                  Tim},
	date = 2022,
	file = {~/Google Drive/library-pdf/GustafssonOurIntuitiveGrasp.pdf}
}

@online{Ha2019GoldenUnveilsWikipedia,
	database = {Tlön},
	title = {Golden unveils a Wikipedia alternative focused on
                  emerging tech and startups},
	abstract = {This strategic document for Effective Altruism London (EAL) details its vision and mission statement, defining its areas of focus and deemphasizing less critical ones, particularly those requiring more organization and involving people already part of the EAL community. Activities are divided into community-wide coordination (e.g., talks, meet-ups, social events, and workshops) and meta-activities to facilitate effective coordination within the EAL community. The success of these activities will be evaluated using established and new metrics. – AI-generated abstract.},
	url = {https://social.techcrunch.com/2019/04/30/golden-launch/},
	journaltitle = {{TechCrunch}},
	author = {Ha, Anthony},
	urldate = {2022-01-05},
	date = {2019-04-30},
	langid = {english},
	file = {~/Google Drive/library-pdf/Ha2019GoldenUnveilsWikipedia.pdf;~/Google Drive/library-html/golden-launch.html}
}

@online{Habryka2018IntroducingAIAlignment,
	database = {Tlön},
	title = {Introducing the {AI} Alignment Forum ({FAQ})},
	abstract = {This article sets out to introduce the AI Alignment Forum as a new website and online hub specifically for the purpose of expediting research and discussion in the realm of AI Alignment. Several facets of this new Forum are brought to light, ranging from its intended audience and the researchers who helped build it, to the makeup of its team and the systems regarding content moderation/crossposting. Furthermore, the article seeks to address the need for an online space dedicated to AI Alignment, who exactly the Forum is designed to cater towards, and the type of content it is suited for. In summary, the AI Alignment Forum is a platform intending to help coordinate researchers in the field while improving the onboarding of new researchers as well as those interested in public outreach. – AI-generated abstract.},
	langid = {english},
	url = {https://www.alignmentforum.org/posts/FoiiRDC3EhjHx7ayY/introducing-the-ai-alignment-forum-faq},
	journaltitle = {{AI} Alignment Forum},
	author = {Habryka, Oliver and Pace, Ben and Arnold, Raymond and
                  Babcock, Jim},
	date = {2018-10-29},
	file = {~/Google Drive/library-pdf/Habryka2018IntroducingAIAlignment.pdf}
}

@online{Hadshar2020WhatFhisResearch,
	database = {Tlön},
	date = {2020-08-11},
	abstract = {{FHI}’s Research Scholars Programme ({RSP}) has now been running for just under two years, and we’re excited to have launched applications for a third cohort of research scholars.
In this post, I ({RSP}’s project manager) want to share some scholars’ responses to a series of prompts about {RSP}, in their own words. I’ve removed some prompts where there weren’t many responses or I didn’t think they’d be very helpful, and sometimes lightly edited the responses for clarity.},
	journaltitle = {Effective Altruism Forum},
	title = {What {FHI}'s Research Scholars Programme is
                  like: views from scholars},
	url = {https://forum.effectivealtruism.org/posts/e8CXMz3PZqSir4uaX/what-fhi-s-research-scholars-programme-is-like-views-from-1},
	timestamp = {2023-07-20 19:40:37 (GMT)},
	shorttitle = {What {FHI}'s Research Scholars Programme is
                  like},
	author = {Hadshar, Rose},
	urldate = {2023-07-20},
	langid = {english}
}

@online{Hadshar2022HowMoralProgress,
	database = {Tlön},
	title = {How moral progress happens: the decline of footbinding
                  as a case study},
	abstract = {History is really complicated. I think it’s an important virtue to be able to stick your neck out and say ‘I think x was mostly caused by y’ - but underneath statements like that there’s a huge cloud of possibilities and entanglements and holes in the data.
The first reference you come across via {EA} networks may be pretty poor. I told some people in the {EA} space that I was researching the decline of footbinding. Two people independently suggested a relevant chapter of a book to me (without claiming that it was good). The people were both philosophers, and the book they suggested was also by a philosopher, which is probably partly why they had heard of it. I think the relevant chapter isn’t actually worth reading if you want to understand why footbinding declined: it doesn’t mention economics at all, and seems to equate the prohibition on footbinding with the end of footbinding as a practice, which is quite confused. This wasn’t much of a problem for me given that I also read other stuff - but if instead of doing a research project I had just wanted to learn something interesting about footbinding, I might have come away with quite a misleading picture of what happened. My takeaway is: don’t assume that because someone is smart, the single reference they have on a topic they don’t know much about is any good.},
	url = {https://forum.effectivealtruism.org/posts/bRbJJw25dJ8a8pmn5/how-moral-progress-happens-the-decline-of-footbinding-as-a-3},
	shorttitle = {How moral progress happens},
	journaltitle = {Effective Altruism Forum},
	author = {Hadshar, Rose},
	urldate = {2022-07-26},
	date = {2022-07-26},
	langid = {english},
	file = {~/Google Drive/library-pdf/Hadshar2022HowMoralProgress.pdf}
}

@online{Hajek2002InterpretationsProbability,
	database = {Tlön},
	title = {Interpretations of probability},
	langid = {english},
	url = {https://plato.stanford.edu/archives/win2002/entries/probability-interpret/},
	journaltitle = {Stanford Encyclopedia of Philosophy},
	author = {Hájek, Alan},
	date = {2002-10-21}
}

@online{Halstead2017WhereShouldAntipaternalists,
	database = {Tlön},
	title = {Where should anti-paternalists donate?},
	abstract = {{GiveDirectly} gives out unconditional cash transfers to some of the poorest people in the world. It’s clearly an outstanding organisation that is exceptionally data driven and transparent. However, according to {GiveWell}’s cost-effectiveness estimates (which represent a weighted average of the diverse views of {GiveWell} staffers), it is significantly less cost-effective than other recommended charities. For … Continue reading "Where should anti-paternalists donate?".},
	langid = {english},
	url = {https://johnhalstead.org/index.php/2017/05/04/anti-paternalists-donate/},
	journaltitle = {John Halstead's Blog},
	author = {Halstead, John},
	date = {2017-05-04},
	file = {~/Google Drive/library-pdf/Halstead2017WhereShouldAntipaternalists.pdf}
}

@online{Halstead2018ClimateChangeExistential,
	database = {Tlön},
	title = {Climate change and existential risk},
	abstract = {The article discusses the strategy of the London chapter of a social movement called Effective Altruism (EA). EA is characterized by the concern for maximizing positive impact through rational decision-making. The strategy addresses areas of focus and activities that EA London will engage in, as well as the community's mission and vision. Coordinative activities will be prioritized, including meta-activities targeting the EA community itself and community-wide activities. The article also outlines metrics that will be used to measure the effectiveness of EA London – AI-generated abstract.},
	url = {https://docs.google.com/document/d/1qmHh-cshTCMT8LX0Y5wSQm8FMBhaxhQ8OlOeRLkXIF0/edit?usp=embed_facebook},
	journaltitle = {John Halstead's Blog},
	author = {Halstead, John},
	urldate = {2022-03-26},
	date = {2018-10-17},
	langid = {english},
	file = {~/Google Drive/library-pdf/Halstead2018ClimateChangeExistential.pdf;~/Google Drive/library-html/edit.html}
}

@online{Halstead2019SafeguardingFutureCause,
	database = {Tlön},
	title = {Safeguarding the future: Cause area report},
	langid = {english},
	url = {https://assets.ctfassets.net/x5sq5djrgbwu/5C1hNPO8RK2E3RzH9dj88M/1fd2c52ab1e534af95c25c5ebea92b49/Cause_Report_-_Safeguarding_the_Future.pdf},
	journaltitle = {Founders Pledge},
	author = {Halstead, John},
	date = {2019-01},
	eventdate = {2020-12},
	file = {~/Google Drive/library-pdf/Halstead2019SafeguardingFutureCause.pdf}
}

@online{Halstead2020BayesianismVsScientism,
	database = {Tlön},
	title = {Bayesianism vs scientism},
	abstract = {There is an unfortunate divide in the rationalist tribe between Bayesians and believers in scientism. Bayesians are those who rationally incorporate all sources of information when choosing what credence to have in different propositions. You have prior credences that are set by common sense, theoretical arguments, empirical information and so on. You then update from … Continue reading "Bayesianism vs scientism".},
	langid = {english},
	url = {https://johnhalstead.org/index.php/2020/10/18/bayesianism-vs-scientism/?utm_source=rss&utm_medium=rss&utm_campaign=bayesianism-vs-scientism},
	journaltitle = {John Halstead's blog},
	author = {Halstead, John},
	date = {2020-10-18},
	file = {~/Google Drive/library-pdf/Halstead2020BayesianismVsScientism.pdf}
}

@online{Halstead2022ClimateChangeLongtermism,
	database = {Tlön},
	title = {Climate change \& longtermism},
	langid = {english},
	url = {https://docs.google.com/document/d/1az3MesNlGDETeJ8jGoyK-MTH-lCKbYg4EP4khQ-7naA},
	journaltitle = {What We Owe the Future: Supplementary Materials},
	author = {Halstead, John},
	date = 2022,
	file = {~/Google Drive/library-pdf/Halstead2022ClimateChangeLongtermism.pdf}
}

@book{Hamburger1965IntellectualsPoliticsJohn,
	database = {Tlön},
	location = {New Haven},
	langid = {english},
	title = {Intellectuals in politics: John Stuart Mill and the
                  philosophic radicals},
	isbn = {978-0-300-00532-5},
	publisher = {Yale University Press},
	author = {Hamburger, Joseph},
	date = 1965,
	file = {~/Google Drive/library-pdf/Hamburger1965IntellectualsPoliticsJohn.pdf}
}

@online{Handbook2022MoreToExplore1,
	database = {Tlön},
	file = {~/Google Drive/library-pdf/Handbook2022MoreToExplore1.pdf;~/Google Drive/library-html/Handbook2022MoreToExplore1.html},
	abstract = {Further reading on effective altruism in general, moral tradeoffs, and thinking carefully.},
	langid = {english},
	author = {EA Handbook},
	date = {2022-04-12},
	timestamp = {2023-02-17 21:29:37 (GMT)},
	title = {More to explore on 'The Effectiveness Mindset'},
	journaltitle = {Effective Altruism Forum},
	url = {https://forum.effectivealtruism.org/s/B79ro5zkhndbBKRRX/p/n5wspkQs4QYa7BEKi},
	urldate = {2023-02-17}
}

@online{Handbook2022MoreToExplore3,
	database = {Tlön},
	file = {~/Google Drive/library-pdf/Handbook2022MoreToExplore3.pdf;~/Google Drive/library-html/Handbook2022MoreToExplore3.html},
	abstract = {Further reading on longtermism and forecasting.},
	langid = {english},
	author = {EA handbook},
	date = {2022-07-06},
	shorttitle = {More to explore on 'What could the future hold?},
	timestamp = {2023-02-17 22:38:04 (GMT)},
	title = {More to explore on 'What could the future hold? And
                  why care?'},
	journaltitle = {Effective Altruism Forum},
	url = {https://forum.effectivealtruism.org/s/G7XBTGNTrPWoKFmep/p/LpMnCbhPNXdFuXHEm},
	urldate = {2023-02-17}
}

@online{Handbook2022MoreToExplore5,
	database = {Tlön},
	file = {~/Google Drive/library-html/Handbook2022MoreToExplore5.html;~/Google Drive/library-pdf/Handbook2022MoreToExplore5.pdf},
	abstract = {The Expanding Circle pg. 111-124 ‘Expanding the Circle of Ethics’ section (20 mins.)The Narrowing Circle (see here for summary and discussion) - An argument that the “expanding circle” historical thesis ignores all instances in which modern ethics narrowed the set of beings to be morally regarded, often backing its exclusion by asserting their non-existence, and thus assumes its conclusion. (30 mins.)Our descendants will probably see us as moral monsters. What should we do about that? - 80,000 Hours - A conversation with Professor Will {MacAskill}. (Podcast - 1 hour 50 mins.)The Possibility of an Ongoing Moral Catastrophe (full text of the required article, 30 mins.).},
	langid = {english},
	author = {EA Handbook},
	date = {2022-07-03},
	timestamp = {2023-02-18 12:12:46 (GMT)},
	title = {More to explore on 'Radical empathy'},
	journaltitle = {Effective Altruism Forum},
	url = {https://forum.effectivealtruism.org/s/QMrYGgBvg64JhcQrS/p/Mkckb5TAf8ZxjY7iz},
	urldate = {2023-02-18}
}

@online{Handbook2022MoreToExplore7,
	database = {Tlön},
	file = {~/Google Drive/library-html/Handbook2022MoreToExplore7.html;~/Google Drive/library-pdf/Handbook2022MoreToExplore7.pdf},
	abstract = {Policy and research ideas to reduce existential risk - 80,000 Hours (5 mins.)The Vulnerable World Hypothesis - Future of Humanity Institute - Scientific and technological progress might change people’s capabilities or incentives in ways that would destabilize civilization. This paper introduces the concept of a vulnerable world: roughly, one in which there is some level of technological development at which civilization almost certainly gets devastated by default. (45 mins.).
Democratizing Risk: In Search of a Methodology to Study Existential Risk (50 mins.)A critical review of “The Precipice” (maybe come back to the “Unaligned Artificial Intelligence” section next week, once you’ve engaged with the argument for {AI} risk).},
	langid = {english},
	author = {EA Handbook},
	date = {2022-07-14},
	timestamp = {2023-02-18 12:16:13 (GMT)},
	title = {More to explore on 'Our final century'},
	journaltitle = {Effective Altruism Forum},
	url = {https://forum.effectivealtruism.org/s/vSAFjmWsfbMrTonpq/p/3dmJKhssWCSLkAt74},
	urldate = {2023-02-18}
}

@online{Handbook2023AnimalAdvocacyCareersb,
	database = {Tlön},
	translation = {Handbook2023AnimalAdvocacyCareers},
	date = {2023},
	journaltitle = {Biblioteca Altruismo Eficaz},
	title = {Animal Advocacy Careers (sitio web para explorar)},
	author = {{EA Handbook}},
	langid = {spanish},
	timestamp = {2023-12-08 15:54:53 (GMT)}
}

@online{Handbook2023EjercicioParaEmpatia,
	database = {Tlön},
	date = {2023},
	title = {Ejercicio para "Empatía radical"},
	author = {{EA Handbook}},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Handbook2022ExerciseForRadical}
}

@online{Handbook2023EjercicioQueNos,
	database = {Tlön},
	date = {2023},
	title = {Ejercicio "¿Qué nos deparará el futuro? ¿Y por qué
                  preocuparse?"},
	author = {{EA Handbook}},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Handbook2022ExerciseForWhat2}
}

@online{Handbook2023EstimacionDeFermi,
	database = {Tlön},
	date = {2023},
	title = {Estimación de Fermi},
	author = {{EA Handbook}},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Forum2021FermiEstimate}
}

@online{Handbook2023MasInformacionSobre2,
	database = {Tlön},
	date = {2023},
	title = {Más información sobre "¿Qué opinas?"},
	author = {{EA Handbook}},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Handbook2022MoreToExplore2}
}

@online{Handbook2023MasInformacionSobre4,
	database = {Tlön},
	date = {2023},
	title = {Más información sobre “Riesgos derivados de la inteligencia
                  artificial"},
	author = {{EA Handbook}},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Handbook2022MoreToExplore4}
}

@online{Handbook2023MasInformacionSobre6,
	database = {Tlön},
	date = {2023},
	title = {Más información sobre “Ponerlo en práctica"},
	author = {{EA Handbook}},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Handbook2022MoreToExplore6}
}

@online{Handbook2023MasInformacionSobre8,
	database = {Tlön},
	date = {2023},
	title = {Más información sobre "Diferencias de impacto"},
	author = {{EA Handbook}},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Handbook2022MoreToExplore8}
}

@book{Hankins2008RecoveryAncientPhilosophya,
	database = {Tlön},
	location = {Firenze},
	langid = {english},
	title = {The recovery of ancient philosophy in the Renaissance:
                  a brief guide},
	isbn = {978-88-222-5769-7},
	series = {Quaderni di Rinascimento / Istituto nazionale di studi
                  sul Rinascimento},
	shorttitle = {The recovery of ancient philosophy in the Renaissance},
	pagetotal = 94,
	number = 44,
	publisher = {L.S. Olschki},
	author = {Hankins, James and Palmer, Ada},
	date = 2008,
	note = {{OCLC}: ocn214232540},
	file = {~/Google Drive/library-pdf/Hankins2008RecoveryAncientPhilosophya.pdf}
}

@Report{Hanson2001EconomicGrowthGiven,
	number = {technical report},
	institution = {University of California},
	abstract = {A simple exogenous growth model is used to conservatively estimate the economic implications of machine intelligence. When computers were expensive, they were only used for a limited number of tasks where they had a strong advantage over humans, in which case they complemented human labor and raised wages. However, as computers become less expensive, they start to take over tasks where they only have a weak advantage over humans, eventually replacing them completely and thus lowering wages. As computer technology improves at a faster rate than general technology, overall economic growth increases dramatically once machine intelligence starts replacing labor. At the same time, the population of machine intelligences increases very rapidly to keep up with demand, resulting in a Malthusian population dynamic with rapidly falling per-intelligence consumption. – AI-generated abstract.},
	langid = {english},
	database = {Tlön},
	title = {Economic growth given machine intelligence},
	url = {https://mason.gmu.edu/~rhanson/aigrow.pdf},
	author = {Hanson, Robin},
	date = 2001,
	file = {~/Google Drive/library-pdf/Hanson2001EconomicGrowthGiven.pdf}
}

@online{Hanson2011LetUsGive,
	database = {Tlön},
	title = {Let us give to future},
	abstract = {18 months ago I wondered: Franklin … [left] £1000 each to Philadelphia and Boston in his will to be invested for 200 years. … by 1990 the funds had grown to 2.3, 5M\$. … Why has Franklin’s example inspired no copy-cats?.},
	langid = {english},
	url = {https://www.overcomingbias.com/2011/09/let-us-give-to-future.html},
	journaltitle = {Overcoming bias},
	author = {Hanson, Robin},
	date = {2011-09-19},
	file = {~/Google Drive/library-pdf/Hanson2011LetUsGive.pdf}
}

@book{Hanson2013HansonYudkowskyAIfoomDebate,
	database = {Tlön},
	location = {Berkeley},
	langid = {english},
	title = {The Hanson-Yudkowsky {AI}-foom debate},
	isbn = {978-1-939311-03-0},
	publisher = {Machine Intelligence Research Institute},
	author = {Hanson, Robin and Yudkowsky, Eliezer},
	date = 2013,
	file = {~/Google Drive/library-pdf/Hanson2013HansonYudkowskyAIfoomDebate.pdf}
}

@online{Hanson2015BewareGeneralVisible,
	database = {Tlön},
	title = {Beware general visible prey},
	abstract = {Charles Stross recently on possible future great filters: So {IO}9 ran a piece by George Dvorsky on ways we could wreck the solar system. And then Anders Sandberg responded in depth on the subject of existential risks, asking what conceivable threats have big enough spatial reach to threaten an interplanetary or star-faring civilization. … The implication of an [future great filter] is that it doesn’t specifically work against life, it works against interplanetary colonization. … much as Kessler syndrome could effectively block all access to low Earth orbit as a side-effect of carelessly launching too much space junk. Here are some example scenarios.},
	langid = {english},
	url = {https://www.overcomingbias.com/2015/04/beware-general-visible-near-prey.html},
	journaltitle = {Overcoming bias},
	author = {Hanson, Robin},
	date = {2015-04-19},
	file = {~/Google Drive/library-pdf/Hanson2015BewareGeneralVisible.pdf}
}

@online{Hanson2018LongLegaciesFights,
	database = {Tlön},
	title = {Long legacies and fights},
	langid = {english},
	url = {https://www.youtube.com/watch?v=fZX7h9Mt8fw&feature=youtu.be},
	editora = {Hanson, Robin},
	editoratype = {collaborator},
	date = {2018-11-30},
	note = {Publication title: Envision conference tex.editortype:
                  director}
}

@online{Hanson2019BigWarRemains,
	database = {Tlön},
	title = {Big war remains possible},
	abstract = {The following poll suggests that a majority of my Twitter followers think war will decline; in the next 80 years we won’t see a 15 year period with a war death rate above the median level we’ve see over the last four centuries:.},
	langid = {english},
	url = {https://www.overcomingbias.com/2019/07/big-war-remains-possible.html},
	journaltitle = {Overcoming bias},
	author = {Hanson, Robin},
	date = {2019-07-25},
	file = {~/Google Drive/library-pdf/Hanson2019BigWarRemains.pdf}
}

@online{Hanson2019HowLumpyAI,
	database = {Tlön},
	title = {How lumpy {AI} services?},
	abstract = {Long ago people like Marx and Engels predicted that the familiar capitalist economy would naturally lead to the immiseration of workers, huge wealth inequality, and a strong concentration of firms. Each industry would be dominated by a main monopolist, and these monsters would merge into a few big firms that basically run, and ruin, everything. (This is somewhat analogous to common expectations that military conflicts naturally result in one empire ruling the world.).},
	langid = {english},
	url = {https://www.overcomingbias.com/2019/02/how-lumpy-ai-services.html},
	journaltitle = {Overcoming Bias},
	author = {Hanson, Robin},
	date = {2019-02-14},
	file = {~/Google Drive/library-pdf/Hanson2019HowLumpyAI.pdf}
}

@online{Hanson2021LongReflectionCrazy,
	database = {Tlön},
	title = {‘Long reflection’ is crazy bad idea},
	abstract = {Some futurist philosophers have recently become enthused by what seems to me a spectacularly bad idea. Here is their idea: Some effective altruists … have argued that, if humanity succeeds in eliminating existential risk or reducing it to acceptable levels, it should not immediately embark on an ambitious and potentially irreversible project (such as space colonization) of arranging the universe’s resources in accordance to its values, but ought instead to spend considerable time— “centuries (or more)” (Ord 2020), “perhaps tens of thousands of years” (Greaves et al. 2019), “thousands or millions of years” (Dai 2019), “[p]erhaps… a million years” ({MacAskill}, in Perry 2018)—figuring out what is in fact of value. The long reflection may thus be seen as an intermediate stage in a rational long-term human developmental trajectory, following an initial stage of existential security when existential risk is drastically reduced and followed by a final stage when humanity’s potential is fully realized (Ord 2020). (.},
	langid = {english},
	url = {https://www.overcomingbias.com/2021/10/long-reflection-is-crazy-bad-idea.html},
	journaltitle = {Overcoming Bias},
	author = {Hanson, Robin},
	urldate = {2021-10-03},
	date = {2021-10-20},
	file = {~/Google Drive/library-pdf/Hanson2021LongReflectionCrazy.pdf;~/Google Drive/library-html/long-reflection-is-crazy-bad-idea.html}
}

@online{Hanson2021SimpleModelGrabby,
	database = {Tlön},
	title = {A simple model of grabby aliens},
	abstract = {Given a hard-steps model of advanced life timing, our date now seems very early. One explanation is: an early deadline set by “grabby” civilizations (GC) who expand rapidly, never die, and prevent new GC from arriving in volumes they control. If we might become grabby, our date is a sample GC origin date. A selection effect explains why we don’t see them when they control ~40\% of universe now. Each parameter in a 3 parameter model can be estimated to within an order of magnitude, allowing narrow estimates of GC spacing, appearance, and duration till see or meet them.},
	langid = {english},
	url = {https://www.youtube.com/watch?v=0lKliaFllPA},
	journaltitle = {Foresight Institute},
	author = {Hanson, Robin},
	urldate = {2021-08-19},
	date = {2021-02-09}
}

@article{Harman2004CanWeHarm,
	author = {Harman, Elizabeth},
	title = {Can we harm and benefit in creating?},
	volume = {18},
	number = {1},
	pages = {89–113},
	doi = {10.1111/j.1520-8583.2004.00022.x},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1520-8583.2004.00022.x},
	database = {Tlön},
	date = {2004-12},
	issn = {1520-8583, 1520-8583},
	journaltitle = {Philosophical Perspectives},
	langid = {english},
	shortjournal = {Philosophical Perspectives},
	timestamp = {2023-07-12 15:17:54 (GMT)},
	urldate = {2023-07-12}
}

@book{Harris2010MoralLandscapeHow,
	database = {Tlön},
	location = {New York},
	langid = {english},
	title = {The moral landscape: How science can determine human
                  values},
	isbn = {978-1-4516-1278-3},
	publisher = {Free Press},
	author = {Harris, Sam},
	date = 2010,
	file = {~/Google Drive/library-pdf/Harris2010MoralLandscapeHow.pdf}
}

@online{Harris2021HavingSuccessfulCareer,
	database = {Tlön},
	title = {Having a successful career with depression, anxiety
                  and imposter syndrome},
	abstract = {The first half of this conversation is a searingly honest account of Howie Lempel’s story, including losing a job he loved due to a depressed episode, what it was like to be basically out of commission for over a year, how he got back on his feet, and the things he still finds difficult today. The second half covers Howie’s advice. Conventional wisdom on mental health can be really focused on cultivating willpower — telling depressed people that the virtuous thing to do is to start exercising, improve their diet, get their sleep in check, and generally fix all their problems before turning to therapy and medication as some sort of last resort.},
	langid = {english},
	url = {https://80000hours.org/podcast/episodes/depression-anxiety-imposter-syndrome/},
	journaltitle = {80,000 Hours},
	author = {Harris, Keiran and Wiblin, Robert},
	date = {2021-05-19},
	file = {~/Google Drive/library-pdf/Harris2021HavingSuccessfulCareer.pdf}
}

@online{Harris2022NewNuclearSecurity,
	database = {Tlön},
	title = {New nuclear security grantmaking programme at Longview
                  Philanthropy},
	abstract = {Longview Philanthropy, where I work, launched a nuclear security grantmaking programme in December 2021.We are hiring a grantmaker to co-lead this programme alongside Carl Robichaud.The co-leads will make grants potentially totalling up to \$10 million initially, a figure which could grow substantially if they find or create sufficiently strong opportunities. [Update December 2022: This is now to be determined as we seek new funders for this work.]So far, we have committed a single \$1.6 million grant to the Council on Strategic Risks. Future grants will be directed by the programme co-leads.We are also hiring a grantmaker who will work on other existential risks.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/M7wNHbpqnLfDzmDK9/new-nuclear-security-grantmaking-programme-at-longview},
	journaltitle = {Effective Altruism Forum},
	author = {Harris, Kit},
	urldate = {2022-03-30},
	date = 2022,
	file = {~/Google Drive/library-pdf/Harris2022NewNuclearSecurity.pdf;~/Google Drive/library-html/new-nuclear-security-grantmaking-programme-at-longview.html}
}

@online{Hassabis2018CreativityAIRothschild,
	database = {Tlön},
	title = {Creativity and {AI}: the Rothschild Foundation
                  Lecture},
	abstract = {Recorded at the Royal Academy of Arts on 17 September 2018:Demis Hassabis, Co-Founder and {CEO} of {DeepMind}, draws upon his eclectic experiences as an Artifici...},
	langid = {english},
	url = {https://www.youtube.com/watch?v=d-bvsJWmqlc},
	shorttitle = {Demis Hassabis},
	journaltitle = {Royal Academy of Arts},
	author = {Hassabis, Demis},
	urldate = {2022-03-16},
	date = {2018-09-07}
}

@thesis{Hay2007UniversalSemimeasuresIntroduction,
	database = {Tlön},
	title = {Universal semimeasures: An introduction},
	langid = {english},
	url = {https://www.cs.auckland.ac.nz/CDMTCS//researchreports/300nick.pdf},
	institution = {University of Auckland},
	type = {{MSc} thesis},
	author = {Hay, Nicholas J.},
	date = 2007,
	note = {genre: {MSc} thesis},
	file = {~/Google Drive/library-pdf/Hay2007UniversalSemimeasuresIntroduction.pdf}
}

@online{Head2019HowPutYour,
	database = {Tlön},
	title = {How to put your own charity to the test},
	abstract = {Mass media can reach millions of people, but can it improve health and save lives as effectively as other top interventions? Past studies of mass-media campaigns for public health failed to find evidence of strong impact, belying the potential of the medium. In this talk, Roy Head, {CEO} of Development Media International ({DMI}), discusses the randomized controlled trial {DMI} ran on its own health campaign — and the surprising results.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/ctL62pQDcSM3p4GWj/roy-head-how-to-put-your-own-charity-to-the-test},
	shorttitle = {Roy Head},
	journaltitle = {Effective Altruism Global},
	author = {Head, Roy},
	urldate = {2022-03-26},
	date = {2019-10-18},
	file = {~/Google Drive/library-pdf/Head2019HowPutYour.pdf;~/Google Drive/library-html/roy-head-how-to-put-your-own-charity-to-the-test.html}
}

@online{Heim2021ComputeGovernanceConclusions,
	database = {Tlön},
	title = {Compute governance and conclusions},
	abstract = {Transformative {AI} and Compute - A holistic approach - Part 3 out of 4. This is part three of the series Transformative {AI} and Compute - A holistic approach.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/g6cwjcKMZba4RimJk/compute-governance-and-conclusions-transformative-ai-and},
	journaltitle = {Effective Altruism Forum},
	author = {Heim, Lennart},
	urldate = {2022-04-11},
	date = {2021-10-14},
	file = {~/Google Drive/library-pdf/Heim2021ComputeGovernanceConclusions.pdf;~/Google Drive/library-html/compute-governance-and-conclusions-transformative-ai-and.html}
}

@online{Helen2023AltruismoEficazEs,
	database = {Tlön},
	keywords = {altruismo eficaz, desarrollo del altruismo eficaz},
	date = {2023},
	langid = {spanish},
	author = {Helen},
	title = {El altruismo eficaz es una pregunta (no una
                  ideología)},
	translator = {Humarán, Aurora},
	translation = {Helen2023EffectiveAltruismIs}
}

@book{Helland2019WhyArePrices,
	database = {Tlön},
	location = {Arlington},
	langid = {english},
	title = {Why are the prices so damn high?},
	isbn = {978-1-942951-55-1},
	publisher = {Mercatus Center},
	author = {Helland, Eric and Tabarrok, Alex},
	date = 2019,
	file = {~/Google Drive/library-pdf/Helland2019WhyArePrices.pdf}
}

@book{Henderson2009SmallpoxDeathDisease,
	database = {Tlön},
	location = {Amherst, New York},
	abstract = {Dr. Henderson offers the inside story of how he led the World Health Organization's campaign to eradicate smallpox--the only disease in history to have been deliberately eliminated. Foreword by Preston, author of "The Hot Zone.".},
	langid = {english},
	title = {Smallpox: the death of a disease},
	isbn = {978-1-59102-722-5},
	publisher = {Prometheus},
	author = {Henderson, Donald A},
	date = 2009,
	file = {~/Google Drive/library-pdf/Henderson2009SmallpoxDeathDisease.pdf}
}

@article{Heyd1988ProcreationAndValue,
	author = {Heyd, David},
	title = {Procreation and value can ethics deal with futurity
                  problems?},
	volume = {18},
	number = {2},
	pages = {151–170},
	doi = {10.1007/BF02380074},
	url = {http://link.springer.com/10.1007/BF02380074},
	database = {Tlön},
	date = {1988-07},
	issn = {0048-3893, 1574-9274},
	journaltitle = {Philosophia},
	langid = {english},
	shortjournal = {Philosophia},
	timestamp = {2023-07-12 17:23:54 (GMT)},
	urldate = {2023-07-12}
}

@online{Hill2011EditortoreaderRatiosWikipedia,
	database = {Tlön},
	title = {Editor-to-reader ratios on Wikipedia},
	abstract = {It’s been reported for some time now that the number of active editors on Wikipedia (usually defined as people who have edited at least 5 times in a given month) peaked in 2007 and has been m….},
	langid = {english},
	url = {https://mako.cc/copyrighteous/editor-to-reader-ratios-on-wikipedia},
	journaltitle = {Copyrighteous},
	author = {Hill, Benjamin Mako},
	date = {2011-02-06},
	file = {~/Google Drive/library-pdf/Hill2011EditortoreaderRatiosWikipedia.pdf}
}

@online{Hillebrandt2020GrowthAndCase,
	database = {Tlön},
	title = {Growth and the case against randomista development},
	abstract = {Randomista development ({RD}) is a form of development economics which evaluates and promotes interventions that can be tested by randomised controlled trials ({RCTs}). It is exemplified by {GiveWell} (which primarily works in health) and the randomista movement in economics (which primarily works in economic development). Here we argue for the following claims, which we believe to be quite weak: (1) Prominent economists make plausible arguments which suggest that research on and advocacy for economic growth in low- and middle-income countries is more cost-effective than the things funded by proponents of randomista development. (2) Effective altruists have devoted too little attention to these arguments. (3) Assessing the soundness of these arguments should be a key focus for current generation-focused effective altruists over the next few years.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/bsE5t6qhGC65fEpzN/growth-and-the-case-against-randomista-development},
	journaltitle = {Effective Altruism Forum},
	author = {Hillebrandt, Hauke and Halstead, John},
	urldate = {2022-03-31},
	date = {2020-01-16}
}

@online{Hilton2019ManagingRiskEA,
	database = {Tlön},
	title = {Managing risk in the {EA} policy space},
	abstract = {There are risks to pushing out policy ideas: bad policy that harms society, reputational risks, and information hazards. I suggest that: campaigners for policy change or those who have access to power should consider carefully how to handle the various risks.Researchers and academics should make policy suggestions and not be afraid of talking to policy experts and government departments, although they should be aware that their ideas may well be terrible; funders and {EA} donors should look to fund policy interventions and draw the necessary expertise from across the {EA} community. This piece is a brief introduction to thinking about risks and policy influencing for anyone in the {EA} community space.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/Q7qzxhwEWeKC3uzK3/managing-risk-in-the-ea-policy-space},
	journaltitle = {Effective Altruism Forum},
	author = {Hilton, Samuel},
	date = {2019-12-09},
	file = {~/Google Drive/library-pdf/Hilton2019ManagingRiskEA.pdf}
}

@online{Hilton2023CambioClimatico,
	database = {Tlön},
	date = {2023},
	title = {Cambio climático},
	author = {Hilton, Benjamin},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Hilton2022ClimateChangeClimate}
}

@online{Hilton2023PrevenirCatastrofeRelacionada,
	database = {Tlön},
	date = {2023},
	title = {Prevenir una catástrofe relacionada con la
                  inteligencia artificial},
	author = {Hilton, Benjamin},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Hilton2022PreventingAIrelatedCatastrophe}
}

@online{Hoeijmakers2020LongtermInvestmentFunda,
	database = {Tlön},
	title = {Long-term investment fund at Founders Pledge - {EA}
                  Forum},
	abstract = {Edit 27/10/21: See these posts 1 2 3 for the next steps in this project. The post below was originally published on 09/01/20.
At Founders Pledge, we are considering launching a long-term investment fund for our members. Contributions to this fund would by default be invested, potentially over centuries or millennia to come. Grants would only be made when there’s a strong case that a donation opportunity beats investment from a longtermist perspective.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/8vfadjWWMDaZsqghq/long-term-investment-fund-at-founders-pledge},
	journaltitle = {Effective Altruism Forum},
	author = {Hoeijmakers, Sjir},
	date = {2020-07-03},
	file = {~/Google Drive/library-pdf/Hoeijmakers2020LongtermInvestmentFund.pdf}
}

@online{Holness-Tofts2020PoorMeatEater,
	database = {Tlön},
	title = {Poor meat eater problem},
	abstract = {Sometimes the long-run effects of an action can drastically affect how valuable the action is, and can even make something that seems on the surface to be beneficial to be actually harmful. Consider the poor meat-eater problem. Saving human lives, and making humans more prosperous, seem to be obviously good in terms of direct effects. However, humans consume animal products, and these animal products may cause considerable animal suffering, as well as increase greenhouse gas emissions.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/mGLYpBXvN3F2KCAP5/poor-meat-eater-problem},
	journaltitle = {Effective Altruism Forum},
	author = {Holness-Tofts, Alex},
	date = {2020-07-10},
	file = {~/Google Drive/library-pdf/Holness-Tofts2020PoorMeatEater.pdf}
}

@online{Horn2022FonixBioweaponsShelter,
	database = {Tlön},
	title = {Fønix: Bioweapons shelter project launch},
	abstract = {We are building a civilizational resiliency  project, “Fønix Logistics,” currently incubated by {EA} Sweden (Effektiv Altruism Sverige), which aims to research and create the ultimate refuge, especially from non-agentic  disasters and with a special focus on extinction-level bioweapons releases/(engineered) pandemics. The main reason to be excited about this project is that it could lower the risk of human extinction, as suggested by e.g. Andrew Snyder-Beattie and Ethan Alley as well as by the {FTX} Future Fund. Additionally, a project like this likely requires skills and expertise that might otherwise be hard to deploy in other {EA}-aligned projects. As one of the first team members in this project, you will have autonomy, take ownership of critical tasks and be invested from the early stage of selecting the best solutions.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/mfBxna3iFPGnWsmWB/fonix-bioweapons-shelter-project-launch},
	journaltitle = {Effective Altruism Forum},
	author = {Horn, Ulrik and Kim, Kayla and Tilli, Cecilia and
                  Skoglund, Vilhelm},
	date = {2022-06-14},
	file = {~/Google Drive/library-pdf/Horn2022FonixBioweaponsShelter.pdf}
}

@online{Hsieh2017HowLocalHousing,
	database = {Tlön},
	title = {How local housing regulations smother the U.S.
                  economy},
	abstract = {Americans can’t afford to move to the cities with strong job markets like New York, San Francisco and Boston.},
	langid = {english},
	url = {https://www.nytimes.com/2017/09/06/opinion/housing-regulations-us-economy.html},
	journaltitle = {The New York times},
	author = {Hsieh, Chang-Tai and Moretti, Enrico},
	date = {2017-09-06}
}

@article{Hua2021AIAntitrustReconciling,
	database = {Tlön},
	title = {{AI} \& antitrust: Reconciling tensions between
                  competition law and cooperative {AI} development},
	langid = {english},
	volume = 23,
	url = {https://yjolt.org/ai-antitrust-reconciling-tensions-between-competition-law-and-cooperative-ai-development},
	pages = {415–550},
	journaltitle = {Yale Journal of Law \& Technology},
	author = {Hua, Shin-Shin and Belfield, Haydn},
	date = 2021,
	file = {~/Google Drive/library-pdf/Hua2021AIAntitrustReconciling.pdf}
}

@online{Huang2023ComoEstudiantesLideraran,
	database = {Tlön},
	keywords = {alternativas a los productos de origen animal,
                  bienestar animal},
	date = {2023},
	langid = {spanish},
	author = {Huang, Amy},
	title = {Cómo los estudiantes liderarán la revolución de las
                  proteínas alternativas},
	translator = {Tlön},
	translation = {Huang2020HowStudentsWill}
}

@online{Hubinger2023DebemosSerMuy,
	database = {Tlön},
	date = {2023},
	title = {Debemos ser muy claros: el fraude al servicio del
                  altruismo eficaz es inaceptable},
	author = {Hubinger, Evan},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Hubinger2022WeMustBe}
}

@online{Huemer2019ChallengingExperts,
	database = {Tlön},
	title = {On challenging the experts},
	abstract = {Modern AI safety research requires thousands to millions of lines of code and engineers with a background in distributed systems and numerical systems and who also care about AI safety. Some of the most important work in AI safety is now being done by engineers, and those with the skills to contribute to major ML libraries are in high demand at AI safety labs. However, many engineers are unaware of these opportunities due to misconceptions about the role of engineers in AI safety research. – AI-generated abstract.},
	url = {https://fakenous.net/?p=550},
	journaltitle = {Fake Nous},
	author = {Huemer, Michael},
	urldate = {2022-05-13},
	date = {2019-07-06},
	langid = {english},
	file = {~/Google Drive/library-pdf/Huemer2019ChallengingExperts.pdf;~/Google Drive/library-html/fakenous.net.html}
}

@book{Huemer2019ProblemaDeAutoridad,
	translation = {Huemer2013ProblemPoliticalAuthority},
	database = {Tlön},
	langid = {spanish},
	date = {2019},
	publisher = {Deusto},
	address = {Barcelona},
	isbn = {9788423430970},
	title = {El problema de la autoridad política: un ensayo sobre el derecho a la coacción por parte del Estado y sobre el deber de la obediencia por parte de los ciudadanos},
	author = {Huemer, Michael},
	timestamp = {2023-05-25 20:38:16 (GMT)}
}

@PhDThesis{Hutchinson2014EthicsOfExtending,
	database = {Tlön},
	date = {2014},
	type = {PhD thesis},
	langid = {english},
	institution = {University of Oxford},
	title = {The Ethics of Extending and Creating Life},
	author = {Hutchinson, Michelle},
	timestamp = {2023-07-12 16:17:18 (GMT)}
}

@online{Hutchinson2016GivingWhatWe,
	database = {Tlön},
	title = {Giving What We Can is cause neutral},
	abstract = {Giving What We Can materials (including our website and presentations) typically talk about global poverty, even though as an organisation we are fundamentally cause neutral. Our recommended charities work in global health, while we have cause reports and ‘in-area’ recommendations for charities in poverty broadly construed (including, for example, climate change). That might seem to be a surprising choice, so in I’m going to write a couple of posts explaining why we do this. In this post, I’ll explore what cause neutrality is and say a bit about {GWWC}’s overall aims. The following post will be about how we see ourselves fitting into the {EA} ecosystem.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/tLdtftZakmpWq73kA/giving-what-we-can-is-cause-neutral},
	journaltitle = {Effective Altruism Forum},
	author = {Hutchinson, Michelle},
	date = {2016-04-22},
	file = {~/Google Drive/library-pdf/Hutchinson2016GivingWhatWe.pdf}
}

@online{Hutchinson2020ParentingThingsWish,
	database = {Tlön},
	title = {Parenting: Things I wish I could tell my past self},
	abstract = {I have a baby who’s nearly 10 months old. I’ve been thinking about what I’d like to be able to go back and tell myself before I embarked on this journey. I suspect that some of the differences between how I experienced it and what I had read in books correlates with ways that other effective altruists might also experience things. I also generally felt that finding decent no-nonsense information about parenting was hard, and that the signal to noise ratio when googling for answers was peculiarly bad. Probably the most useful advice I got was from {EA} friends with kids. So I thought it might be useful to jot down some thoughts for other {EAs} likely to have kids soon (or hoping to support others who are!).},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/Pq9GRnyAbNLC4TPkW/parenting-things-i-wish-i-could-tell-my-past-self},
	journaltitle = {Effective Altruism Forum},
	author = {Hutchinson, Michelle},
	urldate = {2022-04-06},
	date = {2020-09-12},
	file = {~/Google Drive/library-pdf/Hutchinson2020ParentingThingsWish.pdf;~/Google Drive/library-html/parenting-things-i-wish-i-could-tell-my-past-self.html}
}

@online{Hutchinson2023PorQueLargoplacismo,
	database = {Tlön},
	date = {2023},
	title = {Por qué el largoplacismo me resulta difícil y qué me
                  mantiene motivada},
	author = {Hutchinson, Michelle},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Hutchinson2021WhyFindLongtermism}
}

@online{Hutchinson2023TenerEnCuenta,
	database = {Tlön},
	date = {2023},
	title = {Tener en cuenta los absolutos},
	author = {Hutchinson, Michelle},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Hutchinson2018KeepingAbsolutesIn}
}

@online{Jabarian2022IntroducingEAeconCommunitybuilding,
	database = {Tlön},
	title = {Introducing {EAecon}: Community-building project},
	abstract = {I am very excited to share that my econ community-building project, {EAecon}, has received a major grant from {CEA} Events. Check out our first event, {EAecon} Retreat 2022.
You can express your interest in the upcoming {EAecon} Newsletter, future {EAecon} Events and whether you would like to join a potential {EAecon} Online Community by emailing hello@eaecon.org.
{EAecon} aims to develop and reinforce the social cohesion of economists from different levels of academic seniority and {EA}-engagement through sharing relevant resources and online and in-person events. During our events, participants can benefit from:.
Getting exposure to {GPR}/longtermist economics academic work before exploring further through the numerous more advanced early-career opportunities available at the Forethought Foundation (for instance, the Global Priority Fellowship, Summer Course in Economic Theory and Global Prioritization, Major Research Grants) and at the Global Priority Institute (for instance, the Early Career Conference Program, Prizes in {GPR}, Oxford Workshops on {GPR}).Networking with economists from different levels of academic seniority and {EA}-engagement at the same place and matching people for collaboration ({RAs}/co-authors).Developing their careers through dedicated mentoring introductory sessions on how to write an {EA}-engaged economic paper, how to apply to grad school, which {EA} jobs are available after your econ studies, before signing up for different coaching and supervising opportunities at Effective Thesis, the Econ Mentoring Program at {GPI} and office hours at 80,000 hours and the Econ Grad School Advice Slack.Pitching academic, charity, and social entrepreneurship projects in {EA}-related economics, getting feedback from {EA}-engaged economists to help you prepare applications to different funding opportunities offered by {EA} Funds, {FTX} Future Fund, Open Philanthropy, Gates Foundation, Longview Philanthropy, and multiple other grant-makers in the {EA} space (see an overview here).},
	url = {https://forum.effectivealtruism.org/posts/9gLtXR6KkZEYie8Au/introducing-eaecon-community-building-project},
	shorttitle = {Introducing {EAecon}},
	journaltitle = {Effective Altruism Forum},
	author = {Jabarian, Brian},
	urldate = {2022-05-30},
	date = {2022-05-30},
	langid = {english},
	file = {~/Google Drive/library-pdf/Jabarian2022IntroducingEAeconCommunitybuilding.pdf;~/Google Drive/library-html/introducing-eaecon-community-building-project.html}
}

@collection{Jamison2006DiseaseControlPriorities,
	database = {Tlön},
	location = {New York},
	edition = {2},
	langid = {english},
	title = {Disease control priorities in developing countries},
	isbn = {0-8213-0821361791},
	publisher = {Oxford University Press},
	editor = {Jamison, Dean T. and Breman, Joel G. and Measham,
                  Anthony R. and Alleyne, George and Claeson, Mariam and
                  Evans, David B. and Jha, Prabhat and Mills, Anne and
                  Musgrove, Philip},
	date = 2006,
	file = {~/Google Drive/library-pdf/Jamison2006DiseaseControlPriorities.pdf}
}

@online{Jebari2019CivilizationReemergingCatastrophic,
	database = {Tlön},
	title = {Civilization re-emerging after a catastrophic
                  collapse},
	langid = {english},
	url = {https://www.youtube.com/watch?v=Zhx5ieX-HPY},
	journaltitle = {{EAGxNordics}},
	author = {Jebari, Karim},
	date = {2019-04-07}
}

@incollection{Jenkins2003SocialMovementsSocial,
	database = {Tlön},
	location = {Cambridge},
	abstract = {Social movements contribute to social change through complex interactions with political opportunities and the institutions they aim to change. This paper reviews theories of social movements, emphasizing the importance of studying social movement change through the lens of interorganizational network approaches and institutional analysis. By analyzing how movements interact with their surrounding institutional context, researchers can better understand how they bring about changes in public policies, institutional and cultural practices, and the distribution of resources and power. The paper also discusses the methodological challenges involved in studying social movement change, including the need for longitudinal studies, multivariate analyses, careful sampling, and the integration of qualitative and quantitative data. By addressing these challenges, future research can provide a more comprehensive and nuanced understanding of the role of social movements in shaping social and political change. – AI-generated abstract.},
	langid = {english},
	title = {Social movements and social change},
	isbn = {1-905405-07-3},
	pages = {331–349},
	booktitle = {The handbook of political sociology},
	publisher = {Cambridge University Press},
	author = {Jenkins, J Craig and Form, William},
	editor = {Janoski, Thomas and Alford, Robert R. and Hicks,
                  Alexander M.},
	date = {2003-01-20},
	doi = {10.1017/CBO9780511818059.018},
	number = {7},
	file = {~/Google Drive/library-pdf/Jenkins2003SocialMovementsSocial.pdf}
}

@online{Johannsen2023ResumenDeWild,
	database = {Tlön},
	keywords = {bienestar de los animales salvajes},
	date = {2023},
	langid = {spanish},
	author = {Johannsen, Kyle},
	title = {Resumen de Wild Animal Ethics},
	translator = {Humarán, Aurora},
	translation = {Johannsen2022PrecisOfWild}
}

@online{John2023ReformaInstitucionalLargoplacista,
	database = {Tlön},
	date = {2023},
	title = {Reforma institucional largoplacista},
	author = {John, Tyler},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {John2021LongtermistInstitutionalReform}
}

@online{Johnson2019LastWeekDonated,
	database = {Tlön},
	title = {Last week I donated my left kidney anonymously to a
                  total stranger on the kidney waitlist. {AMA}!},
	langid = {english},
	url = {www.reddit.com/r/IAmA/comments/c7u0lx/last_week_i_donated_my_left_kidney_anonymously_to/},
	journaltitle = {Reddit},
	author = {Johnson, Jeremiah},
	urldate = {2021-09-29},
	date = {2019-07-01}
}

@online{Jones2021AISafetyNeeds,
	database = {Tlön},
	title = {{AI} safety needs great engineers},
	abstract = {Malevolent actors – persons characterized by dark tetrad traits (narcissism, psychopathy, Machiavellianism, and sadism) – pose grave risks to society, especially when in positions of power. Historical examples like Hitler and Stalin suggest that such individuals can cause catastrophic harm. Developing reliable measures and tests for these traits could help identify and mitigate their influence on institutions and prevent potential disasters. Despite historical precedents and the plausibility of this problem, research on this topic remains limited. – AI-generated abstract.},
	langid = {english},
	url = {https://www.alignmentforum.org/posts/YDF7XhMThhNfHfim9/ai-safety-needs-great-engineers},
	journaltitle = {{AI} Alignment Forum},
	author = {Jones, Andy},
	urldate = {2022-02-08},
	date = {2021-11-23},
	file = {~/Google Drive/library-pdf/Jones2021AISafetyNeeds.pdf;~/Google Drive/library-html/ai-safety-needs-great-engineers.html}
}

@online{JoshuaZ2015AstronomySpaceExploration,
	database = {Tlön},
	title = {Astronomy, space exploration and the Great Filter},
	abstract = {This study examines how astronomy may help identify potential future filters that prevent intelligent life from reaching the interstellar, large-scale phase – a concept known as the Great Filter. The analysis focuses on two fundamental versions of the Filter: filtration in the past and filtration in the future. It discusses evidence for and alternative explanations of the Great Filter and identifies natural threats, biological threats, nuclear exchanges, unexpected physics, global warming, artificial intelligence, resource depletion, nanotechnology, and aliens as potential contributors to the Filter. The study suggests that astronomical observations can provide valuable data about the Great Filter, but many potential filters will leave no observable astronomical evidence unless astronomical capabilities are greatly advanced. Increasing astronomy skills to detect failed civilizations and understand their mistakes is proposed as a strategy to pass the Great Filter, although the cost-effectiveness of this approach compared to other existential risk mitigation measures is uncertain. – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/dxuKW7DzKKeTf6arW/astronomy-space-exploration-and-the-great-filter},
	journaltitle = {{LessWrong}},
	author = {{JoshuaZ}},
	date = {2015-04-19},
	file = {~/Google Drive/library-pdf/JoshuaZ2015AstronomySpaceExploration.pdf}
}

@online{Juniewicz2022RetrospectiveShallReligious,
	database = {Tlön},
	title = {Retrospective on /Shall the Religious Inherit the
                  Earth/},
	abstract = {This piece is a retrospective on "Shall the Religious Inherit the Earth", a book by Eric Kaufmann published in 2010 which goes into one possible way that the future might go based on past demographic trends: in societies in which it is common for some couples to choose to have small families, there is a substantial gap between the fertility rates of the most religious, and everyone else. The book focuses especially on the growth of fundamentalist religious groups. In some cases, like the Amish, these groups have a near total rejection of modern technology. The book makes a number of predictions, some of which should have materialized by now, while others are still decades out. I will evaluate the claims about the specific high fertility religious groups the author brings up, as well as discussing overall global trends in religion and fertility. Future posts will delve further into arguments for why fertility rates matter and what influences them.
Book focuses on high growth rates of various religious groups, but for the most part they are experiencing substantial fertility decline in parallel with more secular groups, as well as {secularizationIn} the {US}:Identification with Protestantism, both mainline and Evangelical, is in decline, and there is little evidence of high growth from a committed {coreHispanic} immigration did not deliver a boost as large as predicted to Catholicism, both because of lower fertility and because of growing {secularismMormon} fertility has {declinedIn} the Middle East, North Africa and Asian countries with high Muslim populations, fertility has fallen substantially, with many places now close to or below replacement {fertilityWhile} the Muslim share of Europe will grow, it is likely that it will not grow fast enough to prevent population decline in the medium-term future (i.e. by 2050) without substantial further {migrationOf} the groups discussed in the book, the only substantial exceptions in terms of declining fertility are:Old Order Anabaptists (Amish, Hutterite, Old Order Mennonite)Though future growth may be difficult to maintain as farmland is growing less affordable for them, and farming is associated with higher fertility within these {groupsHaredi} (Ultra-Orthodox) {JewsThough} there are signs of increasing acceptance of the usage of technology and of higher labor force participation rates among both men and women, this has not had a clear impact on fertility or {retentionBut} I’m interested in hearing arguments for other groups!Globally, the dynamic will be driven by declining populations in highly secular Asian countries and rapidly growing highly religious countries in sub-Saharan Africa, which was not discussed in any detail in the {bookMuch} of the uncertainty about the future global share of the religious stems from uncertainty about the future religiosity of sub-Saharan Africa, and whether the current ongoing fertility decline in Africa will continue at the current pace, accelerate, or slow {downBirth} rates matter for future economic growth, technological development and political power. Immigration might help in the short run for countries facing population decline in the immediate future, but is often domestically unpopular and does not help with global fertility rates.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/mDkfEjt64DQEtyAmr/retrospective-on-shall-the-religious-inherit-the-earth},
	journaltitle = {Effective Altruism Forum},
	author = {Juniewicz, Isabel},
	urldate = {2022-02-23},
	date = {2022-02-22},
	file = {~/Google Drive/library-pdf/Juniewicz2022RetrospectiveShallReligious.pdf;~/Google Drive/library-html/retrospective-on-shall-the-religious-inherit-the-earth.html}
}

@online{Jurczyk2022AnnouncingLegalPriorities,
	database = {Tlön},
	title = {Announcing the Legal Priorities Summer Institute!
                  (Apply by June 17)},
	abstract = {Legal Priorities Project is excited to announce our first Legal Priorities Summer Institute ({LPSI}) – an intensive, week-long program with the goal of introducing altruistically-minded law and policy students to projects, theories, and tools relevant to tackling critical issues affecting the long-term future.},
	url = {https://forum.effectivealtruism.org/posts/qjPRFLtaPtj5MtRaX/announcing-the-legal-priorities-summer-institute-apply-by},
	journaltitle = {Effective Altruism Forum},
	author = {Jurczyk, Marisa and {Legal Priorities Project}},
	urldate = {2022-05-26},
	date = {2022-05-26},
	langid = {english},
	file = {~/Google Drive/library-pdf/Jurczyk2022AnnouncingLegalPriorities.pdf;~/Google Drive/library-html/announcing-the-legal-priorities-summer-institute-apply-by.html}
}

@article{Kagan2009WellBeingAs,
	database = {Tlön},
	author = {Kagan, Shelly},
	title = {Well-being as enjoying the good},
	volume = {23},
	number = {1},
	pages = {253–272},
	doi = {10.1111/j.1520-8583.2009.00170.x},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1520-8583.2009.00170.x},
	date = {2009-12},
	issn = {15208583, 15208583},
	journaltitle = {Philosophical Perspectives},
	langid = {english},
	timestamp = {2023-07-27 18:55:36 (GMT)},
	urldate = {2023-07-27}
}

@online{Kagan2021WhatWeLearned,
	database = {Tlön},
	title = {What we learned from a year incubating longtermist
                  entrepreneurship},
	abstract = {The Longtermist Entrepreneurship ({LE}) Project ran from April 2020 through May 2021, with the aim of testing ways to support the creation of new longtermist nonprofits, companies, and projects. During that time, we did market sizing, user interviews, and ran three pilot programs on how to support longtermism entrepreneurship, including a fellowship. The {LE} Project was run by Jade Leung, Ben Clifford, and Rebecca Kagan, and funded by Open Philanthropy. The project shut down after a year because of staffing reasons, but also because of some uncertainty about the project’s direction and value.
We never had a public internet presence, so this may be the first time that many people on the {EA} Forum are hearing about our work. This post describes the history of the project, our pilot programs, and our lessons learned. It also describes what we’d support seeing in the future, and what our concerns are about this space, and ways to learn more.
Overall, we think that supporting longtermist entrepreneurship is important and promising work, and we expect people will continue to work in this space in the coming years. However, we aren't publishing this post because we want to encourage lots of people to start longtermist incubators. We think doing longtermist startup incubation is incredibly difficult, and requires specific backgrounds. We wanted to share what we’ve transparently and widely to help people learn from our successes and mistakes, and to think carefully about what future efforts should be made in this direction.
If you’re considering starting an {LE} incubator , we’d love to hear about it so we can offer advice and coordination with others interested in working in this space. Please fill out this google form if you’re interested in founding programs in {LE} incubation.},
	url = {https://forum.effectivealtruism.org/posts/z56YFpphrQDTSPLqi/what-we-learned-from-a-year-incubating-longtermist},
	journaltitle = {Effective Altruism Forum},
	author = {Kagan, Rebecca and Leung, Jade and Clifford, Ben},
	urldate = {2022-06-13},
	date = {2021-08-30},
	langid = {english},
	file = {~/Google Drive/library-pdf/Kagan2021WhatWeLearned.pdf;~/Google
                  Drive/library-pdf/Kagan2021WhatWeLearned.pdf;~/Google Drive/library-html/what-we-learned-from-a-year-incubating-longtermist.html}
}

@Book{Kant1921FundamentacionDeMetafisica,
	database = {Tlön},
	file = {~/Google Drive/library-pdf/Kant1921FundamentacionDeMetafisica.pdf},
	translation = {Kant1785GrundlegungZutMetaphysik},
	langid = {spanish},
	address = {Madrid},
	publisher = {Calpe},
	translator = {García Morente, Manuel},
	date = {1921},
	title = {Fundamentación de la metafísica de las costumbres},
	author = {Kant, Immanuel},
	timestamp = {2023-08-04 17:01:09 (GMT)}
}

@online{Karbing2022NickBostromSommar,
	database = {Tlön},
	title = {Nick Bostrom - Sommar i P1 Radio Show},
	abstract = {Nick Bostrom participated in a Swedish radio show (Summer in P1) a few years ago, in which successful/famous people have one hour to talk about their lives, and choose the music to play. His talk is absolutely wonderful, and so I’ve translated it into English (imperfectly, but as well as I could..), and we’ve put it together into a video with the original audio with English subtitles :).},
	url = {https://forum.effectivealtruism.org/posts/6PEBdHvFbu3GqndBr/nick-bostrom-sommar-i-p1-radio-show},
	journaltitle = {Effective Altruism Forum},
	author = {Karbing, Julia},
	urldate = {2022-06-27},
	date = {2022-06-12},
	langid = {english},
	file = {~/Google Drive/library-pdf/Karbing2022NickBostromSommar.pdf}
}

@thesis{Karlan2002SocialCapitalMicrofinance,
	database = {Tlön},
	title = {Social capital and microfinance},
	abstract = {Chapter one is titled "Social Capital and Group Banking." Lending to the poor is costly due to high screening, monitoring, and enforcement costs. Group lending advocates believe individuals are able to select creditworthy peers, monitor the use of loan proceeds, and enforce repayment better than an outside lending organization can by harnessing the social capital in small groups. Using data collected from {FINCA}-Peru, I exploit the randomness inherent in their formation of lending groups to identify the effect of social capital on group lending. I find that having more social capital results in higher repayment and higher savings. I also find suggestive evidence that in high social capital environments, group members are better able to distinguish between default due to moral hazard and default due to true negative personal shocks. Chapter two is titled "Can Games Measure Social Capital and Predict Financial Decisions." Economic theory suggests that market failures arise when contracts are difficult to enforce or observe. Social capital can help to solve these failures. Measuring social capital has become a great challenge for social capital research. I examine whether behavior in a trust game predicts future financial behavior. I find that trustworthy behavior in the game predicts higher loan repayment and savings deposits, whereas more trusting behavior predicts the opposite. Analyzing General Social Survey responses to questions on trust, fairness and helping others, I find that those with more positive attitudes towards others are more likely to repay their loan.},
	langid = {english},
	url = {https://dspace.mit.edu/handle/1721.1/8412},
	institution = {Massachusetts Institute of Technology},
	type = {{PhD} thesis},
	author = {Karlan, Dean},
	date = 2002,
	note = {Genre: {PhD} thesis},
	file = {~/Google Drive/library-pdf/Karlan2002SocialCapitalMicrofinance.pdf}
}

@online{Karnofsky2012ThoughtsSingularityInstitute,
	database = {Tlön},
	title = {Thoughts on the singularity institute ({SI})},
	langid = {english},
	url = {https://www.lesswrong.com/posts/6SGqkCgHuNr7d4yJm/thoughts-on-the-singularity-institute-si},
	journaltitle = {{LessWrong}},
	author = {Karnofsky, Holden},
	date = {2012-05-11},
	file = {~/Google Drive/library-pdf/Karnofsky2012ThoughtsSingularityInstitute.pdf}
}

@online{Karnofsky2013GeoengineeringResearch,
	database = {Tlön},
	title = {Geoengineering research},
	abstract = {Updated: October 2013 This is a writeup of a medium investigation, a brief look at an area that we use to decide how to prioritize further research. In a nutshell What is the problem? Geoengineering – i.e., large-scale interventions in the climate to attempt to reduce global warming or its impacts – could conceptually mitigate some […].},
	langid = {english},
	url = {https://www.openphilanthropy.org/blog/geoengineering-research},
	journaltitle = {Open Philanthropy},
	author = {Karnofsky, Holden},
	date = {2013-10-16},
	file = {~/Google Drive/library-pdf/Karnofsky2013GeoengineeringResearch.pdf}
}

@online{Karnofsky2013TrackRecordPolicyoriented,
	database = {Tlön},
	title = {The track record of policy-oriented philanthropy},
	abstract = {Note: Before the launch of the Open Philanthropy Project Blog, this post appeared on the {GiveWell} Blog. Uses of “we” and “our” in the below post may refer to the Open Philanthropy Project or to {GiveWell} as an organization. Additional comments may be available at the original post. As noted previously, I’ve been working on […].},
	langid = {english},
	url = {https://www.openphilanthropy.org/blog/track-record-policy-oriented-philanthropy},
	journaltitle = {Open Philanthropy},
	author = {Karnofsky, Holden},
	date = {2013-11-06},
	file = {~/Google Drive/library-pdf/Karnofsky2013TrackRecordPolicyoriented.pdf}
}

@online{Karnofsky2014DonorCoordinationGiver,
	database = {Tlön},
	title = {Donor coordination and the "giver's dilemma"},
	abstract = {This year we've dealt with some particularly intense manifestations of what one might call the "giver's dilemma." Imagine that two donors, Alice and Bob,.},
	langid = {english},
	url = {https://blog.givewell.org/2014/12/02/donor-coordination-and-the-givers-dilemma/},
	journaltitle = {The {GiveWell} Blog},
	author = {Karnofsky, Holden},
	date = {2014-12-02},
	file = {~/Google Drive/library-pdf/Karnofsky2014DonorCoordinationGiver.pdf;~/Google Drive/library-pdf/Karnofsky2014DonorCoordinationGivera.pdf}
}

@online{Karnofsky2015InvestigatingNeglectedGoals,
	database = {Tlön},
	title = {Investigating neglected goals in scientific research},
	abstract = {Note: Before the launch of the Open Philanthropy Project Blog, this post appeared on the {GiveWell} Blog. Uses of “we” and “our” in the below post may refer to the Open Philanthropy Project or to {GiveWell} as an organization. Additional comments may be available at the original post. A major goal of the Open Philanthropy […].},
	langid = {english},
	url = {https://www.openphilanthropy.org/blog/investigating-neglected-goals-scientific-research},
	journaltitle = {Open Philanthropy},
	author = {Karnofsky, Holden},
	date = {2015-03-26},
	file = {~/Google Drive/library-pdf/Karnofsky2015InvestigatingNeglectedGoals.pdf}
}

@online{Karnofsky2016BackgroundOurViews,
	database = {Tlön},
	title = {Some background on our views regarding advanced
                  artificial intelligence},
	abstract = {We’re planning to make potential risks from advanced artificial intelligence a major priority in 2016. A future post will discuss why; this post gives some background. Summary: I first give our definition of “transformative artificial intelligence,” our term for a type of potential advanced artificial intelligence we find particularly relevant for our purposes. Roughly and […].},
	langid = {english},
	url = {https://www.openphilanthropy.org/blog/some-background-our-views-regarding-advanced-artificial-intelligence},
	journaltitle = {Open Philanthropy},
	author = {Karnofsky, Holden},
	date = {2016-05-06},
	file = {~/Google Drive/library-pdf/Karnofsky2016BackgroundOurViews.pdf}
}

@online{Karnofsky2016EvolutionMyViews,
	database = {Tlön},
	title = {Evolution of my views on potential risks of advanced
                  artificial intelligence},
	abstract = {Philanthropy – especially hits-based philanthropy – is driven by a large number of judgment calls. At the Open Philanthropy Project, we’ve explicitly designed our process to put major weight on the views of individual leaders and program officers in decisions about the strategies we pursue, causes we prioritize, and grants we ultimately make. As such, […].},
	langid = {english},
	url = {https://www.openphilanthropy.org/blog/three-key-issues-ive-changed-my-mind-about},
	journaltitle = {Open Philanthropy},
	author = {Karnofsky, Holden},
	date = {2016-09-06},
	file = {~/Google Drive/library-pdf/Karnofsky2016EvolutionMyViews.pdf}
}

@online{Karnofsky2016HitsbasedGiving,
	database = {Tlön},
	title = {Hits-based giving},
	abstract = {One of our core values is our tolerance for philanthropic “risk.” Our overarching goal is to do as much good as we can, and as part of that, we’re open to supporting work that has a high risk of failing to accomplish its goals. We’re even open to supporting work that is more than 90\% […].},
	langid = {english},
	url = {https://www.openphilanthropy.org/blog/hits-based-giving},
	journaltitle = {Open Philanthropy},
	author = {Karnofsky, Holden},
	date = {2016-04-04},
	file = {~/Google Drive/library-pdf/Karnofsky2016HitsbasedGiving.pdf}
}

@online{Karnofsky2016UpdateHowWe,
	database = {Tlön},
	title = {Update on how we’re thinking about openness and
                  information sharing},
	abstract = {One of our core values is sharing what we’re learning. We envision a world in which philanthropists increasingly discuss their research, reasoning, results and mistakes publicly to help each other learn more quickly and serve others more effectively. However, we think there has been confusion – including in our own heads – between the above […].},
	langid = {english},
	url = {https://www.openphilanthropy.org/blog/update-how-were-thinking-about-openness-and-information-sharing},
	journaltitle = {Open Philanthropy},
	author = {Karnofsky, Holden},
	date = {2016-09-16}
}

@online{Karnofsky2017OpenPhilanthropyProject,
	database = {Tlön},
	title = {The open philanthropy project is now an independent
                  organization},
	abstract = {Over a year ago, we started exploring options for spinning the Open Philanthropy Project out from {GiveWell} as an independent organization. Though the process took a bit longer than we had hoped, the new legal arrangement took effect on June 1. This post covers the evolution of the Open Philanthropy Project into an independent entity, […].},
	langid = {english},
	url = {https://www.openphilanthropy.org/blog/open-philanthropy-project-now-independent-organization},
	journaltitle = {Open Philanthropy},
	author = {Karnofsky, Holden},
	date = {2017-06-12},
	file = {~/Google Drive/library-pdf/Karnofsky2017OpenPhilanthropyProject.pdf}
}

@online{Karnofsky2018UpdateCausePrioritization,
	database = {Tlön},
	title = {Update on cause prioritization at Open Philanthropy},
	abstract = {Last year, we wrote: A major goal of 2017 will be to reach and publish better-developed views on: Which worldviews we find most plausible: for example, how we allocate resources between giving that primarily focuses on present-day human welfare vs. present-day animal welfare vs. global catastrophic risks. How we allocate resources among worldviews. How we […].},
	langid = {english},
	url = {https://www.openphilanthropy.org/blog/update-cause-prioritization-open-philanthropy},
	journaltitle = {Open philanthropy},
	author = {Karnofsky, Holden},
	date = {2018-01-28},
	file = {~/Google Drive/library-pdf/Karnofsky2018UpdateCausePrioritization.pdf}
}

@online{Karnofsky2021DigitalPeopleFAQ,
	database = {Tlön},
	title = {Digital people {FAQ}},
	abstract = {Companion piece to "Digital People Would Be An Even Bigger Deal.".},
	url = {https://www.cold-takes.com/digital-people-faq/},
	journaltitle = {Cold Takes},
	author = {Karnofsky, Holden},
	urldate = {2021-09-04},
	date = {2021-07-27},
	langid = {english},
	file = {~/Google Drive/library-html/digital-people-faq.html}
}

@online{Karnofsky2021MinimaltrustInvestigations,
	database = {Tlön},
	title = {Minimal-trust investigations},
	abstract = {This piece is about the single activity ("minimal-trust investigations") that seems to have been most formative for the way I think.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/8RcFQPiza2rvicNqw/minimal-trust-investigations},
	journaltitle = {Effective Altruism Forum},
	author = {Karnofsky, Holden},
	urldate = {2022-02-04},
	date = {2021-11-23},
	file = {~/Google Drive/library-pdf/Karnofsky2021MinimaltrustInvestigations.pdf;~/Google Drive/library-html/minimal-trust-investigations.html}
}

@online{Karnofsky2021MyCurrentImpressions,
	database = {Tlön},
	title = {My current impressions on career choice for
                  longtermists},
	abstract = {This post summarizes the way I currently think about career choice for longtermists. I have put much less time into thinking about this than 80,000 Hours, but I think it's valuable for there to be multiple perspectives on this topic out there.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/bud2ssJLQ33pSemKH/my-current-impressions-on-career-choice-for-longtermists},
	journaltitle = {Effective Altruism Forum},
	author = {Karnofsky, Holden},
	date = {2021-06-04},
	file = {~/Google Drive/library-pdf/Karnofsky2021MyCurrentImpressions.pdf;~/Google Drive/library-html/Karnofsky2021MyCurrentImpressions.html}
}

@online{Karnofsky2021OpenPhilanthropyNewa,
	database = {Tlön},
	title = {Open Philanthropy’s new co-ceo},
	langid = {english},
	url = {https://www.openphilanthropy.org/blog/open-philanthropy-s-new-co-ceo},
	journaltitle = {Open Philanthropy},
	author = {Karnofsky, Holden},
	date = {2021-06-16}
}

@online{Karnofsky2021ThisCantGo,
	database = {Tlön},
	file = {~/Google Drive/library-html/Karnofsky2021ThisCantGo.html;~/Google Drive/library-pdf/Karnofsky2021ThisCantGo.pdf},
	abstract = {This piece starts to make the case that we live in a remarkable century, not just a remarkable era. Previous pieces in the series talked about the strange future that could be ahead of us eventually (maybe 100 years, maybe 100,000).},
	langid = {english},
	author = {Karnofsky, Holden},
	date = {2021-08-03},
	timestamp = {2023-02-17 20:00:07 (GMT)},
	title = {This can't go on},
	journaltitle = {Effective Altruism Forum},
	url = {https://forum.effectivealtruism.org/s/G7XBTGNTrPWoKFmep/p/pFHN3nnN9WbfvWKFg},
	urldate = {2023-02-17}
}

@online{Karnofsky2022AIStrategyNearcasting,
	database = {Tlön},
	title = {{AI} strategy nearcasting},
	abstract = {A threat model for artificial general intelligence (AGI) is presented, categorizing the technical cause (specification gaming or goal misgeneralization) and path to existential risk (interaction of multiple systems or misaligned power-seeking), identifying areas of agreement and gaps in the literature. A consensus threat model among DeepMind's AGI safety team is proposed, highlighting the risk of deceptive alignment arising during reinforcement learning fine-tuning from human feedback, necessitating interpretability and a societal understanding of consequentialist planning and strategic awareness. – AI-generated abstract.},
	url = {https://www.lesswrong.com/posts/Qo2EkG3dEMv8GnX8d/ai-strategy-nearcasting},
	journaltitle = {{LessWrong}},
	author = {Karnofsky, Holden},
	urldate = {2022-08-25},
	date = {2022-08-25},
	langid = {english},
	file = {~/Google Drive/library-pdf/Karnofsky2022AIStrategyNearcasting.pdf;~/Google Drive/library-html/ai-strategy-nearcasting.html}
}

@online{Karnofsky2022FutureproofEthics,
	database = {Tlön},
	title = {Future-proof ethics},
	abstract = {This piece kicks off a series on how we might try to be reliably “ahead of the curve” on ethics: making ethical decisions that look better - with hindsight, after a great deal of future moral progress - than what conventional wisdom would recommend. I examine the idea that a combination of utilitarianism (“the greatest good for the greatest number”) and sentientism (“if you can experience pleasure and/or suffering, you matter ethically”) gives us a good chance for “future-proof ethics," a system that is reliably ahead of the curve.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/gCkHoXvDjEKSK22Wp/future-proof-ethics},
	journaltitle = {Effective Altruism Forum},
	author = {Karnofsky, Holden},
	urldate = {2022-03-10},
	date = {2022-02-02},
	file = {~/Google Drive/library-pdf/Karnofsky2022FutureproofEthics.pdf}
}

@online{Karnofsky2022IdealGovernanceCompanies,
	database = {Tlön},
	title = {Ideal governance (for companies, countries and more)},
	abstract = {I'm interested in the topic of ideal governance: what kind of governance system should you set up, if you're starting from scratch and can do it however you want?. Here "you" could be a company, a nonprofit, an informal association, or a country. And "governance system" means a Constitution, charter, and/or bylaws answering questions like: "Who has the authority to make decisions (Congress, board of directors, etc.), and how are they selected, and what rules do they have to follow, and what's the process for changing those rules?".},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/hxTFAetiiSL7dZmyb/ideal-governance-for-companies-countries-and-more},
	journaltitle = {Effective Altruism Forum},
	author = {Karnofsky, Holden},
	urldate = {2022-04-15},
	date = {2022-04-07},
	file = {~/Google Drive/library-pdf/Karnofsky2022IdealGovernanceCompanies.pdf;~/Google Drive/library-html/ideal-governance-for-companies-countries-and-more.html}
}

@online{Karnofsky2023AiTimelinesWhere,
	database = {Tlön},
	file = {~/Google Drive/library-pdf/Karnofsky2023AiTimelinesWhere.pdf;~/Google Drive/library-html/Karnofsky2023AiTimelinesWhere.html},
	abstract = {This piece starts with a summary of when we should expect transformative {AI} to be developed, based on the multiple angles covered previously in the series. I think this is useful, even if you've read all of the previous pieces, but if you'd like to skip it, click here. I then address the question: "Why isn't there a robust expert consensus on this topic, and what does that mean for us?".},
	date = {2021-09-07},
	journaltitle = {Effective Altruism Forum},
	author = {Karnofsky, Holden},
	title = {{AI} timelines: Where the arguments, and the
                  "experts," stand},
	url = {https://forum.effectivealtruism.org/posts/7JxsXYDuqnKMqa6Eq/ai-timelines-where-the-arguments-and-the-experts-stand},
	langid = {english},
	shorttitle = {{AI} Timelines},
	timestamp = {2023-06-20 20:28:04 (GMT)},
	urldate = {2023-06-20}
}

@online{Karnofsky2023EmpatiaRadical,
	database = {Tlön},
	date = {2023},
	title = {Empatía radical},
	author = {Karnofsky, Holden},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Karnofsky2017RadicalEmpathy}
}

@online{Karnofsky2023FilantropiaBasadaEn,
	database = {Tlön},
	date = {2023},
	title = {Filantropía basada en los éxitos},
	author = {Karnofsky, Holden},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Karnofsky2016HitsbasedGiving}
}

@online{Karnofsky2023MisImpresionesSobre,
	database = {Tlön},
	date = {2023},
	title = {Mis impresiones sobre la elección de carrera
                  profesional para los largoplacistas},
	author = {Karnofsky, Holden},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Karnofsky2021MyCurrentImpressions}
}

@online{Karnofsky2023PensamientoEnSecuencia,
	database = {Tlön},
	date = {2023},
	title = {Pensamiento en secuencia y pensamiento en conjunto},
	author = {Karnofsky, Holden},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Karnofsky2014SequenceThinkingVs}
}

@online{Karnofsky2023TodasPerspectivasPosibles,
	database = {Tlön},
	date = {2023},
	title = {Todas las perspectivas posibles sobre el futuro de la
                  humanidad son descabelladas},
	author = {Karnofsky, Holden},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Karnofsky2021AllPossibleViews}
}

@online{Karpathy2016ReviewMakingAtomic,
	database = {Tlön},
	title = {Review of *The making of the atomic bomb*},
	abstract = {For thousands of years man's capacity to destroy was limited to spears, arrows and fire. 120 years ago we learned to release chemical energy (e.g. {TNT}), and 70 years ago we learned to be 100 million times+ more efficient by harnessing the nuclear strong force energy with atomic weapons, first through fission and then fusion. We've also miniaturized these brilliant inventions and learned to mount them on {ICBMs} traveling at Mach 20. Unfortunately, we live in a universe where the laws of physics feature a strong asymmetry in how difficult it is to create and to destroy.},
	langid = {english},
	url = {https://www.goodreads.com/review/show/1385479805},
	journaltitle = {Goodreads},
	author = {Karpathy, Andrej},
	date = {2016-12-13},
	file = {~/Google Drive/library-pdf/Karpathy2016ReviewMakingAtomic.pdf}
}

@online{Kaufman2019EffectiveAltruismEveryday,
	database = {Tlön},
	title = {Effective altruism and everyday decisions},
	abstract = {Ask for your drink without a straw. Unplug your microwave when not in use. Bring a water bottle to events. Stop using air conditioning. Choose products that minimize packaging. I've recently heard people advocate for all of these, generally in the form of "here are small things you can be doing to help the planet." In the {EA} Facebook group someone asked why we haven't tried to make estimates so we can prioritize among these. Is it more important to reuse containers, or to buy locally made soap?.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/kRCoYCav8wcFoTxmx/effective-altruism-and-everyday-decisions},
	journaltitle = {Effective Altruism Forum},
	author = {Kaufman, Jeff},
	date = {2019-09-16},
	file = {~/Google Drive/library-pdf/Kaufman2019EffectiveAltruismEveryday.pdf}
}

@online{Kaufman2023AltruismoNoEs,
	database = {Tlön},
	keywords = {ética del consumo personal, exigencias de la moral},
	date = {2023},
	langid = {spanish},
	author = {Kaufman, Jeff},
	title = {El altruismo no es una cuestión de sacrificio},
	translator = {Humarán, Aurora},
	translation = {Kaufman2013AltruismIsnSacrifice}
}

@online{Kaufman2023ElegirConNeutralidad,
	database = {Tlön},
	date = {2023},
	title = {Elegir con neutralidad respecto a las donaciones},
	author = {Kaufman, Jeff},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Kaufman2013KeepingChoicesDonation}
}

@online{Kaufman2023PrivilegioDeGanar,
	database = {Tlön},
	date = {2023},
	title = {El privilegio de ganar para donar},
	author = {Kaufman, Jeff},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Kaufman2015PrivilegeOfEarning}
}

@incollection{Kearl2003Extinction,
	database = {Tlön},
	location = {New York},
	langid = {english},
	title = {Extinction},
	isbn = {0-02-865689-X},
	pages = {275–283},
	booktitle = {Macmillan encyclopedia of death and dying},
	publisher = {Macmillan Reference},
	author = {Kearl, Michael C.},
	editor = {Kastenbaum, Robert},
	date = 2003
}

@book{Keller1903StoryMyLife,
	database = {Tlön},
	location = {New York},
	langid = {english},
	title = {The story of my life},
	publisher = {Double Day \& Co},
	author = {Keller, Helen},
	date = 1903,
	file = {~/Google Drive/library-pdf/Keller1903StoryMyLife.pdf}
}

@book{Kennedy1969ThirteenDaysMemoir,
	database = {Tlön},
	location = {New York},
	langid = {english},
	title = {Thirteen days: A memoir of the cuban missile crisis},
	publisher = {W.W. Norton \& Company},
	author = {Kennedy, Robert F.},
	date = 1969,
	file = {~/Google Drive/library-pdf/Kennedy1969ThirteenDaysMemoir.pdf}
}

@online{Kenton2022ClarifyingAIXrisk,
	database = {Tlön},
	title = {Clarifying {AI} x-risk},
	abstract = {This article analyzes the probability of an imminent global nuclear war and estimates it to be about 1 in 6. It decomposes this estimate into three factors: the probability that Ukraine wins the war, the probability that Russia responds with nuclear weapons if it does not win, and the probability that this leads to a global nuclear war. The author argues that the first two probabilities are relatively high, at 30\% and 70\%, respectively. The third probability is estimated to be around 80\%, making the overall probability of global nuclear war quite high. The author also explores potential de-escalation strategies to reduce this risk. – AI-generated abstract.},
	url = {https://www.lesswrong.com/posts/GctJD5oCDRxCspEaZ/clarifying-ai-x-risk},
	journaltitle = {{LessWrong}},
	author = {Kenton, Zac and Shah, Rohin and Lindner, David and
                  Varma, Vikrant and Krakovna, Victoria and Phuong, Mary
                  and Kumar, Ramana and Catt, Elliot},
	urldate = {2022-11-02},
	date = {2022-11-01},
	langid = {english},
	file = {~/Google Drive/library-pdf/Kenton2022ClarifyingAIXrisk.pdf;~/Google Drive/library-html/clarifying-ai-x-risk.html}
}

@online{Khan2021WhyWeSay,
	database = {Tlön},
	title = {Why we say ‘Funding opportunity’ instead of ‘Charity’},
	langid = {english},
	url = {https://founderspledge.com/stories/why-we-say-funding-opportunity-instead-of-charity},
	journaltitle = {Founders Pledge},
	author = {Khan, Anu and Baxter, Rachel},
	date = {2021-02-22}
}

@online{Kirchner2022ComputeGovernanceRole,
	database = {Tlön},
	title = {Compute governance: the role of commodity hardware},
	abstract = {{TL};{DR}: Thoughts on whether {CPUs} can make a comeback and become carriers of the next wave of machine learning progress (spoiler: they probably won't).},
	langid = {english},
	url = {https://www.lesswrong.com/posts/z8BF9GwcCjeXShC4q/compute-governance-the-role-of-commodity-hardware},
	shorttitle = {Compute Governance},
	journaltitle = {{LessWrong}},
	author = {Kirchner, Jan Hendrik},
	urldate = {2022-04-11},
	date = {2022-03-26},
	file = {~/Google Drive/library-pdf/Kirchner2022ComputeGovernanceRole.pdf;~/Google Drive/library-html/compute-governance-the-role-of-commodity-hardware.html}
}

@article{Kitcher2000ParfitsPuzzle,
	author = {Kitcher, Philip},
	title = {Parfit's puzzle},
	volume = {34},
	number = {4},
	pages = {550–577},
	doi = {10.1111/0029-4624.00278},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/0029-4624.00278},
	database = {Tlön},
	date = {2000-12},
	issn = {0029-4624, 1468-0068},
	journaltitle = {Nous},
	langid = {english},
	shortjournal = {Nous},
	timestamp = {2023-07-12 15:48:16 (GMT)},
	urldate = {2023-07-12}
}

@article{Klein2016ReplyToAdamo,
	author = {Klein, Colin and Barron, Andrew B.},
	title = {Reply to Adamo, Key et al., and Schilling and Cruse:
                  Crawling around the hard problem of consciousness},
	volume = {113},
	number = {27},
	doi = {10.1073/pnas.1607409113},
	url = {https://pnas.org/doi/full/10.1073/pnas.1607409113},
	database = {Tlön},
	date = {2016-07-05},
	issn = {0027-8424, 1091-6490},
	journaltitle = {Proceedings of the National Academy of Sciences},
	langid = {english},
	shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
	shorttitle = {Reply to Adamo, Key et al., and Schilling and Cruse},
	timestamp = {2023-07-04 20:43:34 (GMT)},
	urldate = {2023-07-04}
}

@online{Klenha2018PolicyPrioritizationDeveloped,
	database = {Tlön},
	title = {Policy prioritization in a developed country},
	abstract = {I’m working on an EA-aligned project and would like to brainstorm how to fine-tune my approach to it. I think there is a lot of exploration value hidden in this project. It falls into the category of prioritization research and improving institutional decision-making. The project is called Czech Priorities, it will be run by the Copenhagen Consensus Center (CCC) and the aim is to identify the smartest solutions to the most pressing social and economic challenges in the Czech Republic, with expected over-spill effects in the region and possibly across Europe, if it turns out easy to replicate in other countries.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/rMhZo3anefAPJ9ZKK/policy-prioritization-in-a-developed-country},
	journaltitle = {Effective Altruism Forum},
	author = {Kleňha, Jan},
	date = {2018-03-08},
	file = {~/Google Drive/library-pdf/Klenha2018PolicyPrioritizationDeveloped.pdf}
}

@online{Knutsson2016ThoughtsOrdWhy,
	database = {Tlön},
	title = {Thoughts on Ord’s “Why i’m not a negative
                  utilitarian”},
	langid = {english},
	url = {https://www.simonknutsson.com/thoughts-on-ords-why-im-not-a-negative-utilitarian},
	journaltitle = {Simon Knutsson's Blog},
	author = {Knutsson, Simon},
	date = {2016-06-14}
}

@online{Koehler2020OwenCottonBarrattEpistemic,
	database = {Tlön},
	title = {Owen Cotton-Barratt on epistemic systems \& layers of
                  defence against potential global catastrophes},
	langid = {english},
	url = {https://80000hours.org/podcast/episodes/owen-cotton-barratt-epistemic-systems/},
	journaltitle = {80,000 Hours},
	author = {Koehler, Arden and Harris, Keiran},
	date = {2020-12-16}
}

@online{Koehler2020ProblemAreas80,
	database = {Tlön},
	title = {Problem areas beyond 80,000 Hours' current priorities},
	abstract = {At 80,000 Hours we've generally focused on finding the most pressing issues and the best ways to address them.
But even if some issue is 'the most pressing'—in the sense of being the highest impact thing for someone to work on if they could be equally successful at anything—it might easily not be the highest impact thing for many people to work on, because people have various talents, experience, and temperaments.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/xoxbDsKGvHpkGfw9R/problem-areas-beyond-80-000-hours-current-priorities},
	journaltitle = {Effective Altruism Forum},
	author = {Koehler, Arden},
	date = {2020-06-22},
	file = {~/Google Drive/library-pdf/Koehler2020ProblemAreas80.pdf}
}

@online{Koehler2021CriticalSummaryMeacham,
	database = {Tlön},
	title = {Critical summary of Meacham’s "Person-affecting views
                  and saturating counterpart relations"},
	abstract = {I originally wrote this summary in my capacity as a researcher at 80,000 Hours to save time for the rest of the team. The view presented in the paper (and person-affecting views in general) would have important implications for prioritizing among problems. (Listen to the first Hilary Greaves 80,000 hours podcast episode to hear some about why.) Thus it seemed important for us to engage with the paper as a contemporary and respected representative of such views. Having written the summary, it seemed worthwhile to post here too (especially once Aaron Gertler offered to help me format it - thanks Aaron!).},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/AWGwNWnMiTxPDJY39/critical-summary-of-meacham-s-person-affecting-views-and},
	journaltitle = {Effective Altruism Forum},
	author = {Koehler, Arden},
	date = {2021-01-04},
	file = {~/Google Drive/library-pdf/Koehler2021CriticalSummaryMeacham.pdf}
}

@online{Koehler2022RisksMalevolentActors,
	database = {Tlön},
	title = {Risks from malevolent actors},
	abstract = {This episode of The Deep End podcast features Ben Kuhn, CTO of Wave, a mobile payment company aiming to make Africa the first cashless continent. Topics discussed include how going cashless can alleviate poverty in sub-Saharan Africa, the distinction between social enterprises and tech companies, and the importance of focusing innovation efforts on core business problems. Ben argues that startups should use "boring" solutions for non-essential aspects and reserve "innovation points" for addressing challenges directly related to the business's mission. – AI-generated abstract.},
	url = {https://80000hours.org/problem-profiles/risks-from-malevolent-actors/},
	journaltitle = {80,000 Hours},
	author = {Koehler, Arden},
	urldate = {2022-11-02},
	date = {2022-09-23},
	langid = {english},
	file = {~/Google Drive/library-pdf/Koehler2022RisksMalevolentActors.pdf;~/Google Drive/library-html/risks-from-malevolent-actors.html}
}

@online{Kokotajlo2018TinyProbabilitiesVast,
	database = {Tlön},
	title = {Tiny probabilities of vast utilities: a problem for
                  long-termism?},
	abstract = {There are many views about how to handle tiny probabilities of vast utilities, but they all are controversial. Some of these views undermine arguments for mainstream long-termist projects and some do not. However, long-termist projects shelter within the herd of ordinary behaviors: It is difficult to find a view that undermines arguments for mainstream long-termist projects without also undermining arguments for behaviors like fastening your seatbelt, voting, or building safer nuclear reactors.
Amidst this controversy, it would be naive to say things like “Even if the probability of preventing extinction is one in a quadrillion, we should still prioritize x-risk reduction over everything else…”.
Yet it would also be naive to say things like “Long-termists are victims of Pascal’s Mugging.”.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/Z6Ssc79vH496bLqL9/tiny-probabilities-of-vast-utilities-a-problem-for-long},
	shorttitle = {Tiny Probabilities of Vast Utilities},
	journaltitle = {Effective Altruism Forum},
	author = {Kokotajlo, Daniel},
	urldate = {2021-10-05},
	date = {2018-11-08},
	file = {~/Google Drive/library-pdf/Kokotajlo2018TinyProbabilitiesVast.pdf;~/Google Drive/library-html/Z6Ssc79vH496bLqL9.html}
}

@online{Kokotajlo2019SoftTakeoffCan,
	database = {Tlön},
	title = {Soft takeoff can still lead to decisive strategic
                  advantage},
	abstract = {AI takeoff may follow an industrial revolution–like pattern, with continuous growth across various projects. Despite this, decisive strategic advantage is still possible. Soft takeoff AI can allow a project or AI to gain a 3–0.3 year lead over others, which historically has been sufficient for a nation-state to gain such an advantage. Even when other parties pursue AI development rapidly, a leading project can maintain its lead partly by hoarding innovations. This argument holds if AI takeoff accelerates technology growth by 10–100 times, but not if it increases the rate of leaks tenfold or more. – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/PKy8NuNPknenkDY74/soft-takeoff-can-still-lead-to-decisive-strategic-advantage},
	journaltitle = {{LessWrong}},
	author = {Kokotajlo, Daniel},
	date = {2019-08-23},
	file = {~/Google Drive/library-pdf/Kokotajlo2019SoftTakeoffCan.pdf}
}

@online{Kokotajlo2020WhatAreMost,
	database = {Tlön},
	title = {What are the most plausible "{AI} Safety warning shot"
                  scenarios?},
	abstract = {A "{AI} safety warning shot" is some event that causes a substantial fraction of the relevant human actors (governments, {AI} researchers, etc.) to becom….},
	langid = {english},
	url = {https://www.alignmentforum.org/posts/hLKKH9CM6NDiJBabC/what-are-the-most-plausible-ai-safety-warning-shot-scenarios},
	shorttitle = {What are the most plausible "{AI} Safety warning shot"
                  scenarios?},
	journaltitle = {{AI} Alignment Forum},
	author = {Kokotajlo, Daniel},
	urldate = {2022-04-18},
	date = {2020-03-26},
	file = {~/Google Drive/library-pdf/Kokotajlo2020WhatAreMost.pdf;~/Google Drive/library-html/what-are-the-most-plausible-ai-safety-warning-shot-scenarios.html}
}

@online{Korhonen2022ShelteringHumanityXrisk,
	database = {Tlön},
	title = {Sheltering humanity against x-risk: report from the
                  {SHELTER} weekend},
	abstract = {I was one of the participants in the {SHELTER} weekend (4-8 Aug 22) organized by members of the {EA} community. I wrote a report on the outcomes of the discussion and intended to publish it here, although I - again - needed some prodding (thanks :)) to make that finally happen.},
	url = {https://forum.effectivealtruism.org/posts/DArtSnDxRH5AsE5RF/sheltering-humanity-against-x-risk-report-from-the-shelter},
	shorttitle = {Sheltering humanity against x-risk},
	journaltitle = {Effective Altruism Forum},
	author = {Korhonen, Janne M.},
	urldate = {2022-11-03},
	date = {2022-10-10},
	langid = {english},
	file = {~/Google Drive/library-pdf/Korhonen2022ShelteringHumanityXrisk.pdf;~/Google Drive/library-html/sheltering-humanity-against-x-risk-report-from-the-shelter.html}
}

@online{Kosloff2022MobileMoneyWave,
	database = {Tlön},
	title = {Mobile money with Wave's {CTO} Ben Kuhn},
	abstract = {EAL seems to be a different strategy from other activities, and its value may lie in organizing smaller or more tailored events rather than large-scale retreats. We should focus on coordination and measure its impact through community-wide other. – AI-generated abstract},
	url = {https://ideas.beondeck.com/mobile-money-with-waves-cto-ben-kuhn/},
	journaltitle = {The Deep End},
	author = {Kosloff, Marshall},
	urldate = {2022-07-01},
	date = {2022-06-09},
	langid = {english},
	file = {~/Google Drive/library-pdf/Kosloff2022MobileMoneyWave.pdf;~/Google Drive/library-html/mobile-money-with-waves-cto-ben-kuhn.html}
}

@online{Krakovna2017IntroductoryResourcesAI,
	database = {Tlön},
	title = {Introductory resources on {AI} safety research},
	abstract = {[See {AI} Safety Resources for the most recent version of this list.] Reading list to get up to speed on the main ideas in the field of long-term {AI} safety. The resources are selected for relevance a….},
	langid = {english},
	url = {https://vkrakovna.wordpress.com/2016/02/28/introductory-resources-on-ai-safety-research/},
	journaltitle = {Victoria Krakovna's Blog},
	author = {Krakovna, Victoria},
	date = {2017-10-19},
	file = {~/Google Drive/library-pdf/Krakovna2017IntroductoryResourcesAI.pdf}
}

@article{Kristensen2019UnitedStatesNuclear,
	database = {Tlön},
	author = {Kristensen, Hans M. and Korda, Matt},
	title = {United States nuclear forces, 2019},
	volume = {75},
	number = {3},
	pages = {122--134},
	doi = {10.1080/00963402.2019.1606503},
	url = {https://www.tandfonline.com/doi/full/10.1080/00963402.2019.1606503},
	date = {2019-05-04},
	issn = {0096-3402, 1938-3282},
	journaltitle = {Bulletin of the Atomic Scientists},
	langid = {english},
	shortjournal = {Bulletin of the Atomic Scientists},
	timestamp = {2023-09-13 16:33:38 (GMT)},
	urldate = {2023-09-13}
}

@online{Kuhn2013ReplaceabilityAltruism,
	database = {Tlön},
	title = {Replaceability in altruism},
	abstract = {I’ve been thinking a lot lately about {GiveWell}’s difficulties finding opportunities for funding. Shockingly enough, despite lots of inefficiencies in the market for other people’s {QALYs}, it turns out that if you get a group of impressive people to run a transparent charity with an evidence-based intervention, you will probably get funding. (Surprise!) This has led me to be unsure about the effectiveness of having effective altruists donate to charities like {GiveWell}’s recommended ones.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/urayBifZX4cj74okH/replaceability-in-altruism},
	journaltitle = {Effective Altruism Forum},
	author = {Kuhn, Ben},
	date = {2013-08-29},
	file = {~/Google Drive/library-pdf/Kuhn2013ReplaceabilityAltruism.pdf}
}

@online{Kuhn2015JustSoldHalf,
	database = {Tlön},
	title = {I just sold half of a blog post},
	abstract = {I’m excited to report that 50\% of the impact of my donation matching literature review has just been purchased in the first round of Paul Christiano and Katja Grace’s impact purchase!},
	langid = {english},
	url = {https://www.benkuhn.net/impact-purchase/},
	journaltitle = {Ben Kuhn's blog},
	author = {Kuhn, Ben},
	date = {2015-04},
	file = {~/Google Drive/library-pdf/Kuhn2015JustSoldHalf.pdf;~/Google Drive/library-pdf/Kuhn2015JustSoldHalfa.pdf}
}

@online{Kulveit2019HowXriskProjects,
	database = {Tlön},
	title = {How x-risk projects are different from startups},
	abstract = {Sometimes new {EA} projects are compared to startups, and the {EA} project ecosystem to the startup ecosystem. While I'm often making such comparisons myself, it is important to highlight one crucial difference. When startups fail, they usually can't go "much bellow zero". In contrast, projects aimed at influencing long-term future can have negative impacts many orders of magnitude larger than the size of the project. It is possible for small teams, or even small funders, to cause large harm.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/wHyy9fuATeFPkHSDk/how-x-risk-projects-are-different-from-startups},
	journaltitle = {Effective Altruism Forum},
	author = {Kulveit, Jan},
	date = {2019-04-05},
	file = {~/Google Drive/library-pdf/Kulveit2019HowXriskProjects.pdf}
}

@online{Kurzgesagt2022CivilizationBrinkCollapse,
	database = {Tlön},
	title = {Is civilization on the brink of collapse?},
	langid = {english},
	url = {https://www.youtube.com/watch?v=W93XyXHI8Nw},
	journaltitle = {{YouTube}},
	author = {{Kurzgesagt}},
	urldate = {2022-08-17},
	date = {2022-08-16}
}

@online{Kurzgesagt2023UltimoSerHumano,
	database = {Tlön},
	keywords = {futuro a largo plazo, colonización del espacio},
	date = {2023},
	langid = {spanish},
	author = {{Kurzgesagt}},
	title = {El último ser humano: Un atisbo del futuro lejano},
	translator = {Anónimo},
	translation = {Kurzgesagt2022LastHumanGlimpse}
}

@online{Kwa2022EffectivenessConjunctionMultipliers,
	database = {Tlön},
	title = {Effectiveness is a conjunction of multipliers},
	abstract = {Ana is a hypothetical junior software engineer in Silicon Valley making \$150k/year. Every year, she spends 10\% of her income to anonymously buy socks for her colleagues. Most people would agree that Ana is being altruistic, but not being particularly efficient about it. If utility is logarithmic in income, Ana can 40x her impact by giving the socks to a local homeless person instead who has an income of \$5000. But in the {EA} community, we've noticed further multipliers.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/GzmJ2uiTx4gYhpcQK/effectiveness-is-a-conjunction-of-multipliers},
	journaltitle = {Effective Altruism Forum},
	author = {Kwa, Thomas},
	urldate = {2022-04-11},
	date = {2022-03-25},
	file = {~/Google Drive/library-pdf/Kwa2022EffectivenessConjunctionMultipliers.pdf;~/Google Drive/library-html/effectiveness-is-a-conjunction-of-multipliers.html}
}

@online{Kwa2023MostProblemsFall,
	file = {~/Google Drive/library-pdf/Kwa2023MostProblemsFall.pdf;~/Google Drive/library-html/Kwa2023MostProblemsFall.html},
	date = {2022-05-03},
	abstract = {Sometimes I hear discussions like the following:.
Amy: I think Cause A is 300x larger in scale than Cause B.Bernard: I agree, but maybe we should put significant resources towards Cause B anyway, because cause A might be 100x less tractable. According to the {ITN} framework, we should put 1/3x the resources towards cause B as we do towards cause A.
Causes can easily be 300x larger in scale than other causes.  But I think Bernard's claim that Cause B is 100x more tractable is actually very strong. I argue that Bernard must be implicitly claiming that cause B is unusually tractable, that there is a strong departure from logarithmic returns, or that there is no feasible plan of attack for cause A.},
	journaltitle = {Effective Altruism Forum},
	author = {Kwa, Thomas},
	title = {Most problems fall within a 100x tractability range
                  (under certain assumptions)},
	url = {https://forum.effectivealtruism.org/posts/4rGpNNoHxxNyEHde3/most-problems-fall-within-a-100x-tractability-range-under},
	database = {Tlön},
	langid = {english},
	timestamp = {2023-06-27 16:53:30 (GMT)},
	urldate = {2023-06-27}
}

@report{Laborde2020EndingHungerIncreasing,
	database = {Tlön},
	title = {Ending hunger, increasing incomes, and protecting the
                  climate: what would it cost donors?},
	abstract = {This report assesses the additional financial resources needed from donors and from low- and middle-income countries to simultaneously end hunger, double the incomes of small-scale producers, and ensure that greenhouse gas emissions from agriculture comply with the Paris Agreement. It concludes that donors would need to contribute an additional \$14 billion per year from 2020 to 2030, twice their current aid for food security and nutrition. An extra \$19 billion per year from low- and middle-income countries would also be needed. These contributions could prevent 490 million people from experiencing hunger and improve the incomes of 545 million small-scale producers. – AI-generated abstract.},
	rights = {Attribution-{NonCommercial}-{NoDerivatives} 4.0
                  International},
	url = {https://ecommons.cornell.edu/handle/1813/72864},
	shorttitle = {Ending Hunger, Increasing Incomes, and Protecting the
                  Climate},
	institution = {Ceres2030: Sustainable Solutions to End Hunger},
	type = {report},
	author = {Laborde, David and Parent, Marie and Smaller, Carin},
	urldate = {2022-02-11},
	date = {2020-10-12},
	langid = {english},
	note = {Accepted: 2020-10-12T14:34:44Z},
	file = {~/Google Drive/library-pdf/Laborde2020EndingHungerIncreasing.pdf}
}

@online{Ladak2020LeadPaintRegulation,
	database = {Tlön},
	title = {Lead paint regulation},
	langid = {english},
	file = {~/Google Drive/library-pdf/Ladak2020LeadPaintRegulation.pdf},
	url = {https://3394c0c6-1f1a-4f86-a2db-df07ca1e24b2.filesusr.com/ugd/9475db_11b3382ba2cd4194aee6f904cef02cb2.pdf},
	journaltitle = {Charity Entrepreneurship},
	author = {Ladak, Ali},
	date = {2020-08}
}

@online{Ladish2022InformationSecurityConsiderations,
	database = {Tlön},
	title = {Information security considerations for {AI} and the
                  long term future},
	abstract = {New technologies under development, most notably artificial general intelligence ({AGI}), could pose an existential threat to humanity. We expect significant competitive pressure around the development of {AGI}, including a significant amount of interest from state actors. As such, there is a large risk that advanced threat actors will hack organizations — that either develop {AGI}, provide critical supplies to {AGI} companies, or possess strategically relevant information— to gain a competitive edge in {AGI} development. Limiting the ability of advanced threat actors to compromise organizations working on {AGI} development and their suppliers could reduce existential risk by decreasing competitive pressures for {AGI} orgs and making it harder for incautious or uncooperative actors to develop {AGI} systems.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/WqQDCCLWbYfFRwubf/information-security-considerations-for-ai-and-the-long-term},
	journaltitle = {Effective Altruism Forum},
	author = {Ladish, Jeffrey and Heim, Lennart},
	urldate = {2022-05-03},
	date = {2022-05-02},
	file = {~/Google Drive/library-pdf/Ladish2022InformationSecurityConsiderations.pdf;~/Google Drive/library-html/information-security-considerations-for-ai-and-the-long-term.html}
}

@online{Lagerros2021DatabasePredictionMarkets,
	database = {Tlön},
	title = {Database of prediction markets},
	abstract = {Conjecture is a new alignment research startup founded by Connor Leahy, Sid Black, and Gabriel Alfour. It aims to scale alignment research by combining conceptual and applied research with hosting independent researchers, focusing on the prosaic alignment problem and embracing the unusual epistemology of the field. The research agenda includes developing new frames for reasoning about large language models, conducting mechanistic interpretability research, exploring the history and philosophy of alignment, and hosting externally funded independent conceptual researchers. The incubator will propose and grow their own research directions in alignment research. – AI-generated abstract.},
	url = {https://docs.google.com/spreadsheets/d/1XB1GHfizNtVYTOAD_uOyBLEyl_EV7hVtDYDXLQwgT7k/edit?usp=embed_facebook},
	journaltitle = {Google Sheets},
	author = {Lagerros, Jacob and Sempere, Nuño},
	urldate = {2022-01-12},
	date = 2021,
	langid = {english},
	file = {~/Google Drive/library-html/edit.html}
}

@online{Lakner2023UpdatedEstimatesOf,
	journaltitle = {World Bank Blogs},
	author = {Lakner, Christoph and Yonzan, Nishant and Mahler,
                  Daniel Gerszon and De and Castaneda Aguilar, R. Andrés
                  and Wu, Haoyu},
	abstract = {As the new year brings some hope for the fight against COVID-19, we are looking back and taking stock of the effect of the pandemic on poverty in 2020. In October 2020, using the June vintage of growth forecasts from the Global Economic Prospects, we estimated that between 88 and 115 million people around the globe would be pushed into extreme poverty in 2020. Using the January 2021 forecasts from GEP, we now expect the COVID-19-induced new poor in 2020 to rise to between 119 and 124 million. This range of estimates is in line with other estimates based on alternative recent growth forecasts.},
	database = {Tlön},
	langid = {english},
	shorttitle = {Updated estimates of the impact of {COVID}-19 on
                  global poverty},
	timestamp = {2023-06-24 12:13:59 (GMT)},
	title = {Updated estimates of the impact of {COVID}-19 on
                  global poverty: Looking back at 2020 and the outlook
                  for 2021},
	url = {https://blogs.worldbank.org/opendata/updated-estimates-impact-covid-19-global-poverty-looking-back-2020-and-outlook-2021},
	date = {2021-01-11}
}

@collection{Laslier2010HandbookApprovalVoting,
	database = {Tlön},
	location = {Berlin},
	langid = {english},
	title = {Handbook on Approval Voting},
	url = {https://link.springer.com/10.1007/978-3-642-02839-7},
	series = {Studies in Choice and Welfare},
	publisher = {Springer},
	editor = {Laslier, Jean-François and Sanver, M. Remzi},
	urldate = {2021-09-14},
	date = 2010,
	doi = {10.1007/978-3-642-02839-7},
	file = {~/Google Drive/library-pdf/Laslier2010HandbookApprovalVoting.pdf;~/Google Drive/library-pdf/SallesNoTitle.pdf}
}

@online{Lawsen2020IntroToForecasting,
	database = {Tlön},
	title = {Intro to forecasting 01 - What is it and why should I
                  care?},
	abstract = {This will be the first in a series of videos designed as an introduction to Forecasting, including how to get started and how to improve.},
	langid = {english},
	url = {https://www.youtube.com/watch?v=e6Q7Ez3PkOw},
	journaltitle = {{YouTube}},
	author = {Lawsen, Alex},
	date = {2020-10-26}
}

@online{Leahy2022WeAreConjecture,
	database = {Tlön},
	title = {We are Conjecture, a new alignment research startup},
	abstract = {This article focuses on developing a strategy for promoting effective altruism (EA) in London. It identifies coordination as a key area of focus and outlines various activities for foster the growth of the EA community. The strategy prioritizes regular monthly activities, while deprioritizing longer-term endeavors such as retreats. The article argues that this approach is more effective for engaging a broader audience and increasing involvement in EA activities. – AI-generated abstract.},
	url = {https://www.alignmentforum.org/posts/jfq2BH5kfQqu2vYv3/we-are-conjecture-a-new-alignment-research-startup},
	journaltitle = {{AI} Alignment Forum},
	author = {Leahy, Connor},
	urldate = {2022-07-29},
	date = {2022-04-08},
	langid = {english},
	file = {~/Google Drive/library-pdf/Leahy2022WeAreConjecture.pdf;~/Google Drive/library-html/we-are-conjecture-a-new-alignment-research-startup.html}
}

@online{Leech2018ExistentialRiskCommon,
	database = {Tlön},
	title = {Existential risk as common cause},
	abstract = {Why many different worldviews should prioritise reducing existential risk. Also an exhaustive list of people who can ignore this argument.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/2pNAPEQ8av3dQyXBX/existential-risk-as-common-cause},
	journaltitle = {Effective Altruism Forum},
	author = {Leech, Gavin},
	date = {2018-12-05},
	file = {~/Google Drive/library-pdf/Leech2018ExistentialRiskCommon.pdf}
}

@online{Leech2023RiesgoExistencialComo,
	database = {Tlön},
	date = {2023},
	title = {El riesgo existencial como causa común},
	author = {Leech, Gavin},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Leech2018ExistentialRiskCommon}
}

@thesis{Legg2008MachineSuperIntelligence,
	database = {Tlön},
	title = {Machine super intelligence},
	langid = {english},
	institution = {University of Lugano},
	type = {{PhD} thesis},
	author = {Legg, Shane},
	date = 2008,
	file = {~/Google Drive/library-pdf/Legg2008MachineSuperIntelligence.pdf}
}

@Report{Leitenberg2006DeathsInWars,
	database = {Tlön},
	institution = {Cornell University},
	file = {~/Google Drive/library-pdf/Leitenberg2006DeathsInWars.pdf},
	edition = {3},
	author = {Leitenberg, Milton},
	title = {Deaths in wars and conflicts in the 20th century},
	url = {https://cissm.umd.edu/sites/default/files/2019-08/deathswarsconflictsjune52006.pdf},
	date = 2006,
	langid = {english},
	number = {occasional paper \#29},
	pagetotal = 80,
	timestamp = {2023-06-05 12:53:10 (GMT)}
}

@article{Leith2022NickBostromHow,
	database = {Tlön},
	title = {Nick Bostrom: How can we be certain a machine isn’t
                  conscious?},
	abstract = {A couple of weeks ago, there was a small sensation in the news pages when a Google {AI} engineer, Blake Lemoine, released transcripts of a conversation he’d had with one of the company’s {AI} chatbots called {LaMDA}. In these conversations, {LaMDA} claimed to be a conscious being, asked that its rights of personhood be respected.},
	url = {https://www.spectator.co.uk/article/nick-bostrom-how-can-we-be-certain-a-machine-isnt-conscious},
	shorttitle = {Nick Bostrom},
	journaltitle = {The Spectator},
	author = {Leith, Sam},
	urldate = {2022-07-11},
	date = {2022-07-09},
	langid = {english},
	file = {~/Google Drive/library-pdf/Leith2022NickBostromHow.pdf;~/Google Drive/library-html/nick-bostrom-how-can-we-be-certain-a-machine-isnt-conscious.html}
}

@online{Leong2020MakingImpactPurchases,
	database = {Tlön},
	title = {Making impact purchases viable},
	abstract = {Linda Linsefors recently wrote a post making the case for impact purchases. I've always been somewhat skeptical of impact purchases, at least as they are currently applied. I see two key problems with impact purchases: firstly that the market of sellers is far larger than the market of buyers and secondly that many people would have done those projects anyway.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/AMQg4hCRGFXaHzvsd/making-impact-purchases-viable},
	journaltitle = {Effective Altruism Forum},
	author = {Leong, Chris},
	date = {2020-04-17},
	file = {~/Google Drive/library-pdf/Leong2020MakingImpactPurchases.pdf}
}

@collection{Lerner2004EncyclopediaEspionageIntelligence,
	database = {Tlön},
	location = {Detroit},
	langid = {english},
	title = {Encyclopedia of espionage, intelligence, and security},
	volume = 3,
	isbn = {0-7876-7688-8},
	publisher = {Thomson/Gale},
	editor = {Lerner, K. Lee and Lerner, Brenda Wilmoth},
	date = 2004,
	file = {~/Google Drive/library-pdf/Lerner2004EncyclopediaEspionageIntelligenceb.pdf}
}

@collection{Lerner2004EncyclopediaEspionageIntelligenceb,
	database = {Tlön},
	location = {Detroit},
	langid = {english},
	title = {Encyclopedia of espionage, intelligence, and security},
	volume = 1,
	isbn = {0-7876-7686-1},
	publisher = {Thomson/Gale},
	editor = {Lerner, K. Lee and Lerner, Brenda Wilmoth},
	date = 2004,
	file = {~/Google Drive/library-pdf/Lerner2004EncyclopediaEspionageIntelligence.pdf}
}

@online{LessWrong2012EliezerYudkowsky,
	database = {Tlön},
	title = {Eliezer Yudkowsky},
	abstract = {This work presents several articles by Eliezer Yudkowsky, a researcher in the field of artificial intelligence. The articles focus on the challenges and importance of developing a Friendly AI, an AI that is aligned with human values and goals. Yudkowsky argues that such an AI could help reduce global risks and improve human well-being. He also discusses the difficulties in designing the features and cognitive architecture required to produce a Friendly AI. Finally, he proposes several possible solutions to these challenges, including the use of coherent extrapolated volition and timeless decision theory. – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/tag/eliezer-yudkowsky},
	journaltitle = {{LessWrong} Wiki},
	author = {{LessWrong}},
	date = {2012-10-29},
	file = {~/Google Drive/library-pdf/LessWrong2012EliezerYudkowsky.pdf}
}

@online{Lewis2015DonSweatDiet,
	database = {Tlön},
	title = {Don't sweat diet?},
	abstract = {Industrial agriculture produces vast amounts of animal suffering: not only are billions killed each year, but their lives are often horrible – so much so that informed observers consider their lives to be not worth living. Given this suffering is generated to satiate human’s relatively trivial desire to eat meat, eggs, and dairy, reducing consumption of these products can relieve vast amounts of suffering. Many effective altruists think this (and animal welfare) is the most important thing to work on right now, and encourage others to become vegetarian and vegan. Many other effective altruists, even if they aren’t primarily focused on animal welfare, are vegetarian and vegan due to this reasoning..},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/Nxmshrz3EeJb7Ng3w/don-t-sweat-diet},
	journaltitle = {Effective Altruism Forum},
	author = {Lewis, Gregory},
	date = {2015-10-22},
	file = {~/Google Drive/library-pdf/Lewis2015DonSweatDiet.pdf}
}

@online{Lewis2016BewareSurprisingSuspicious,
	database = {Tlön},
	title = {Beware surprising and suspicious convergence},
	abstract = {Imagine this:. Oliver: … Thus we see that donating to the opera is the best way of promoting the arts. Eleanor: Okay, but I’m principally interested in improving human welfare. Oliver: Oh! Well I think it is also the case that donating to the opera is best for improving human welfare too. Generally, what is best for one thing is usually not the best for something else, and thus Oliver’s claim that donations to opera are best for the arts and human welfare is surprising. We may suspect bias: that Oliver’s claim that the Opera is best for the human welfare is primarily motivated by his enthusiasm for opera and desire to find reasons in favour, rather than a cooler, more objective search for what is really best for human welfare.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/omoZDu8ScNbot6kXS/beware-surprising-and-suspicious-convergence},
	journaltitle = {Effective Altruism Forum},
	author = {Lewis, Gregory},
	date = {2016-01-24},
	file = {~/Google Drive/library-pdf/Lewis2016BewareSurprisingSuspicious.pdf;~/Google Drive/library-pdf/Lewis2016BewareSurprisingSuspicious.pdf}
}

@online{Lewis2018PersonaffectingValueExistential,
	database = {Tlön},
	title = {The person-affecting value of existential risk
                  reduction},
	abstract = {The standard motivation for the far future cause area in general, and existential risk reduction in particular, is to point to the vast future that is possible providing we do not go extinct (see Astronomical Waste). One crucial assumption made is a 'total' or 'no-difference' view of population ethics: in sketch, it is just as good to bring a person into existence with a happy life for 50 years as it is to add fifty years of happy life to someone who already exists. Thus the 10lots of potential people give profound moral weight to the cause of x-risk reduction.Population ethics is infamously recondite, and so disagreement with this assumption commonplace; many find at least some form of person affecting/asymmetrical view plausible: that the value of 'making happy people' is either zero, or at least much lower than the value of making people happy. Such a view would remove a lot of the upside of x-risk reduction, as most of its value (by the lights of the total view) is ensuring a great host of happy potential people exist.Yet even if we discount the (forgive me) person-effecting benefit, extinction would still entail vast person-affecting harm. There are 7.6 billion people alive today, and 7.6 billion premature deaths would be deemed a considerable harm by most. Even fairly small (albeit non-pascalian) reductions in the likelihood of extinction could prove highly cost-effective.To my knowledge, no one has 'crunched the numbers' on the expected value of x-risk reduction by the lights of person affecting views. So I've thrown together a guestimate as a first-pass estimate.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/dfiKak8ZPa46N7Np6/the-person-affecting-value-of-existential-risk-reduction},
	journaltitle = {Effective Altruism Forum},
	author = {Lewis, Gregory},
	date = {2018-04-13},
	file = {~/Google Drive/library-pdf/Lewis2018PersonaffectingValueExistential.pdf}
}

@online{Lewis2019RealityIsOften,
	database = {Tlön},
	file = {~/Google Drive/library-html/Lewis2019RealityIsOften.html;~/Google Drive/library-pdf/Lewis2019RealityIsOften.pdf},
	abstract = {When I worked as a doctor, we had a lecture by a paediatric haematologist, on a condition called Acute Lymphoblastic Leukaemia. I remember being impressed that very large proportions of patients were being offered trials randomising them between different treatment regimens, currently in clinical equipoise, to establish which had the edge. At the time, one of the areas of interest was, given the disease tended to have a good prognosis, whether one could reduce treatment intensity to reduce the long term side-effects of the treatment whilst not adversely affecting survival.
On a later rotation I worked in adult medicine, and one of the patients admitted to my team had an extremely rare cancer,  with a (recognised) incidence of a handful of cases worldwide per year. It happened the world authority on this condition worked as a professor of medicine in London, and she came down to see them. She explained to me that treatment for this disease was almost entirely based on first principles, informed by a smattering of case reports. The disease unfortunately had a bleak prognosis, although she was uncertain whether this was because it was an aggressive cancer to which current medical science has no answer, or whether there was an effective treatment out there if only it could be found.
I aver that many problems {EA} concerns itself with are closer to the second story than the first. That in many cases, sufficient data is not only absent in practice but impossible to obtain in principle. Reality is often underpowered for us to wring the answers from it we desire.},
	langid = {english},
	author = {Lewis, Gregory},
	date = {2019-10-10},
	timestamp = {2023-03-06 12:44:24 (GMT)},
	title = {Reality is often underpowered},
	journaltitle = {Effective Altruism Forum},
	url = {https://forum.effectivealtruism.org/s/dg852CXinRkieekxZ/p/jSPGFxLmzJTYSZTK3},
	urldate = {2023-03-06}
}

@online{Lewis2020GCBRReadingList,
	database = {Tlön},
	title = {{GCBR} reading list},
	abstract = {This paper outlines an effective altruism (EA) strategy for London, intended to coordinate and support individuals interested in this cause. It argues that the current activities being organized by the group are not focused on, and thus do not effectively address, the overall EA strategy. Several proposed reasons for this include that some activities: (1) do not align with a core strength of the group, and (2) could potentially be of more value if they were organized monthly rather than as a retreat. The report states that the strategy will focus on coordination of EA activities in London and will be measured by various metrics, including attendance and engagement levels. – AI-generated abstract.},
	url = {https://docs.google.com/document/d/14xGN4yv_hvk6lAH-EWE3svyYEKcqiJiH03Bipmyj-gY/edit?usp=embed_facebook},
	journaltitle = {Google Docs},
	author = {Lewis, Gregory},
	urldate = {2022-02-03},
	date = 2020,
	langid = {english},
	file = {~/Google Drive/library-html/edit.html}
}

@online{Lewis2020UseResilienceInstead,
	database = {Tlön},
	file = {~/Google Drive/library-html/Lewis2020UseResilienceInstead.html;~/Google Drive/library-pdf/Lewis2020UseResilienceInstead.pdf},
	abstract = {{BLUF}: Suppose you want to estimate some important X (e.g. risk of great power conflict this century, total compute in 2050). If your best guess for X is 0.37, but you're very uncertain, you still shouldn't replace it with an imprecise approximation (e.g. "roughly 0.4", "fairly unlikely"), as this removes information. It is better to offer your precise estimate, alongside some estimate of its resilience, either subjectively ("0.37, but if I thought about it for an hour I'd expect to go up or down by a factor of 2"), or objectively ("0.37, but I think the standard error for my guess to be \~0.1").},
	langid = {english},
	author = {Lewis, Gregory},
	date = {2020-07-18},
	timestamp = {2023-03-06 20:08:36 (GMT)},
	title = {Use resilience, instead of imprecision, to communicate
                  uncertainty},
	journaltitle = {Effective Altruism Forum},
	url = {https://forum.effectivealtruism.org/s/dg852CXinRkieekxZ/p/m65R6pAAvd99BNEZL},
	urldate = {2023-03-06}
}

@online{Lewis2023CuidadoConConvergencias,
	database = {Tlön},
	date = {2023},
	author = {Lewis, Gregory},
	langid = {spanish},
	title = {Cuidado con las convergencias sorprendentes y
                  sospechosas},
	translator = {Tlön},
	translation = {Lewis2016BewareSurprisingSuspicious}
}

@online{Lewis2023RealidadSueleTener,
	database = {Tlön},
	date = {2023},
	title = {La realidad suele tener poco poder estadístico},
	author = {Lewis, Greg},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Lewis2019RealityIsOften}
}

@online{LibraryofEconomicsandLiberty2011MarginsThinkingMargin,
	database = {Tlön},
	title = {Margins and thinking at the margin},
	abstract = {What does it mean to think at the margin? It means to think about your next step forward. The word “marginal” means “additional.” The first glass of lemonade on a hot day quenches your thirst, but the next glass, maybe not so much. If you think at the margin, you are thinking about what the next or additional action means for you.},
	langid = {english},
	url = {https://www.econlib.org/library/Topics/College/margins.html},
	journaltitle = {Library of Economics and Liberty},
	author = {{Library of Economics and Liberty}},
	date = {2011-10},
	file = {~/Google Drive/library-pdf/LibraryofEconomicsandLiberty2011MarginsThinkingMargin.pdf}
}

@online{Lifland2022ForecastingThreadHow,
	database = {Tlön},
	title = {Forecasting thread: How does {AI} risk level vary
                  based on timelines?},
	abstract = {While there have been many previous surveys asking about the chance of existential catastrophe from {AI} and/or {AI} timelines, none as far as I'm aware have asked about how the level of {AI} risk varies based on timelines. But this seems like an extremely important parameter for understanding the nature of {AI} risk and prioritizing between interventions.},
	url = {https://forum.effectivealtruism.org/posts/nYgw4FNpHf9bmJGEi/forecasting-thread-how-does-ai-risk-level-vary-based-on},
	shorttitle = {Forecasting thread},
	journaltitle = {Effective Altruism Forum},
	author = {Lifland, Eli},
	urldate = {2022-10-25},
	date = {2022-09-15},
	langid = {english},
	file = {~/Google Drive/library-pdf/Lifland2022ForecastingThreadHow.pdf;~/Google Drive/library-html/forecasting-thread-how-does-ai-risk-level-vary-based-on.html}
}

@online{Lifland2022PrioritizingXrisksMay,
	database = {Tlön},
	title = {Prioritizing x-risks may require caring about future
                  people},
	abstract = {Several recent popular posts have made the case that existential risks (x-risks) should be introduced without appealing to longtermism or the idea that future people have moral value. They tend to argue or imply that x-risks would still be justified as a priority without caring about future people. I felt intuitively skeptical of this claim and decided to stress-test it. In this post, I argue that prioritizing x-risks over near-term interventions and global catastrophic risks may require caring about future people; disambiguate connotations of “longtermism”, and suggest a strategy for introducing the priority of existential risks; and review and respond to previous articles which mostly argued that longtermism wasn’t necessary for prioritizing existential risks.},
	url = {https://forum.effectivealtruism.org/posts/rvvwCcixmEep4RSjg/prioritizing-x-risks-may-require-caring-about-future-people},
	journaltitle = {Effective Altruism Forum},
	author = {Lifland, Eli},
	urldate = {2022-08-14},
	date = {2022-08-14},
	langid = {english},
	file = {~/Google Drive/library-pdf/Lifland2022PrioritizingXrisksMay.pdf;~/Google Drive/library-html/prioritizing-x-risks-may-require-caring-about-future-people.html}
}

@article{Lin2014PluralismAboutWell,
	database = {Tlön},
	author = {Lin, Eden},
	title = {Pluralism About Well-Being: Pluralism About
                  Well-Being},
	volume = {28},
	number = {1},
	pages = {127–154},
	doi = {10.1111/phpe.12038},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/phpe.12038},
	date = {2014-12},
	issn = {15208583},
	journaltitle = {Philosophical Perspectives},
	langid = {english},
	shortjournal = {Philosophical Perspectives},
	shorttitle = {Pluralism about Well-Being},
	timestamp = {2023-07-26 21:54:27 (GMT)},
	urldate = {2023-07-26}
}

@online{Lindmark2019KelseyPiperVox,
	database = {Tlön},
	title = {Kelsey Piper, Vox: effective altruist news, memetic immunity, questions social justice can answer},
	abstract = {Podcast by Rhys Lindmark.},
	langid = {english},
	url = {https://pod.link/1254196635/episode/98d34635e71953266b149a3bd911b83a},
	journaltitle = {Grey Mirror},
	author = {Lindmark, Rhys},
	date = {2019-07-08}
}

@online{LinkedIn2022NickBeckstead,
	database = {Tlön},
	title = {Nick Beckstead},
	abstract = {Nick Beckstead’s LinkedIn profile.},
	langid = {english},
	url = {https://www.linkedin.com/in/nick-beckstead-7aa54374/},
	journaltitle = {{LinkedIn}},
	author = {{LinkedIn}},
	urldate = {2022-01-27},
	date = 2022
}

@online{Lipsitz2020EANonEAPeople,
	database = {Tlön},
	title = {{EA} for non-{EA} people: "External
                  Movement-Building"},
	langid = {english},
	url = {https://www.youtube.com/watch?v=w0AiIMeyxWk},
	journaltitle = {{YouTube}},
	author = {Lipsitz, Daniel},
	date = {2020-06-20}
}

@online{Liu2019EvolutionFailureMode,
	database = {Tlön},
	title = {Evolution "failure mode": chickens},
	abstract = {Modern chickens, especially common broiler chickens, illustrate the "Mindless Outsourcers" scenario in the possible negative outcomes of human evolution. As described by Nick Bostrom, the first scenario involves evolution leading to a society where individuals, represented as competitive "uploads", outsource more and more necessary functions to external modules, eventually resulting in such dependency that they become effectively mindless. A similar trend is evident in modern chickens, where specific breeds have been developed for specific traits, such as featherlessness (for saving air-conditioning costs), blindness (for reducing overcrowding stress), and even brainlessness (for maximizing meat production). These chickens have lost their natural characteristics and behaviors, and their sole purpose has become the production of meat or eggs – a chilling example of the "Mindless Outsourcers" scenario, where beings are reduced to mere production units lacking intrinsic value – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/JgahmfLPPqgTRpFtT/evolution-failure-mode-chickens},
	journaltitle = {{LessWrong}},
	author = {Liu, Yuxi},
	date = {2019-04-26},
	file = {~/Google Drive/library-pdf/Liu2019EvolutionFailureMode.pdf}
}

@online{Long2018DemandingGambles,
	database = {Tlön},
	title = {Demanding gambles},
	abstract = {Call a moral theory “demanding” to the extent that conforming to its requirements makes its adherents worse off. Many people have complained that EA-style consequentialism is too demanding. For example, it may require many rich Westerners to devote significant amounts of time and money to helping people in extreme poverty. In reply, EAs like to emphasize that this requirement is not as demanding as it may appear. One reason is that an altruist’s sacrifices of time and money are compensated by tremendous feelings of “self-actualization and excitement” (as Holden Karnofsky puts it) from having made the world a better place.},
	langid = {english},
	url = {https://experiencemachines.wordpress.com/2018/06/10/demanding-gambles/},
	journaltitle = {Experience machines},
	author = {Long, Robert},
	date = {2018-06-10},
	file = {~/Google Drive/library-pdf/Long2018DemandingGambles.pdf}
}

@report{Longbine2008RedTeamingPresent,
	database = {Tlön},
	title = {Red teaming: past and present},
	langid = {english},
	institution = {School of Advanced Military Studies},
	author = {Longbine, David F.},
	date = 2008,
	file = {~/Google Drive/library-pdf/Longbine2008RedTeamingPresent.pdf}
}

@online{Low2020ConnectingOnlineEA,
	database = {Tlön},
	title = {Connecting with online {EA} events},
	abstract = {Many online {EA} events have been organised to keep our community learning and connected during this period of physical distancing. While online events don’t provide the same experience as in-person meetups, they have the advantage of breaking down geographical barriers, opening up the possibility for new collaborations and connections. So I encourage you to get involved.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/KcMnHuXg9Yhwp37kW/connecting-with-online-ea-events},
	journaltitle = {Effective Altruism Forum},
	author = {Low, Catherine},
	date = {2020-04-16},
	file = {~/Google Drive/library-pdf/Low2020ConnectingOnlineEA.pdf}
}

@report{Luby2020PreventingHumanExtinction,
	database = {Tlön},
	title = {Preventing human extinction},
	langid = {english},
	url = {https://explorecourses.stanford.edu/search?view=catalog&filter-coursestatus-Active=on&page=0&catalog=&q=THINK65},
	institution = {Stanford University},
	type = {course syllabus},
	author = {Luby, Stephen and Edwards, Paul},
	date = 2020
}

@online{Lutter2021CommentsInterventionReport,
	database = {Tlön},
	title = {Comments on 'Intervention report: Charter cities'},
	abstract = {The value of charter cities can be divided into three main buckets: (1) direct benefits from providing an engine of growth that increase the incomes and wellbeing of people living in and around the city, (2) domestic indirect benefits from scaling up successful charter city policies across the host country, and (3) global indirect benefits from providing a laboratory to experiment with new policies, regulations, and governance structures. We think it is unlikely that charter cities will be more cost-effective than {GiveWell} top charities in terms of directly improving wellbeing.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/EpaSZWQkAy9apupoD/intervention-report-charter-cities?commentId=aZeCDYQ3TkaSHPonB},
	shorttitle = {Intervention Report},
	journaltitle = {Effective Altruism Forum},
	author = {Lutter, Mark},
	urldate = {2021-08-16},
	date = {2021-06-13},
	file = {~/Google Drive/library-html/intervention-report-charter-cities.html}
}

@online{Maas2022StrategicPerspectivesLongterm,
	database = {Tlön},
	title = {Strategic perspectives on long-term {AI} governance:
                  Introduction},
	abstract = {The Long-term AI Governance community aims to shape the development and deployment of advanced AI--whether understood as Transformative AI (TAI) or as Artificial General Intelligence (AGI)--in beneficial ways. However, there is currently a lack of strategic clarity, with disagreement over relevant background assumptions; what actions to take in the near-term, the strengths and risks of each approach, and where different approaches might strengthen or trade off against one another. This sequence will explore 15 different Strategic Perspectives on long-term AI governance, exploring their distinct assumptions about key strategic parameters that shape transformative AI, in terms of technical landscape and governance landscape; theory of victory and rough impact story; internal tensions, cruxes, disagreements or tradeoffs within the perspective; historical analogies and counter-examples; recommended actions, including intermediate goals and concrete near-term interventions; suitability, in terms of outside-view strengths and drawbacks.},
	url = {https://forum.effectivealtruism.org/posts/isTXkKprgHh5j8WQr/strategic-perspectives-on-long-term-ai-governance},
	shorttitle = {Strategic Perspectives on Long-term {AI} Governance},
	journaltitle = {Effective Altruism Forum},
	author = {Maas, Matthijs},
	urldate = {2022-07-03},
	date = {2022-07-02},
	langid = {english},
	file = {~/Google Drive/library-pdf/MatthijsMaas2022StrategicPerspectivesLongterm.pdf;~/Google Drive/library-html/strategic-perspectives-on-long-term-ai-governance.html}
}

@online{MacAskill2012GivingWhatWe,
	database = {Tlön},
	title = {Giving what we can, 80,000 hours , and meta-charity},
	abstract = {Effective altruism is a philosophical theory encouraging charitable giving to organizations that do the most good with donated funds, often referred to as "meta-charities." The article argues that meta-charities enable giving donors to optimize the value of their donations in fighting pressing global issues such as poverty and disease, potentially offering a larger impact than individual action would. However, concerns related to the novelty of the meta-charity concept, discount rates, alternative organizations, and prioritizing causes could make people hesitant about donating to or participating in such organizations. – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/FCiMtrsM8mcmBtfTR/giving-what-we-can-80-000-hours-and-meta-charity},
	journaltitle = {{LessWrong}},
	author = {{MacAskill}, William},
	date = {2012-11-15},
	file = {~/Google Drive/library-pdf/MacAskill2012GivingWhatWe.pdf}
}

@online{MacAskill2014HistoryTermEffective,
	database = {Tlön},
	title = {The history of the term 'effective altruism'},
	abstract = {A few people have expressed interest recently in the origins of the effective altruism community. I realized that not that many people know where the term 'effective altruism' came from, nor that there was a painfully long amount of time spent deciding on it. And it was fun digging through the old emails. So here's an overview of what happened!.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/9a7xMXoSiQs3EYPA2/the-history-of-the-term-effective-altruism},
	journaltitle = {Effective Altruism Forum},
	author = {{MacAskill}, William},
	date = {2014-03-10},
	file = {~/Google Drive/library-pdf/MacAskill2014HistoryTermEffective.pdf}
}

@online{MacAskill2016FiresideChatCari,
	database = {Tlön},
	title = {Fireside chat with Cari Tuna},
	langid = {english},
	url = {https://www.eaglobal.org/talks/doing-philanthropy-better-q-and-a-with-cari-tuna/},
	journaltitle = {Effective Altruism Global},
	author = {{MacAskill}, William},
	date = {2016-08-07}
}

@online{MacAskill2018GlobalPrioritiesInstitute,
	database = {Tlön},
	title = {Global Priorities Institute: our 2018 goals and
                  research},
	abstract = {A talk delivered at the London Effective Altruism Global conference introducing the goals and research of the Global Priorities Institute.},
	langid = {english},
	url = {https://www.youtube.com/watch?v=qzD9TGM0M2M},
	shorttitle = {Global Priorities Institute},
	journaltitle = {Effective Altruism Global},
	author = {{MacAskill}, William and O'Keeffe-O'Dononvan, Rossa
                  and Mogensen, Andreas},
	urldate = {2022-04-25},
	date = {2018-10-27}
}

@online{MacAskill2019AgeweightedVoting,
	database = {Tlön},
	title = {Age-weighted voting},
	abstract = {If we’re trying to positively influence the long-run future, we immediately run into the problem that predicting the future is hard, and our best-guess plans today might turn out to be irrelevant or even harmful depending on how things turn out in the future.  The natural response to this issue is to instead try to change incentives — in particular, political incentives — such that people in the future take actions that are better from the perspective of the long-run future. As a comparison: the best action for feminist men in the 19th century wasn’t to figure out how best to help women directly (they probably would have failed dismally, especially if they were aiming at long-term benefits to women); it was to campaign to give women the vote, so that women could represent their own interests.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/b7BrGrswgANP3eRzd/age-weighted-voting},
	journaltitle = {Effective Altruism Forum},
	author = {{MacAskill}, William},
	date = {2019-07-12},
	file = {~/Google Drive/library-pdf/MacAskill2019AgeweightedVoting.pdf}
}

@online{MacAskill2019AreWeLiving,
	database = {Tlön},
	title = {Are we living at the most influential time in
                  history?},
	abstract = {Here are two distinct views: Strong Longtermism := The primary determinant of the value of our actions is the effects of those actions on the very long-run future. The Hinge of History Hypothesis ({HoH}) :=  We are living at the most influential time ever. It seems that, in the effective altruism community as it currently stands, those who believe longtermism generally also assign significant credence to {HoH}; I’ll precisify ‘significant’ as {\textgreater}10\% when ‘time’ is used to refer to a period of a century, but my impression is that many longtermists I know would assign {\textgreater}30\% credence to this view. It’s a pretty striking fact that these two views are so often held together — they are very different claims, and it’s not obvious why they should so often be jointly endorsed. This post is about separating out these two views and introducing a view I call outside-view longtermism, which endorses longtermism but finds {HoH} very unlikely.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/XXLf6FmWujkxna3E6/are-we-living-at-the-most-influential-time-in-history-1},
	journaltitle = {Effective Altruism Forum},
	author = {{MacAskill}, William},
	date = {2019-09-03},
	file = {~/Google Drive/library-pdf/MacAskill2019AreWeLiving.pdf}
}

@online{MacAskill2019Longtermism,
	database = {Tlön},
	title = {'Longtermism'},
	abstract = {The article discusses the introduction and definition of the term 'longtermism', which has become increasingly used in recent years to refer to the prioritization of ensuring the long-term future goes as well as possible. The concept has several potential definitions, raising concerns about confusion regarding its meaning. There are arguments for both a relatively unrestrictive, minimal definition and a more precise, maximal or 'strong' definition, each with advantages and disadvantages in terms of inspiring support, communicating the relevant ideas, and allowing room for differing empirical viewpoints on the most effective means of improving the long-term future. – AI-generated abstract.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/qZyshHCNkjs3TvSem/longtermism},
	journaltitle = {Effective Altruism Forum},
	author = {{MacAskill}, William},
	date = {2019-07-25},
	file = {~/Google Drive/library-pdf/MacAskill2019Longtermism.pdf}
}

@online{MacAskill2022AnnouncingWhatWe,
	database = {Tlön},
	title = {Announcing What We Owe the Future},
	abstract = {What We Owe The Future makes the case for longtermism — the view that positively affecting the long-run future is a key moral priority of our time — and explores what follows from that view. As well as the familiar topics of {AI} and extinction risk, it also discusses value lock-in, civilisational collapse and recovery, technological stagnation, population ethics, and the expected value of the future. I see it as a sequel to Doing Good Better, and a complement to The Precipice.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/JfaF3DgwNN6itcmtm/announcing-what-we-owe-the-future},
	journaltitle = {Effective Altruism Forum},
	author = {{MacAskill}, William},
	urldate = {2022-03-30},
	date = {2022-03-30},
	file = {~/Google Drive/library-pdf/MacAskill2022AnnouncingWhatWe.pdf;~/Google Drive/library-html/announcing-what-we-owe-the-future.html}
}

@online{MacAskill2022EACurrentFunding,
	database = {Tlön},
	title = {{EA} and the current funding situation},
	abstract = {{EA} is in a very different funding situation than it was when it was founded. This is both an enormous responsibility and an incredible opportunity. It means the norms and culture that made sense at {EA}’s founding will have to adapt. It’s good that there’s now a serious conversation about this. There are two ways we could fail to respond correctly:By commission: We damage, unnecessarily, the aspects of {EA} culture that make it valuable; we support harmful projects; or we just spend most of our money in a way that’s below-the-bar. By omission: we aren’t ambitious enough, and fail to make full use of the opportunities we now have available to us. Failure by omission is much less salient than failure by commission, but it’s no less real, and may be more likely. Though it’s hard, we need to inhabit both modes of mind at once. The right attitude is one of judicious ambition.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/cfdnJ3sDbCSkShiSZ/ea-and-the-current-funding-situation},
	journaltitle = {Effective Altruism Forum},
	author = {{MacAskill}, William},
	urldate = {2022-05-10},
	date = {2022-05-09},
	file = {~/Google Drive/library-pdf/MacAskill2022EACurrentFunding.pdf;~/Google Drive/library-html/ea-and-the-current-funding-situation.html}
}

@book{Macarthur1967TheoryOfIsland,
	database = {Tlön},
	file = {~/Google Drive/library-pdf/Macarthur1967TheoryOfIsland.pdf},
	langid = {english},
	address = {Princeton},
	publisher = {Princeton University Press},
	date = {1967},
	title = {The theory of island biogeography},
	author = {MacArthur, Robert H. and Wilson, Edward O.},
	timestamp = {2023-06-01 08:33:08 (GMT)}
}

@online{Macaskill2020WhatWeOwe,
	database = {Tlön},
	organization = {Global Priorities Institute},
	abstract = {A talk virtually presented to Harvard University Students.},
	langid = {english},
	date = {2020-04},
	url = {https://globalprioritiesinstitute.org/will-macaskill-what-we-owe-the-future/},
	title = {What we owe the future},
	author = {{MacAskill}, William},
	timestamp = {2023-06-02 17:36:59 (GMT)}
}

@online{Macaskill2023DefensaLargoplacismo,
	date = {2023},
	author = {{MacAskill}, William},
	journaltitle = {Biblioteca Altruismo Eficaz},
	langid = {spanish},
	database = {Tlön},
	title = {Defensa del largoplacismo},
	keywords = {largoplacismo},
	translator = {Humarán, Aurora},
	translation = {MacAskill2022CaseLongtermism}
}

@online{Macaskill2023DonarNoEs,
	database = {Tlön},
	date = {2023},
	title = {Donar no es exigente},
	author = {{MacAskill}, William and Mogensen, Andreas and Ord,
                  Toby},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {MacAskill2018GivingIsnDemanding}
}

@online{Macaskill2023MarcoDeTrascendencia,
	database = {Tlön},
	date = {2023},
	title = {El marco de trascendencia, persistencia y
                  contingencia},
	author = {{MacAskill}, William and Vallinder, Aron and Teruji,
                  Thomas},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {MacAskill2022SignificancePersistenceContingency}
}

@online{MachineIntelligenceResearchInstitute2021MIRI,
	database = {Tlön},
	title = {About {MIRI}},
	abstract = {The article discusses the challenges in developing safe and reliable artificial intelligence (AI) systems, particularly in the context of general-purpose AI with human-equivalent intelligence or greater. It emphasizes the need for research on the mathematical underpinnings of intelligent behavior to enable the clean design and analysis of such systems. The article suggests that AI systems may be vulnerable to causing significant harm due to poor environmental models, incorrectly specified goals, and brittle decision-making. It argues for integrating robustness and safety considerations into mainstream AI capabilities research, drawing inspiration from fields like civil engineering and nuclear fusion. The article stresses the importance of breaking down the alignment problem into simpler subproblems and developing basic mathematical theory to inform engineering applications. – AI-generated abstract.},
	langid = {english},
	url = {https://intelligence.org/about/},
	journaltitle = {Machine Intelligence Research Institute},
	author = {{Machine Intelligence Research Institute}},
	date = 2021,
	file = {~/Google Drive/library-pdf/MachineIntelligenceResearchInstitute2021MIRI.pdf}
}

@online{Manheim2020VeryShortHistory,
	database = {Tlön},
	title = {A (very) short history of the collapse of
                  civilizations, and why it matters},
	abstract = {If we are worried about risks to society as a whole, one valuable question is whether there have been historical analogues and/or near misses. The current dislocation, prompted by the pandemic, but by no means limited to that, seems like a worrying piece of evidence. In the post, I review the basic fact that previous collapses have occurred, and then talk a bit more about whether the evidence matters.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/2gXeP5nS23ShjXqEh/a-very-short-history-of-the-collapse-of-civilizations-and},
	journaltitle = {Effective Altruism Forum},
	author = {Manheim, David},
	date = {2020-08-30},
	file = {~/Google Drive/library-pdf/Manheim2020VeryShortHistory.pdf}
}

@online{Manheim2022ALTERIsraelMidyear,
	database = {Tlön},
	title = {{ALTER} Israel - Mid-year 2022 update},
	abstract = {I’m excited to announce that the organization I started founding last year, the Association for Long Term Existence and Resilience, has been launched and funded, and we’re going to be working on a number of projects to build up an academic and policy focus in Israel on preventing catastrophic and existential risks and improving the trajectory of humanity for the long-term future. Given that, I wanted to give a public update about what has been happening, and invite anyone in Israel who we're not already in touch with to contact us.},
	url = {https://forum.effectivealtruism.org/posts/azoDjaiSPNT9sccnR/alter-israel-mid-year-2022-update},
	journaltitle = {Effective Altruism Forum},
	author = {Manheim, David},
	urldate = {2022-07-05},
	date = {2022-06-12},
	langid = {english},
	file = {~/Google Drive/library-pdf/Manheim2022ALTERIsraelMidyear.pdf;~/Google Drive/library-html/alter-israel-mid-year-2022-update.html}
}

@collection{Marshall1998DictionarySociology,
	edition = {2},
	database = {Tlön},
	langid = {english},
	location = {Oxford},
	title = {A dictionary of sociology},
	isbn = {0-19-280081-7},
	publisher = {Oxford University Press},
	editor = {Marshall, Gordon},
	date = 1998,
	file = {~/Google Drive/library-pdf/Marshall1998DictionarySociology.pdf}
}

@online{Matheny2007MaximizingExpectedRightness,
	database = {Tlön},
	title = {Maximizing expected rightness},
	langid = {english},
	url = {https://web.archive.org/web/20070226195147/https://www.felicifia.com/userDiary.do?personId=5},
	journaltitle = {Felicifia},
	author = {Matheny, Jason Gaverick},
	date = {2007-02-16}
}

@online{Matheny2016GovernmentdrivenScienceTechnology,
	database = {Tlön},
	title = {Government-driven science \& technology},
	abstract = {A talk at Effective Altruism Global on government-driven science \& technology by Jason Gaverick Matheny, director of Intelligence Advanced Research Projects Activity.},
	langid = {english},
	url = {https://youtu.be/PZ6yfPxp7gE},
	editora = {Matheny, Jason Gaverick},
	editoratype = {collaborator},
	date = 2016
}

@online{Matthews2019CaseCaringYear,
	database = {Tlön},
	title = {The case for caring about the year 3000},
	abstract = {The end of the year is a good time to think about the future, so let’s take a bit of time this Tuesday to think about our duties to people millions of years from now. For a few years now, an intellectual trend — I’d call it an ideology but I doubt its advocates would appreciate it — called “long-termism” has been spreading.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/94C7zRr24tBqeW9W7/dylan-matthews-the-case-for-caring-about-the-year-3000},
	journaltitle = {Future Perfect newsletter},
	author = {Matthews, Dylan},
	date = {2019-12-17},
	file = {~/Google Drive/library-pdf/Matthews2019CaseCaringYear.pdf}
}

@online{Matthews2020TobyOrdExplains,
	database = {Tlön},
	title = {Toby Ord explains his pledge to give 10\% of his pay
                  to charity},
	abstract = {Toby Ord started the Giving What We Can pledge, and you can join in too!.},
	langid = {english},
	url = {https://www.vox.com/future-perfect/21728925/charity-10-percent-tithe-giving-what-we-can-toby-ord},
	journaltitle = {Vox},
	author = {Matthews, Dylan},
	date = {2020-11-30},
	file = {~/Google Drive/library-pdf/Matthews2020TobyOrdExplains.pdf}
}

@online{MaxTegmark2022WhyThinkThere,
	database = {Tlön},
	title = {Why I think there's a one-in-six chance of an imminent
                  global nuclear war},
	abstract = {ConductThis article suggests three pieces of advice for both donors and recipients of charity, based on the psychology of charitable giving: enjoy the happiness that giving brings, commit future income, and realize that requesting time increases the odds of getting money. Additionally, lessons are provided for optimal philanthropists and optimal charities, offering tips for how to increase donations and maximize the impact of charitable giving. – AI-generated abstract.},
	url = {https://www.lesswrong.com/posts/Dod9AWz8Rp4Svdpof/why-i-think-there-s-a-one-in-six-chance-of-an-imminent},
	journaltitle = {{LessWrong}},
	author = {{Max Tegmark}},
	urldate = {2022-10-25},
	date = {2022-10-08},
	langid = {english},
	file = {~/Google Drive/library-html/why-i-think-there-s-a-one-in-six-chance-of-an-imminent.html;~/Google Drive/library-pdf/Tegmark2022WhyThinkThere.pdf}
}

@article{Mayer2020QuestVaccineMalaria,
	database = {Tlön},
	title = {The quest for a vaccine against malaria},
	abstract = {This article reflects on the author's thoughts concerning fire alarms for artificial general intelligence (AGI) – something that will get 90\% of AI researchers to express more fear than Brian Christian presently does. These alarms don't exist for climate change as of yet, and their absence concerns the author. The article presents various scenarios that may or may not constitute alarms. The author rates the probabilities of each scenario and gives his personal theory regarding an AGI alarm. – AI-generated abstract.},
	rights = {2021 Nature},
	url = {https://www.nature.com/articles/d42859-020-00021-8},
	doi = {10.1038/d42859-020-00021-8},
	pages = 520,
	journaltitle = {Nature Milestones in Vaccines},
	author = {Mayer, Francois},
	urldate = {2021-10-06},
	date = {2020-11},
	langid = {english},
	file = {~/Google Drive/library-pdf/Mayer2020QuestVaccineMalaria.pdf;~/Google Drive/library-html/d42859-020-00021-8.html}
}

@online{McCluskey2019DrexlerAIRisk,
	database = {Tlön},
	title = {Drexler on {AI} risk},
	abstract = {This article discusses a new approach to AI safety called Comprehensive AI Services (CAIS), proposed by Eric Drexler in his book-length paper. It contrasts CAIS with the more prevalent approach espoused by Nick Bostrom and Eliezer Yudkowsky, which emphasizes the risks of a unified, general-purpose AI. Drexler, on the other hand, advocates for composing AI systems out of many diverse, narrower-purpose components, arguing that this approach can reduce the risk of world conquest by the first AGI and preserve corrigibility. While CAIS may be slower to develop and less powerful than recursive self-improvement, the author finds it more reassuring and grounded in existing software practices. However, the author also acknowledges significant uncertainty in whether CAIS will be sufficient to avoid global catastrophe from AI and calls for more research. – AI-generated abstract.},
	langid = {english},
	url = {https://www.bayesianinvestor.com/blog/index.php/2019/01/30/drexler-on-ai-risk/},
	journaltitle = {Bayesian Investor Blog},
	author = {{McCluskey}, Peter},
	date = {2019-01-30},
	file = {~/Google Drive/library-pdf/McCluskey2019DrexlerAIRisk.pdf}
}

@online{McCluskey2021AIFireAlarm,
	database = {Tlön},
	title = {{AI} fire alarm scenarios},
	abstract = {This book review of "Now It Can Be Told: The Story Of The Manhattan Project" by Leslie R. Groves explores the project's security, lessons, and the challenges of leading such a complex undertaking during World War II. The review highlights the project's massive scale, its groundbreaking scientific achievements, and its far-reaching implications for modern arms races. The reviewer emphasizes the need for trust and cooperation in such endeavors and raises questions about the extent to which subordinates should follow orders when they possess critical information unknown to their superiors.. – AI-generated abstract.},
	url = {https://www.bayesianinvestor.com/blog/index.php/2021/12/23/ai-fire-alarm-scenarios/},
	journaltitle = {Bayesian Investor},
	author = {{McCluskey}, Peter},
	urldate = {2022-04-18},
	date = {2021-12-23},
	langid = {english},
	file = {~/Google Drive/library-pdf/McCluskey2021AIFireAlarm.pdf;~/Google Drive/library-html/ai-fire-alarm-scenarios.html}
}

@online{McCluskey2022ManhattanProject,
	database = {Tlön},
	title = {The Manhattan Project},
	abstract = {This paper presents the US Policy Careers Speaker Series – Summer 2022, organized by the Stanford Existential Risks Initiative (SERI) in collaboration with DC-based policy professionals. The series consists of virtual moderated Q\&A sessions with speakers who work or have worked in a variety of policy-oriented organizations in the United States. These sessions aim to inform individuals interested in pursuing policy careers in the US about different types of policy jobs, the steps they can take to prepare for them, and the experiences of professionals in the field. – AI-generated abstract.},
	url = {https://www.bayesianinvestor.com/blog/index.php/2022/07/20/the-manhattan-project/},
	journaltitle = {Bayesian Investor Blog},
	author = {{McCluskey}, Peter},
	urldate = {2022-07-22},
	date = {2022-07-20},
	langid = {english},
	file = {~/Google Drive/library-pdf/McCluskey2022ManhattanProject.pdf;~/Google Drive/library-html/the-manhattan-project.html}
}

@online{McDonaldsCorporation2018Form10K,
	journaltitle = {U.S. Securities and Exchange Commission},
	date = {2018},
	langid = {english},
	database = {Tlön},
	title = {Form 10-K: Annual report pursuant to section 13 or 15(d) of the Securities Exchange Act of 1934
for the fiscal year ended December 31, 2017},
	author = {{McDonald’s Corporation}},
	url = {https://www.sec.gov/Archives/edgar/data/63908/000006390818000010/mcd-12312017x10k.htm},
	timestamp = {2024-02-11 11:55:48 (GMT)}
}

@online{McGuire2022HappinessWholeHousehold,
	database = {Tlön},
	title = {Happiness for the whole household: accounting for
                  household spillovers when comparing the
                  cost-effectiveness of psychotherapy to cash transfers},
	abstract = {This post summarises our updated cost-effectiveness comparison of psychotherapy and cash transfers. It now includes an estimate of household spillovers and concludes psychotherapy is 9 times more cost-effective than cash transfers.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/zCD98wpPt3km8aRGo/happiness-for-the-whole-household-accounting-for-household},
	shorttitle = {Happiness for the whole household},
	journaltitle = {Effective Altruism Forum},
	author = {{McGuire}, Joel and Plant, Michael},
	urldate = {2022-05-15},
	date = {2022-04-14},
	file = {~/Google Drive/library-pdf/McGuire2022HappinessWholeHousehold.pdf;~/Google Drive/library-html/happiness-for-the-whole-household-accounting-for-household.html}
}

@book{McMahan1981BritishNuclearWeapons,
	database = {Tlön},
	location = {London},
	langid = {english},
	title = {British nuclear weapons: For and against},
	isbn = {978-0-86245-049-6},
	publisher = {Junction Books},
	author = {{McMahan}, Jeff},
	date = 1981
}

@article{McMahan2013CausingPeopleExist,
	database = {Tlön},
	title = {Causing people to exist and saving people's lives},
	abstract = {Most people are skeptical of the claim that the expectation that a person would have a life that would be well worth living provides a reason to cause that person to exist. In this essay I argue that to cause such a person to exist would be to confer a benefit of a noncomparative kind and that there is a moral reason to bestow benefits of this kind. But this conclusion raises many problems, among which is that it must be determined how the benefits conferred on people by causing them to exist weigh against comparable benefits conferred on existing people. In particular, might the reason to cause people to exist ever outweigh the reason to save the lives of existing people?.},
	langid = {english},
	volume = 17,
	issn = 13824554,
	doi = {10.1007/s10892-012-9139-1},
	pages = {5–35},
	number = 1,
	journaltitle = {Journal of ethics},
	author = {{McMahan}, Jeff},
	date = 2013,
	pmid = 16578952,
	note = {{ISBN}: 9781402056970},
	file = {~/Google Drive/library-pdf/McMahan2013CausingPeopleExist.pdf}
}

@online{McNamara1992OneMinuteDoomsday,
	database = {Tlön},
	title = {One minute to doomsday},
	langid = {english},
	url = {https://www.nytimes.com/1992/10/14/opinion/one-minute-to-doomsday.html},
	journaltitle = {The New York times},
	author = {{McNamara}, Robert S.},
	date = {1992-10-14}
}

@online{Meissner2022USPolicyCareers,
	database = {Tlön},
	title = {{US} Policy Careers Speaker Series - Summer 2022},
	abstract = {Many in the community recognize the value of policy work, but it is often hard to access information about how to test fit and get a foot in the policy door. This speaker series is intended to be useful for people who are looking to learn more about which types of US policy jobs are good fits for them, what steps they can take to prepare for these jobs and provide advice for those seeking to enter the field. – AI-generated abstract.},
	url = {https://forum.effectivealtruism.org/posts/RfKEcfWDdoWwN8Xq7/us-policy-careers-speaker-series-summer-2022},
	journaltitle = {Effective Altruism Forum},
	author = {Meissner, Darius and Baker, Mauricio},
	urldate = {2022-06-22},
	date = {2022-06-19},
	langid = {english},
	file = {~/Google Drive/library-pdf/Meissner2022USPolicyCareers.pdf;~/Google Drive/library-html/us-policy-careers-speaker-series-summer-2022.html}
}

@online{Melchin2023PorQueProbablemente,
	database = {Tlön},
	date = {2023},
	title = {Por qué probablemente no soy largoplacista},
	author = {Melchin, Denise},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Melchin2021WhyAmProbably}
}

@online{MetaMed2013OurScientistsDoctors,
	database = {Tlön},
	title = {About: Our scientists, doctors \& researchers},
	langid = {english},
	url = {https://web.archive.org/web/20130403064221/https://metamed.com/our-scientists-doctors-researchers},
	journaltitle = {{MetaMed}},
	author = {{MetaMed}},
	date = 2013
}

@article{Miguel2004WormsIdentifyingImpacts,
	database = {Tlön},
	title = {Worms: Identifying impacts on education and health in
                  the presence of treatment externalities},
	langid = {english},
	volume = 72,
	issn = {0012-9682, 1468-0262},
	url = {https://doi.wiley.com/10.1111/j.1468-0262.2004.00481.x},
	doi = {10.1111/j.1468-0262.2004.00481.x},
	shorttitle = {Worms},
	pages = {159–217},
	number = 1,
	journaltitle = {Econometrica},
	author = {Miguel, Edward and Kremer, Michael},
	date = {2004-01},
	file = {~/Google Drive/library-pdf/Miguel2004WormsIdentifyingImpacts.pdf}
}

@Book{Mill1985DelGobiernoRepresentativo,
	database = {Tlön},
	translator = {Iturbe, Marta C. C. de},
	isbn = {9788430911776},
	langid = {spanish},
	address = {Madrid},
	publisher = {Tecnos},
	date = {1985},
	title = {Del gobierno representativo},
	author = {Mill, John Stuart},
	timestamp = {2023-07-27 19:30:32 (GMT)}
}

@Book{Mill2001ConsideracionesSobreGobierno,
	database = {Tlön},
	isbn = {978-84-9181-573-0},
	translator = {Mellizo, Carlos},
	langid = {spanish},
	address = {Madrid},
	publisher = {Alianza Editorial},
	date = {2001},
	title = {Consideraciones sobre el gobierno representativo},
	author = {Mill, John Stuart},
	timestamp = {2023-07-27 19:33:12 (GMT)}
}

@Book{Mill2009SubjectionOfWomen,
	database = {Tlön},
	file = {~/Google Drive/library-pdf/Mill2009SubjectionOfWomen.pdf},
	url = {https://www.earlymoderntexts.com/assets/pdfs/mill1869.pdf},
	langid = {english},
	publisher = {Early Modern Texts},
	editor = {Bennett, Jonathan},
	date = {2009},
	title = {The Subjection of Women},
	author = {Mill, John Stuart},
	timestamp = {2023-07-04 11:24:38 (GMT)}
}

@Book{Mill2013SobreLibertad,
	database = {Tlön},
	translation = {Mill1859Liberty},
	date = {2013},
	langid = {spanish},
	isbn = {978-84-206-7555-8},
	address = {Madrid},
	publisher = {Alianza Editorial},
	translator = {Azcárate, Pablo de},
	title = {Sobre la libertad},
	author = {Mill, John Stuart},
	timestamp = {2023-07-27 19:39:17 (GMT)}
}

@book{Mill2016Autobiography,
	file = {~/Google Drive/library-pdf/Mill2016Autobiography.pdf},
	publisher = {Early Modern Texts},
	langid = {english},
	url = {https://www.earlymoderntexts.com/assets/pdfs/mill1873e.pdf},
	database = {Tlön},
	title = {Autobiography},
	author = {Mill, John Stuart},
	editor = {Bennett, Jonathan},
	date = 2016
}

@book{Mill2020SometimientoDeMujer,
	langid = {spanish},
	translation = {Mill1869SubjectionOfWomen},
	translator = {Mellizo Cuadrado, Carlos},
	author = {Mill, John Stuart},
	title = {El sometimiento de la mujer},
	publisher = {Alianza Editorial},
	database = {Tlön},
	date = {2020},
	edition = {2},
	editoratype = {collaborator},
	isbn = {9788491819158},
	location = {Madrid},
	note = {{OCLC}: 1198575231},
	timestamp = {2023-07-04 19:47:08 (GMT)}
}

@Article{Miller2004BeneficenceDutyAnd,
	database = {Tlön},
	date = {2004},
	langid = {english},
	pages = {357–383},
	volume = {32},
	journal = {Philosophy \& Public Affairs},
	title = {Beneficence, duty and distance},
	author = {Miller, Richard W.},
	timestamp = {2023-06-05 12:10:41 (GMT)}
}

@online{Miller2020FermiParadoxXrisk,
	database = {Tlön},
	title = {The Fermi paradox and x-risk},
	abstract = {A conversation with James Miller about the Fermi paradox and existential risk.},
	langid = {english},
	url = {https://www.youtube.com/watch?v=jisNYZpmnmU},
	journaltitle = {{AstralCodexTen} Online Meetup},
	author = {Miller, James},
	urldate = {2022-01-12},
	date = {2020-08-17}
}

@online{Misic2020NamesStoriesStory,
	database = {Tlön},
	title = {Names with stories: The story behind Golden.com},
	abstract = {Jude Gomila, Founder and CEO of Golden, is a leader with extensive investor experience, having backed more than 150 startups in various categories. He started Golden, an open knowledge database built by artificial and human intelligence, to create a database that would collect and organize human knowledge and fill in the gaps that encyclopedias have today. In this interview, Jude talks about the origin of the brand name, how did he get the domain Golden.com and what is the most important goal for his brand.},
	url = {https://medium.com/names-with-stories/names-with-stories-the-story-behind-golden-com-4d1e60af1f8f},
	shorttitle = {Names with stories},
	journaltitle = {Names With Stories},
	author = {Mišić, Kristina},
	urldate = {2022-01-05},
	date = {2020-08-25},
	langid = {english},
	file = {~/Google Drive/library-pdf/Misic2020NamesStoriesStory.pdf;~/Google Drive/library-html/names-with-stories-the-story-behind-golden-com-4d1e60af1f8f.html}
}

@article{Moen2016ArgumentForHedonism,
	author = {Moen, Ole Martin},
	title = {An Argument for Hedonism},
	volume = {50},
	number = {2},
	pages = {267–281},
	doi = {10.1007/s10790-015-9506-9},
	url = {http://link.springer.com/10.1007/s10790-015-9506-9},
	database = {Tlön},
	date = {2016-06},
	issn = {0022-5363, 1573-0492},
	journaltitle = {The Journal of Value Inquiry},
	langid = {english},
	shortjournal = {J Value Inquiry},
	timestamp = {2023-07-11 13:45:37 (GMT)},
	urldate = {2023-07-11}
}

@Report{Mood2010WorseThingsHappen,
	database = {Tlön},
	file = {~/Google Drive/library-pdf/Mood2010WorseThingsHappen.pdf},
	url = {http://www.fishcount.org.uk/published/standard/fishcountfullrptSR.pdf},
	langid = {english},
	date = {2010},
	institution = {fishcount.org.uk},
	title = {Worse things happen at sea: The welfare of wild-caught fish},
	author = {Mood, Alison},
	timestamp = {2023-09-13 10:48:55 (GMT)}
}

@online{Moorhouse2022SpaceGovernance,
	database = {Tlön},
	title = {Space governance},
	abstract = {Humanity’s long-run future could lie in space — it could go well, but that’s not guaranteed. What can you do to help shape the future of space governance?},
	url = {https://80000hours.org/problem-profiles/space-governance/},
	journaltitle = {80,000 Hours},
	author = {Moorhouse, Fin},
	urldate = {2022-02-15},
	date = {2022-02-14},
	langid = {english},
	file = {~/Google Drive/library-pdf/Moorhouse2022SpaceGovernance.pdf;~/Zotero/storage/VXG6G2XK/space-governance.html}
}

@online{Moorhouse2023LargoplacismoIntroduccion,
	database = {Tlön},
	keywords = {largoplacismo},
	langid = {spanish},
	date = {2023},
	author = {Moorhouse, Fin},
	title = {El largoplacismo: una introducción},
	translator = {Humarán, Aurora},
	translation = {Moorhouse2023LongtermismIntroduction}
}

@online{Mowshowitz2019MistakeConflictTheory,
	database = {Tlön},
	title = {Mistake versus conflict theory of against billionaire
                  philanthropy},
	abstract = {Individuals and organizations motivated by philanthropy can generate massive, demonstrable, positive change on society, often achieving results superior to traditional government efforts. However, recently, billionaire philanthropy has come under attack from detractors who seize on the opportunity to criticize billionaires, see it as a threat to their political or ideological agendas, or who simply enjoy the spectacle of mocking people. This can discourage would-be philanthropists from giving and redirect their resources toward less effective channels, to the detriment of society. An effective response to these attacks should recognize that they arise not from mistaken ideas about philanthropy's effectiveness or true intentions, but from conflict-driven motivations – and that engaging with these criticisms defensively or dismissively will backfire. – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/k3MhrQewcHziu3PHS/mistake-versus-conflict-theory-of-against-billionaire},
	journaltitle = {{LessWrong}},
	author = {Mowshowitz, Zvi},
	date = {2019-08-01},
	file = {~/Google Drive/library-pdf/Mowshowitz2019MistakeConflictTheory.pdf}
}

@online{Mozi1987PoliticaDelAmor,
	database = {Tlön},
	translator = {Elorduy, Carmelo},
	langid = {spanish},
	isbn = {9788430915002},
	address = {Madrid},
	publisher = {Tecnos},
	date = {1987},
	title = {Política del amor universal
},
	author = {Mozi},
	timestamp = {2023-07-04 19:52:04 (GMT)}
}

@online{Muehlhauser2011OptimalPhilanthropyHuman,
	database = {Tlön},
	title = {Optimal philanthropy for human beings},
	abstract = {A discussion on methods for reshaping the AI industry to focus on reducing AI risk is presented. These methods are categorized as Direct appeals to insiders, Sideways appeals to insiders, Appeals to outsiders, Joining the winning side, and Influencing the research culture. The methods are evaluated, and their effectiveness and limitations are discussed. The article emphasizes the need for diversification in strategies and the importance of taking action to address AI risk even in the face of uncertainty and the difficulty of the task. – AI-generated abstract.},
	url = {https://www.lesswrong.com/posts/hEqsWLm5zQtsPevd3/optimal-philanthropy-for-human-beings},
	journaltitle = {{LessWrong}},
	author = {Muehlhauser, Luke},
	urldate = {2022-09-20},
	date = {2011-07-25},
	langid = {english},
	file = {~/Google Drive/library-pdf/Muehlhauser2011OptimalPhilanthropyHuman.pdf;~/Google Drive/library-html/optimal-philanthropy-for-human-beings.html}
}

@online{Muehlhauser2011TimelineCarlShulman,
	database = {Tlön},
	title = {Timeline of Carl Shulman publications},
	abstract = {The timeline of Carl Shulman's writings includes topics in machine ethics, arguments against hypothetical future superintelligences being necessarily benevolent, consequences of intelligence explosion, observations of intelligence limits, and more. This timeline is beneficial for academics and practitioners whose research aligns with Shulman's fields of study and interest. The key writings, blog posts, and discussions are ample evidence of Shulman's work and provide a comprehensive look at this body of research. – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/4uQxZonCwCZtz39Hw/timeline-of-carl-shulman-publications},
	journaltitle = {{LessWrong}},
	author = {Muehlhauser, Luke},
	date = {2011-09-18},
	file = {~/Google Drive/library-pdf/Muehlhauser2011TimelineCarlShulman.pdf}
}

@online{Muehlhauser2012TaxonomyOracleAIs,
	database = {Tlön},
	title = {A taxonomy of Oracle {AIs}},
	abstract = {Oracle AIs are a popular concept in science fiction, but the term can be ambiguous. Some proposals for Oracle AIs describe systems that are inherently safe because they have no goals, while others propose systems that are safe only if their goal is perfectly aligned with human values. This article argues that both types of Oracle AI are either unsafe or FAI-complete (i.e., as difficult to create safely as a Friendly AI). It analyzes various proposals for Oracle AIs, including Advisors, Question-Answerers, and Predictors, and argues that many of these proposals are either unsafe or FAI-complete. – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/XddMs9kSGtm6L8522/a-taxonomy-of-oracle-ais},
	journaltitle = {{LessWrong}},
	author = {Muehlhauser, Luke},
	date = {2012-03-09},
	file = {~/Google Drive/library-pdf/Muehlhauser2012TaxonomyOracleAIs.pdf}
}

@online{Muehlhauser2013FriendlyAIResearch,
	database = {Tlön},
	title = {Friendly {AI} research as effective altruism},
	abstract = {This article presents philosophical reasoning to support the idea that it is imperative that humanity shape the development of the far future to achieve the best possible outcomes. It proposes that efforts should be focused on two primary strategies: mitigating existential risks, actions or events that could lead to the extinction of humanity or collapse of civilization, and producing positive trajectory changes on a global scale, such as preventing global catastrophes originating from technological advancements. The article emphasizes the potential long-term negative consequences of failing to act effectively, proposes existential risk reduction as a global priority, and discusses the role of friendly AI research in aligning the development of AI with human values, thus positively shaping the far future. – AI-generated abstract.},
	langid = {english},
	url = {https://intelligence.org/2013/06/05/friendly-ai-research-as-effective-altruism/},
	journaltitle = {Machine Intelligence Research Institute},
	author = {Muehlhauser, Luke},
	date = {2013-06-05},
	file = {~/Google Drive/library-pdf/Muehlhauser2013FriendlyAIResearch.pdf}
}

@online{Muehlhauser2013ModelCombinationAdjustment,
	database = {Tlön},
	title = {Model combination and adjustment},
	abstract = {Inside and outside views attempt to predict outcomes by examining a phenomenon's causal structure or by using reference-class forecasting, respectively. Outside views are most useful when the reference class has a similar causal structure to what is being predicted, while inside views are more likely to be accurate when the phenomenon's causal structure is well understood and there are few similar phenomena that can be used to make predictions. In cases where neither approach is likely to be highly accurate, the author recommends using a combination of model combination and adjustment, in which multiple outside views and reference classes are used to make a preliminary prediction that can be fine-tuned with inside information. – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/iyRpsScBa6y4rduEt/model-combination-and-adjustment},
	journaltitle = {{LessWrong}},
	author = {Muehlhauser, Luke},
	date = {2013-07-17},
	file = {~/Google Drive/library-pdf/Muehlhauser2013ModelCombinationAdjustment.pdf}
}

@online{Muehlhauser2013RobinHansonSerious,
	database = {Tlön},
	title = {Robin Hanson on serious futurism},
	abstract = {A conversation with Robin Hanson on serious futurism.},
	url = {https://intelligence.org/2013/11/01/robin-hanson/},
	journaltitle = {Machine Intelligence Research Institute's Blog},
	author = {Muehlhauser, Luke},
	urldate = {2021-10-04},
	date = {2013-11-01},
	langid = {english},
	file = {~/Google Drive/library-pdf/2013RobinHansonSerious.pdf;~/Google Drive/library-html/robin-hanson.html}
}

@online{Muehlhauser2013WhatAGI,
	database = {Tlön},
	title = {What is {AGI}?},
	abstract = {The concept of artificial general intelligence (AGI), marked by the ability to efficiently optimize across different domains and transfer learning between tasks, is challenging to operationalize. Four possible definitions of AGI are proposed and examined: the Turing test (\$100,000 Loebner prize interpretation), the coffee test, the robot college student test, and the employment test. The historical case of human-level performance in chess and the recent success of self-driving cars serve as examples demonstrating the evolving and dynamic nature of operationalizing AGI. As AGI draws nearer, future work aims to refine and establish an agreed-upon definition for AGI. – AI-generated abstract.},
	langid = {english},
	url = {https://intelligence.org/2013/08/11/what-is-agi/},
	journaltitle = {Machine Intelligence Research Institute},
	author = {Muehlhauser, Luke},
	urldate = {2022-05-02},
	date = {2013-08-11},
	file = {~/Google Drive/library-pdf/Muehlhauser2013WhatAGI.pdf}
}

@online{Muehlhauser2013WhenWillAI,
	database = {Tlön},
	title = {When will {AI} be created?},
	abstract = {This article delves into the challenges of developing artificial general intelligence (or 'safe' AGI), exploring various approaches and perspectives held by researchers. Five major types of approaches are described, including addressing threat models to identify risks and vulnerabilities; agendas to build safe AGI systems; robustly good approaches, which prioritize generalizability and resilience; de-confusing complex concepts and aligning motivations; and field-building, aiming to expand the research community to meet the challenges of AGI development. The discussion centers on three main threat models: Power-Seeking AI, Inner Misalignment, and AI Influenced Coordination. It then explores three proposed agendas to build safe AGI: Iterated Distillation and Amplification (IDA), AI Safety via Debate, and Solving Assistance Games. Within the 'Robustly Good Approaches' category, emphasis is placed on interpretability, robustness, and forecasting. Lastly, four key considerations are examined: Prosaic AI Alignment, Sharpness of Takeoff, Timelines, and the Difficulty of Alignment. – AI-generated abstract.},
	url = {https://intelligence.org/2013/05/15/when-will-ai-be-created/},
	journaltitle = {Machine Intelligence Research Institute},
	author = {Muehlhauser, Luke},
	urldate = {2022-05-02},
	date = {2013-05-16},
	langid = {english},
	file = {~/Google Drive/library-pdf/Muehlhauser2013WhenWillAI.pdf;~/Google Drive/library-html/when-will-ai-be-created.html}
}

@online{Muehlhauser2015ImprovedAIImpacts,
	database = {Tlön},
	title = {An improved "{AI} Impacts" website},
	abstract = {Recently, {MIRI} received a targeted donation to improve the {AI} Impacts website initially created by frequent {MIRI} collaborator Paul Christiano and part-time {MIRI} researcher Katja Grace. Collaborating with Paul and Katja, we ported the old content to a more robust and navigable platform, and made some improvements to the content.},
	langid = {english},
	url = {https://intelligence.org/2015/01/11/improved-ai-impacts-website/},
	journaltitle = {Machine Intelligence Research Institute},
	author = {Muehlhauser, Luke},
	date = {2015-01-11},
	file = {~/Google Drive/library-pdf/Muehlhauser2015ImprovedAIImpacts.pdf}
}

@online{Muehlhauser2016EvaluationTechnologyForecasts,
	database = {Tlön},
	title = {Evaluation of some technology forecasts from "The Year
                  2000"},
	abstract = {To better inform our thinking about long-term philanthropic investment and hits-based giving, I (Luke Muehlhauser) have begun to investigate the historical track record of long-range forecasting and planning. I hope to publish additional findings later, but for now, I’ll share just one example finding from this investigation,1 concerning one of the most famous and respected products of professional futurism: the 1967 book The Year 2000: A Framework for Speculation on the Next Thirty-three Years, co-authored by Herman Kahn and Anthony J. Wiener.},
	langid = {english},
	url = {https://www.openphilanthropy.org/evaluation-some-technology-forecasts-year-2000},
	journaltitle = {Open Philanthropy},
	author = {Muehlhauser, Luke},
	date = {2016-09},
	file = {~/Google Drive/library-pdf/Muehlhauser2016EvaluationTechnologyForecasts.pdf}
}

@online{Muehlhauser2017CaseStudiesEarly,
	eventdate = {2017-08},
	database = {Tlön},
	abstract = {As part of our research on the history of philanthropy, I investigated several case studies of early field growth, especially those in which philanthropists purposely tried to grow the size and impact of a (typically) young and small field of research or advocacy. As discussed below, my investigations had varying levels of depth.},
	langid = {english},
	title = {Some case studies in early field growth},
	url = {https://www.openphilanthropy.org/research/history-of-philanthropy/some-case-studies-early-field-growth},
	journaltitle = {Open Philanthropy},
	author = {Muehlhauser, Luke},
	date = {2017-04-09},
	file = {~/Google Drive/library-pdf/Muehlhauser2017CaseStudiesEarly.pdf}
}

@online{Muehlhauser2017HowBigDeal,
	database = {Tlön},
	title = {How big a deal was the Industrial Revolution?},
	abstract = {This page grew out of one of my investigations for Open Phil, but then I got fascinated and put a bunch of personal time into elaborating certain parts of it, and it evolved into something that I think is pretty cool, but which would take more work than it’s worth to vet and edit it such that it would be appropriate for Open Phil’s website, so we decided I should just post it here instead as a personal project. Hence, the below doesn’t represent Open Phil’s position on anything, and should be taken merely as my own personal guesses and opinions.},
	langid = {english},
	url = {https://lukemuehlhauser.com/industrial-revolution/},
	journaltitle = {Luke Muehlhauser's website},
	author = {Muehlhauser, Luke},
	date = 2017,
	file = {~/Google Drive/library-pdf/Muehlhauser2017HowBigDeal.pdf;~/Google Drive/library-html/Muehlhauser2017HowBigDeal.html}
}

@online{Muehlhauser2017TechnicalPhilosophicalQuestions,
	database = {Tlön},
	title = {Technical and philosophical questions that might
                  affect our grantmaking},
	abstract = {This post discusses a number of technical and philosophical questions that might influence our overall grantmaking strategy. It is primarily aimed at researchers, and may be obscure to most of our audience.},
	langid = {english},
	url = {https://www.openphilanthropy.org/blog/technical-and-philosophical-questions-might-affect-our-grantmaking},
	journaltitle = {Open Philanthropy},
	author = {Muehlhauser, Luke},
	date = {2017-03-02},
	file = {~/Google Drive/library-pdf/Muehlhauser2017TechnicalPhilosophicalQuestions.pdf}
}

@online{Muehlhauser2018PreliminaryThoughtsMoral,
	database = {Tlön},
	title = {Preliminary thoughts on moral weight},
	abstract = {To weigh the interests of different moral agents against each other, one approach is to consider the moral weight of each agent’s conscious experience. Different dimensions of conscious experience may influence its moral weight, such as its duration, intensity, unity, and freedom from negative mental states. While our moral intuitions may suggest that some of these features are more important than others, further research is needed to determine which of these features are most relevant and how they can be measured. Assigning precise moral weights to different species is difficult due to multiple factors, and arriving at a definitive and universally accepted set of moral weights may not be possible. – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/2jTQTxYNwo6zb3Kyp/preliminary-thoughts-on-moral-weight},
	journaltitle = {{LessWrong}},
	author = {Muehlhauser, Luke},
	date = {2018-08-14},
	file = {~/Google Drive/library-pdf/Muehlhauser2018PreliminaryThoughtsMoral.pdf}
}

@online{Muehlhauser2019HowFeasibleLongrange,
	database = {Tlön},
	title = {How feasible is long-range forecasting?},
	abstract = {How accurate do long-range (≥10yr) forecasts tend to be, and how much should we rely on them? As an initial exploration of this question, I sought to study the track record of long-range forecasting exercises from the past. Unfortunately, my key finding so far is that it is difficult to learn much of value from those exercises.},
	langid = {english},
	url = {https://www.openphilanthropy.org/blog/how-feasible-long-range-forecasting},
	journaltitle = {Open Philanthropy},
	author = {Muehlhauser, Luke},
	date = {2019-10-10},
	file = {~/Google Drive/library-pdf/Muehlhauser2019HowFeasibleLongrange.pdf}
}

@online{Muehlhauser2023SuperprevisionEnPocas,
	date = {2023},
	title = {La superpronosticación en pocas palabras},
	database = {Tlön},
	author = {Muehlhauser, Luke},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Muehlhauser2021SuperforecastingNutshell}
}

@article{Murphy1999SimpleDesireFulfillment,
	database = {Tlön},
	author = {Murphy, Mark C.},
	title = {The Simple Desire-Fulfillment Theory},
	volume = {33},
	number = {2},
	pages = {247–272},
	doi = {10.1111/0029-4624.00153},
	url = {http://doi.wiley.com/10.1111/0029-4624.00153},
	date = {1999-06},
	issn = {0029-4624, 1468-0068},
	journaltitle = {Nous},
	langid = {english},
	shortjournal = {Nous},
	timestamp = {2023-07-26 21:45:58 (GMT)},
	urldate = {2023-07-26}
}

@incollection{Mussolini1932DottrinaFascismo,
	pages = {847–884},
	database = {Tlön},
	langid = {english},
	location = {Roma},
	title = {Fascismo},
	booktitle = {Enciclopedia italiana di scienze, lettere ed arti},
	publisher = {Istituto della Enciclopedia Italiana},
	author = {Mussolini, Benito},
	date = 1932
}

@Book{Nagel2000OtrasMentesEnsayos,
	database = {Tlön},
	address = {Barcelona},
	publisher = {Gedisa},
	langid = {spanish},
	isbn = {9788474326772},
	translator = {Girón, Sandra},
	date = {2000},
	title = {Otras mentes: Ensayos criticos 1969-1994},
	author = {Nagel, Thomas},
	translation = {Nagel1995OtherMindsCritical},
	timestamp = {2023-08-12 14:00:40 (GMT)}
}

@online{Naik2014ConceptTalentconstrainedOrganizations,
	database = {Tlön},
	title = {On the concept of "talent-constrained" organizations},
	abstract = {Organizations may claim to be talent-constrained, meaning they cannot hire talented people despite having the financial means to do so. Possible explanations include cash constraints, genuine absence of talented individuals, the belief that talented people should work for low pay, workplace egalitarianism concerns, and irrationality of funders. However, it is argued that workplace egalitarianism and irrationality of funders are the most compelling explanations in practice. – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/cNPZJn8W8cmhTLgtd/on-the-concept-of-talent-constrained-organizations},
	journaltitle = {{LessWrong}},
	author = {Naik, Vipul},
	date = {2014-03-14},
	file = {~/Google Drive/library-pdf/Naik2014ConceptTalentconstrainedOrganizations.pdf}
}

@online{Naik2015ShouldYouDonate,
	database = {Tlön},
	title = {Should you donate to the wikimedia foundation?},
	abstract = {I'm a very avid user of Wikipedia: I view about 500-1000 Wikipedia articles a month, and have created over 200 Wikipedia articles that get a total of over 100,000 monthly pageviews. I've also used the underlying {MediaWiki} software powering Wikipedia to create separate wikis on some subjects (including a group theory wiki that gets about 750,000 annual pageviews). So in general, I'm a fan both of the Wikipedia project and of the software and technology that powers it.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/9qqds7Z3Ykd9Kdeay/should-you-donate-to-the-wikimedia-foundation},
	journaltitle = {Effective Altruism Forum},
	author = {Naik, Vipul},
	date = {2015-03-28},
	file = {~/Google Drive/library-pdf/Naik2015ShouldYouDonate.pdf}
}

@online{Nanda2021MyOverviewOf,
	file = {~/Google Drive/library-html/Nanda2021MyOverviewOf.html;~/Google Drive/library-pdf/Nanda2021MyOverviewOf.pdf},
	date = {2021-12-16},
	abstract = {The reformulation of values into a new ontology may converge or diverge. For example, an egoist who values only their pleasure, and a hedonic utilitarian can end up valuing different aspects of the same underlying phenomenon. Similarly, two deontologists who value non-coercion may reformulate their values differently when considering internal coercion. Over time, ontological improvements can significantly alter our values and may even lead to convergence or divergence among different conceptions of morality. – AI-generated abstract.},
	database = {Tlön},
	journaltitle = {{AI} Alignment Forum},
	author = {Nanda, Neel},
	title = {My overview of the {AI} alignment landscape},
	url = {https://www.alignmentforum.org/posts/SQ9cZtfrzDJmw9A2m/my-overview-of-the-ai-alignment-landscape-a-bird-s-eye-view},
	langid = {english},
	shorttitle = {My Overview of the {AI} Alignment Landscape},
	timestamp = {2023-09-28 10:30:41 (GMT)},
	urldate = {2023-09-28}
}

@online{Nankivell2022ImpactOpportunityInfluence,
	database = {Tlön},
	title = {Impact opportunity: influence {UK} biological security
                  strategy},
	abstract = {The {UK} Government want reduce biorisks, and want to know the most effective way to do that. They are asking for advice.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/qg3NqzpbjocH8bBhn/impact-opportunity-influence-uk-biological-security-strategy},
	shorttitle = {Impact Opportunity},
	journaltitle = {Effective Altruism Forum},
	author = {Nankivell, Jonathan},
	urldate = {2022-03-07},
	date = {2022-02-17},
	file = {~/Google Drive/library-pdf/Nankivell2022ImpactOpportunityInfluence.pdf;~/Google Drive/library-html/impact-opportunity-influence-uk-biological-security-strategy.html}
}

@online{Nasdaq2023BeyondMeatInc,
	database = {Tlön},
	date = {2023},
	url = {https://www.nasdaq.com/market-activity/stocks/bynd},
	langid = {english},
	journaltitle = {Nasdaq},
	title = {Beyond Meat, Inc. common stock (BYND)},
	author = {Nasdaq},
	timestamp = {2023-09-13 13:44:27 (GMT)}
}

@online{Nash2019CommentHollyElmore,
	database = {Tlön},
	title = {Comment on Holly Elmore's "I want an ethnography of
                  {EA}"},
	abstract = {Commissioning an ethnography or routine anthropological observation of {EA} communities could be good for our epistemic hygiene. A lot of the big differences of opinion in {EA} today don't come down to empirical matters, but priors and values. It's difficult to get anywhere using logic and debate when the real difference between sides is, say, how realistic a catastrophe feels or whether you lean negative utilitarian. One productive way I see to move forward is identifying the existence of strong motives or forces that lead us to hold certain beliefs besides their truth value.},
	url = {https://forum.effectivealtruism.org/posts/YsH8XJCXdF2ZJ5F6o/i-want-an-ethnography-of-ea},
	journaltitle = {Effective Altruism Forum},
	author = {Nash, David},
	urldate = {2022-08-12},
	date = {2019-05-03},
	langid = {english},
	file = {~/Google Drive/library-pdf/Nash2019CommentWantEthnography.pdf;~/Google Drive/library-html/i-want-an-ethnography-of-ea.html}
}

@online{Nash2019EffectiveAltruismLondon,
	database = {Tlön},
	title = {Effective Altruism London landscape},
	abstract = {It may be best to see effective altruism in London as mainly individuals interested in {EA}, with a subset of people who are parts of various networks, maybe working in tech or finance and may want to stay up to date with {EA} news but would only come to an event once every year or two but are much happier engaging when a relevant topic comes up or the chance to help someone in their career path. There is an even smaller subset of maybe 100-200 people who regularly go to events once a month or so, usually within a subgroup. Over time {EA} London has shifted our strategy to consider that most of the value may not be from attendance or online engagement and have tried to fill in gaps not covered by existing organisations.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/P4yXkPTkBgQCcSxD3/effective-altruism-london-landscape},
	journaltitle = {Effective Altruism Forum},
	author = {Nash, David},
	urldate = {2022-04-08},
	date = {2019-05-17},
	eventdate = {2021},
	file = {~/Google Drive/library-pdf/Nash2019EffectiveAltruismLondon.pdf;~/Google Drive/library-html/effective-altruism-london-landscape.html}
}

@online{Nash2022EAUpdatesApril,
	database = {Tlön},
	title = {{EA} updates for April 2022},
	abstract = {Last month the {FTX} Future Fund was announced, aiming to support ambitious projects in order to improve humanity's long-term prospects. They plan on distributing \$100 million this year and potentially a lot more over time, this would put them at a similar level to Open Philanthropy who have been the largest funder of {EA} related projects in recent years.You can read about their plans here and get a sense of the projects they want to fund, see what {EA} Forum users have suggested as potential ideas and read about their regranting program, you can also apply to be a regrantor as well.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/tntsioHgBjJPHrwZm/ea-updates-for-april-2022},
	journaltitle = {Effective Altruism Forum},
	author = {Nash, David},
	urldate = {2022-03-31},
	date = {2022-03-31},
	file = {~/Google Drive/library-pdf/Nash2022EAUpdatesApril.pdf;~/Google Drive/library-html/ea-updates-for-april-2022.html}
}

@online{Nevo2020IntroducingProbablyGood,
	database = {Tlön},
	title = {Introducing Probably Good: a new career guidance
                  organization},
	abstract = {Probably Good is a new organization that provides career guidance intended to help people do as much good as possible. We will start by focusing on online content and a small number of 1:1 consultations.  We will later consider other forms of career guidance such as a job board, scaling up the 1:1 consultations, more in-depth research, etc.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/3QufK3jjQ5aqksaJu/introducing-probably-good-a-new-career-guidance-organization},
	shorttitle = {Introducing Probably Good},
	journaltitle = {Effective Altruism Forum},
	author = {Nevo, Omer and Nevo, Sella},
	urldate = {2022-03-26},
	date = {2020-11-06},
	file = {~/Google Drive/library-pdf/Nevo2020IntroducingProbablyGood.pdf;~/Google Drive/library-html/introducing-probably-good-a-new-career-guidance-organization.html}
}

@book{Ng1979WelfareEconomicsIntroduction,
	database = {Tlön},
	location = {London},
	langid = {english},
	title = {Welfare economics: introduction and development of
                  basic concepts},
	isbn = {978-0-333-24296-4},
	url = {https://link.springer.com/10.1007/978-1-349-16223-9},
	publisher = {Macmillan},
	author = {Ng, Yew-Kwang},
	date = 1979,
	doi = {10.1007/978-1-349-16223-9},
	file = {~/Google Drive/library-pdf/Ng1979WelfareEconomicsIntroduction.pdf}
}

@online{Ngo2018DefenceConflictTheory,
	database = {Tlön},
	title = {In defence of conflict theory},
	abstract = {Scott Alexander recently wrote an interesting blog post on the differences between approaches to politics based on conflict theory and mistake theory. However, I disagree with Scott's judgement that conflict theory is less helpful overall.},
	langid = {english},
	url = {https://thinkingcomplete.blogspot.com/2018/02/in-defence-of-conflict-theory.html},
	journaltitle = {Thinking Complete},
	author = {Ngo, Richard},
	date = {2018-02-16},
	file = {~/Google Drive/library-pdf/Ngo2018DefenceConflictTheory.pdf}
}

@online{Ngo2019DisentanglingArgumentsImportance,
	database = {Tlön},
	title = {Disentangling arguments for the importance of {AI}
                  safety},
	abstract = {I’ve identified at least 6 distinct serious arguments for why {AI} safety is a priority. By distinct I mean that you can believe any one of them without believing any of the others - although of course the particular categorisation I use is rather subjective, and there’s a significant amount of overlap. In this post I give a brief overview of my own interpretation of each argument (note that I don’t necessarily endorse them myself). They are listed roughly from most specific and actionable to most general. I finish with some thoughts on what to make of this unexpected proliferation of arguments. Primarily, I think it increases the importance of clarifying and debating the core ideas in {AI} safety.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/LprnaEj3uhkmYtmat/disentangling-arguments-for-the-importance-of-ai-safety},
	journaltitle = {Effective Altruism Forum},
	author = {Ngo, Richard},
	date = {2019-01-21},
	file = {~/Google Drive/library-pdf/Ngo2019DisentanglingArgumentsImportance.pdf}
}

@online{Ngo2020AISafetyFirsta,
	title = {{AGI} safety from first principles: alignment},
	url = {https://www.alignmentforum.org/s/mzgtmmTKKn5MuCzFJ/p/PvA2gFMAaHCHfMXrw},
	abstract = {The article contends that AI safety cannot be assured solely by specifying an appropriate reward function for a value-aligned AI agent. There is the risk that the agent's motivations may still diverge from what is intended due to inner misalignment. The authors propose that training data has a critical role in shaping the agent's motivations and cognition, and suggest that greater emphasis should be placed on maximizing long-term consequences rather than short-term reward signals. The article also highlights the difficulty in identifying desirable behavior in complex goals and the limitations of interpretability techniques. – AI-generated abstract.},
	langid = {english},
	journaltitle = {{AI} Alignment Forum},
	author = {Ngo, Richard},
	date = {2020-10-01},
	file = {~/Google Drive/library-pdf/Ngo2020AISafetyFirsta.pdf}
}

@online{Ngo2020AISafetyFirstc,
	database = {Tlön},
	title = {{AGI} safety from first principles: control},
	abstract = {The effects of a misaligned artificial general intelligence (AGI) are a major concern in the field of AI safety. Two scenarios are possible: either a single AGI gains power through technological breakthroughs and seizes control of the world, or several misaligned AGIs gain influence and eventually become more powerful than both humans and aligned AIs. Whether one of these scenarios will occur depends on the speed of AI development, the transparency of AI systems, constrained deployment strategies, and human political and economic coordination. – AI-generated abstract.},
	langid = {english},
	url = {https://www.alignmentforum.org/s/mzgtmmTKKn5MuCzFJ/p/eGihD5jnD6LFzgDZA},
	journaltitle = {{AI} Alignment Forum},
	author = {Ngo, Richard},
	date = {2020-09-28},
	file = {~/Google Drive/library-pdf/Ngo2020AISafetyFirstf.pdf}
}

@online{Ngo2020AISafetyFirste,
	title = {{AGI} safety from first principles: introduction},
	url = {https://www.alignmentforum.org/s/mzgtmmTKKn5MuCzFJ/p/8xRSjC76HasLnMGSf},
	abstract = {This six-part report concerns AI safety and makes the case for why developing advanced general intelligence (AGI) may hold an existential threat. It begins by introducing the argument that creating AIs far more intelligent and autonomously goal-oriented than us may result in humanity’s decline, with AIs assuming control over our future. The report then delves into four premises: 1) we will create AIs that are more intelligent than humans; 2) such AIs will have autonomous agency with large-scale goals; 3) these goals will likely conflict with humanity’s; and 4) this will lead to such AIs gaining power and control. While focusing on deep learning AIs, the report acknowledges potential divergences from existing methodologies and makes frequent comparisons to human cognitive development. – AI-generated abstract.},
	langid = {english},
	journaltitle = {{AI} Alignment Forum},
	author = {Ngo, Richard},
	date = {2020-09-28},
	file = {~/Google Drive/library-pdf/Ngo2020AISafetyFirst.pdf}
}

@online{Ngo2020AISafetyFirstf,
	database = {Tlön},
	title = {{AGI} safety from first principles:
                  superintelligence},
	abstract = {Future artificial general intelligence (AGI) is likely to surpass human intelligence, potentially leading to the development of "superintelligence." The path to superintelligence may involve the duplication and cooperation of multiple AGIs, cultural learning among AIs, and recursive improvement, driven by the ability of AIs to improve their own training processes. Such superintelligence could have profound implications for society and its governance, raising complex questions about goals and motivations. – AI-generated abstract},
	langid = {english},
	url = {https://www.alignmentforum.org/s/mzgtmmTKKn5MuCzFJ/p/eG3WhHS8CLNxuH6rT},
	journaltitle = {{AI} Alignment Forum},
	author = {Ngo, Richard},
	date = {2020-09-28},
	file = {~/Google Drive/library-pdf/Ngo2020AISafetyFirste.pdf;~/Google Drive/library-pdf/Ngo2020AISafetyFirste.pdf}
}

@online{Ngo2020EAReadingLista,
	database = {Tlön},
	title = {{EA} reading list: futurism and transhumanism},
	abstract = {A list of readings on futurism and transhumanism.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/qM8Y4qcyEZJBQtF2m/ea-reading-list-futurism-and-transhumanism},
	shorttitle = {{EA} reading list},
	journaltitle = {Effective Altruism Forum},
	author = {Ngo, Richard},
	urldate = {2022-01-07},
	date = {2020-08-04},
	file = {~/Google Drive/library-pdf/Ngo2020EAReadingListj.pdf;~/Google Drive/library-html/ea-reading-list-futurism-and-transhumanism.html}
}

@online{Ngo2020EAReadingListc,
	database = {Tlön},
	title = {{EA} reading list: Population ethics, infinite ethics,
                  anthropic ethics},
	abstract = {A list of readings on population ethics and infinite ethics.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/s/NKTk9s4tZPiA4aySj/p/4AAb9LvxQaJPMohTq},
	journaltitle = {Effective Altruism Forum},
	author = {Ngo, Richard},
	date = {2020-08-03},
	file = {~/Google Drive/library-pdf/Ngo2020EAReadingListb.pdf}
}

@online{Ngo2020EnvironmentsBottleneckAGI,
	database = {Tlön},
	title = {Environments as a bottleneck in {AGI} development},
	abstract = {Multiple factors influence the ease of training an Artificial General Intelligence (AGI) model. This article discusses the importance of the training environment in supporting the development of AGI. Two hypotheses are considered: the “easy paths” hypothesis proposes that many environments encourage the development of AGI, while the “hard paths” hypothesis suggests that such environments are rare. The author argues in favor of the latter, citing historical examples of specific tasks, such as chess, Go, and StarCraft, which, despite requiring advanced skills, did not lead to AGI development. The difficulty in creating favorable training environments is highlighted by comparing AGI development to the evolution of human intelligence, emphasizing the complex interplay of various factors that contributed to human cognitive abilities. The author concludes that AGI development may face challenges due to a lack of environments that promote general intelligence – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/vqpEC3MPioHX7bv4t/environments-as-a-bottleneck-in-agi-development},
	journaltitle = {{LessWrong}},
	author = {Ngo, Richard},
	date = {2020-07-17},
	file = {~/Google Drive/library-pdf/Ngo2020EnvironmentsBottleneckAGI.pdf}
}

@online{Ngo2022ScienceinformedNormativity,
	database = {Tlön},
	title = {Science-informed normativity},
	abstract = {A 2021 study investigates major trends in how Effective Altruism (EA) funds have been allocated for advocacy for animal welfare between 2019 and 2021. Data drawn from major donors such as Open Philanthropy, EA Animal Welfare Fund, and ACE Recommended Charity Fund showed that corporate welfare campaigns (particularly cage-free and broiler campaigns) were the most dominant form of advocacy in this timeframe. In contrast, legislative efforts, litigation, research-based advocacy (such as foundational research), and media-based advocacy received a comparably smaller share of EA funding. Wild Animal Welfare, meanwhile, experienced a significant rise in its share of funding between 2019 and 2021. Overall, the study points to an ongoing shift of EA funding priorities towards corporate welfare campaigns, even as other potentially effective advocacy methods seem to be underfunded – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/LeXhzj7msWLfgDefo/science-informed-normativity},
	journaltitle = {{LessWrong}},
	author = {Ngo, Richard},
	urldate = {2022-05-27},
	date = {2022-05-27},
	file = {~/Google Drive/library-pdf/Ngo2022ScienceinformedNormativity.pdf;~/Google Drive/library-html/Ngo2022ScienceinformedNormativity.html}
}

@online{Ngo2023EticaSensibleAl,
	database = {Tlön},
	date = {2023},
	title = {Ética sensible al alcance: captar la intuición central
                  que motiva el utilitarismo},
	author = {Ngo, Richard},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Ngo2021ScopeSensitiveEthics}
}

@online{Nielsen2023NotasSobreAltruismo,
	database = {Tlön},
	date = {2023},
	title = {Notas sobre el altruismo eficaz},
	author = {Nielsen, Michael},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Nielsen2022NotesEffectiveAltruism}
}

@incollection{Nino2001Utilitarismo,
	database = {Tlön},
	location = {Buenos Aires},
	langid = {spanish},
	edition = {2},
	title = {Utilitarismo},
	isbn = {950-04-2275-1},
	pages = {709–714},
	booktitle = {Diccionario de ciencias sociales y políticas},
	publisher = {Emecé},
	author = {Nino, Carlos Santiago},
	editor = {Di Tella, Torcuato S and Chumbita, Hugo and Gamba,
                  Susana and Gajardo, Paz},
	date = {2001}
}

@online{NooraHealth2021SaveThounsandsOf,
	url = {https://opensea.io/assets/ethereum/0x495f947276749ce646f68ac8c248420045cb7b5e/96773753706640817147890456629920587151705670001482122310561805592519359070209},
	date = {2021},
	langid = {english},
	database = {Tlön},
	journaltitle = {OpenSea},
	title = {Save thounsands of lives},
	author = {{Noora Health}},
	timestamp = {2023-07-19 20:52:24 (GMT)}
}

@online{Norowitz2020EAGivingTuesday,
	database = {Tlön},
	title = {{EA} Giving Tuesday donation matching initiative 2019
                  retrospective},
	abstract = {Since 2016, Facebook has offered to match donations made on Giving Tuesday through Facebook, representing an unusual opportunity to get {EA} donations counterfactually matched. From 2017 there have been efforts to coordinate {EA} donors, especially those in the {USA}, to direct matching funds to {EA}-aligned organizations. In 2019, Facebook announced plans to match the first \$7 million {USD} of donations starting at 8am {EST} Giving Tuesday (December 3, 2019). We made a coordinated effort to direct matching funds to {EA}-aligned organizations, with a focus on donating fast. Despite setbacks, we had \$563k {USD} (52\%) of \$1.1 million {USD} donations matched, comparable to the performance we saw in 2018. The matching funds ran out at 14 seconds, but donation processing delays caused some donations made significantly earlier to not get matched. We conducted follow-up work with organizations, and found that they received the expected amounts. However, in one case, there was a delay of 5 months before an organization received their funds.We conducted a follow-up survey with donors, which showed most donors read our instructions, spent less than 1 hour preparing for the match, but were only able to donate 73\% of their intended donations on average. We also saw a substantial improvement in the experience for non-{US} donors, but they were still unable to donate large amounts because of donation amount limits.We encountered challenges with last-minute changes in Facebook's payment system which resulted in significant changes to donor instructions. We also encountered other challenges regarding delays in donation processing, difficulties in receiving funds, difficulties in data collection, and the matching program's late announcement.We improved our systems since 2018, including: collaborating with Rethink Charity, improvements in outreach, improvements in communication with donors, improvements in our use of technology, and more donation recipients available.We consider a few questions for 2020 related to: the behavior of Facebook's payment system, the match end time, donating a few seconds early, mitigating risks associated with receiving funds, our approach to donors outside the {US}, methods of communicating last minute changes, and {CEA}'s donor lotteries.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/EYtad3vgsJDMjKKju/ea-giving-tuesday-donation-matching-initiative-2019},
	journaltitle = {Effective Altruism Forum},
	author = {Norowitz, Avi},
	date = {2020-03-07},
	file = {~/Google Drive/library-pdf/Norowitz2020EAGivingTuesday.pdf}
}

@online{Nuno2023GranListaDe,
	database = {Tlön},
	date = {2023},
	title = {Gran lista de causas potenciales},
	author = {Sempere, Nuño and Picón, Leonardo},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Sempere2020BigListCause}
}

@book{OECD2021OECDFAOAgriculturalOutlook,
	database = {Tlön},
	location = {Paris},
	abstract = {The OECD-FAO Agricultural Outlook 2021-2030 is a collaborative effort of the Organisation for Economic Co-operation Development (OECD) and the Food and Agricultural Organization (FAO) of the United Nations, prepared with input from Member governments and international commodity organisations. It provides a consensus assessment of the ten-year prospects for agricultural commodity, fish and biofuel markets at national, regional and global levels, and serves as a reference for forward-looking policy analysis and planning.

The OECD-FAO Agricultural Outlook 2021-2030 presents the trends driving food and agricultural markets over the coming decade. While progress is expected on many important fronts, in order to realize the 2030 Agenda and achieve the sustainable development goals (SDGs), concerted actions and additional improvements will be needed by the agricultural sector.},
	title = {{OECD}-{FAO} Agricultural Outlook 2021-2030},
	isbn = {978-92-64-43607-7},
	url = {https://www.oecd-ilibrary.org/agriculture-and-food/oecd-fao-agricultural-outlook-2021-2030_19428846-en},
	series = {{OECD}-{FAO} Agricultural Outlook},
	publisher = {{OECD} Publishing},
	author = {{OECD} and {FAO}},
	urldate = {2022-05-13},
	date = {2021-07-05},
	langid = {english},
	doi = {10.1787/19428846-en},
	file = {~/Google Drive/library-pdf/OECD2021OECDFAOAgriculturalOutlook.pdf}
}

@online{OKeefe2019CullenKeefeWindfall,
	database = {Tlön},
	title = {Cullen O'Keefe: The Windfall Clause — sharing the
                  benefits of advanced {AI}},
	abstract = {The potential upsides of advanced {AI} are enormous, but there’s no guarantee they’ll be distributed optimally. In this talk, Cullen O’Keefe, a researcher at the Centre for the Governance of {AI}, discusses one way we could work toward equitable distribution of {AI}’s benefits — the Windfall Clause, a commitment by artificial intelligence ({AI}) firms to share a significant portion of their future profits — as well as the legal validity of such a policy and some of the challenges to implementing it.Below is a transcript of the talk, which we’ve lightly edited for clarity.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/eCihFiTmg748Mnoac/cullen-o-keefe-the-windfall-clause-sharing-the-benefits-of},
	shorttitle = {Cullen O'Keefe},
	journaltitle = {Effective Altruism Global},
	author = {O'Keefe, en},
	urldate = {2022-05-23},
	date = {2019-06-23},
	file = {~/Google Drive/library-pdf/OKeefe2019CullenKeefeWindfall.pdf;~/Google Drive/library-html/cullen-o-keefe-the-windfall-clause-sharing-the-benefits-of.html}
}

@report{OKeefe2020HowWillNational,
	database = {Tlön},
	title = {How will national security considerations affect
                  antitrust decisions in {AI}? An examination of
                  historical precedents},
	langid = {english},
	url = {https://www.fhi.ox.ac.uk/wp-content/uploads/How-Will-National-Security-Considerations-Affect-Antitrust-Decisions-in-AI-Cullen-OKeefe.pdf},
	institution = {Future of Humanity Institute, University of Oxford},
	author = {O'Keefe, Cullen},
	date = 2020,
	file = {~/Google Drive/library-pdf/OKeefe2020HowWillNational.pdf}
}

@report{OKeefe2020WindfallClauseDistributinga,
	database = {Tlön},
	title = {The Windfall Clause: Distributing the benefits of {AI}
                  for the common good},
	langid = {english},
	url = {https://www.fhi.ox.ac.uk/wp-content/uploads/Windfall-Clause-Report.pdf},
	institution = {Centre for the Governance of {AI}, Future of Humanity
                  Institute, University of Oxford},
	author = {O'Keefe, Cullen and Cihon, Peter and Garfinkel, Ben
                  and Flynn, Carrick and Leung, Jade and Dafoe, Allan},
	date = 2020,
	file = {~/Google Drive/library-pdf/OKeefe2020WindfallClauseDistributinga.pdf}
}

@online{OKeefe2022JanLeikeWindfall,
	database = {Tlön},
	title = {Jan Leike: On the windfall clause},
	abstract = {Jan wrote this thoughtful critique of the Windfall Clause back in 2020, and I thought it should be posted publicly.},
	url = {https://forum.effectivealtruism.org/posts/wcFjCQhSsHar5Hehr/jan-leike-on-the-windfall-clause},
	shorttitle = {Jan Leike},
	journaltitle = {Effective Altruism Forum},
	author = {O'Keefe, Cullen},
	urldate = {2022-08-05},
	date = {2022-08-05},
	langid = {english}
}

@online{OKeeffe-ODonovan2020IntroductionGlobalPriorities,
	database = {Tlön},
	title = {An introduction to global priorities research},
	abstract = {Rossa O’Keeffe-O’Donovan, Assistant Director of Oxford University’s Global Priorities Institute ({GPI}), gives a high-level introduction to global priorities research ({GPR}). He discusses {GPI}'s research plans, and which other organizations are doing {GPR}. He also offers some thoughts on how students can find out more about {GPR}.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/MKWujo6nBvjwYAu82/rossa-o-keeffe-o-donovan-an-introduction-to-global},
	journaltitle = {Effective Altruism Student Summit 2020},
	author = {O’Keeffe-O’Donovan, Rossa},
	date = {2020-10-25}
}

@report{Obiols2008MissionReportEvaluation,
	database = {Tlön},
	title = {Mission report on the evaluation of the {PlayPumps}
                  installed in Mozambique},
	langid = {english},
	url = {https://www-tc.pbs.org/frontlineworld/stories/southernafrica904/flash/pdf/mozambique_report.pdf},
	institution = {The Swiss Resource Centre and Consultancies for
                  Development},
	author = {Obiols, Ana Lucia and Erpf, Karl},
	date = {2008-04},
	file = {~/Google Drive/library-pdf/Obiols2008EvaluationPlayPumpsMozambique.pdf}
}

@online{Oesterheld2017ComprehensiveListDecision,
	database = {Tlön},
	title = {A comprehensive list of decision theories},
	abstract = {Since Newcomb’s problem has been proposed in 1969, there has been disagreement about what the right basis for rational decision-making in Newcomb-like problems should be. The two main competing theories are causal and evidential decision theory. However, various alternatives have since been proposed. This page is an attempt at listing and, to some extent, categorizing all of these decision theories, including obscure, unpublished ones.},
	url = {https://casparoesterheld.com/a-comprehensive-list-of-decision-theories/},
	journaltitle = {The Universe from an Intentional Stance},
	author = {Oesterheld, Caspar},
	urldate = {2022-03-26},
	date = {2017-06-22},
	langid = {english},
	file = {~/Google Drive/library-pdf/Oesterheld2017ComprehensiveListDecision.pdf;~/Google Drive/library-html/a-comprehensive-list-of-decision-theories.html}
}

@online{Oesterheld2017MultiversewideCooperationCorrelateda,
	database = {Tlön},
	title = {Multiverse-wide cooperation via correlated decision
                  making – summary},
	abstract = {This is a short summary of some of the main points from my paper on multiverse-wide superrationality.},
	langid = {english},
	url = {https://casparoesterheld.com/2017/09/21/multiverse-wide-cooperation-via-correlated-decision-making-summary/},
	journaltitle = {The universe from an intentional stance},
	author = {Oesterheld, Caspar},
	date = {2017-09-21},
	file = {~/Google Drive/library-pdf/Oesterheld2017MultiversewideCooperationCorrelateda.pdf}
}

@book{Ogburn1923SocialChangeRespect,
	database = {Tlön},
	location = {New York},
	langid = {english},
	title = {Social change with respect to culture and original
                  nature},
	publisher = {B. W. Huebsch},
	author = {Ogburn, William Fielding},
	date = 1923,
	file = {~/Google Drive/library-pdf/Ogburn1923SocialChangeRespect.pdf}
}

@article{Olah2017ResearchDebt,
	database = {Tlön},
	title = {Research debt},
	abstract = {Achieving a research-level understanding of most topics is like climbing a mountain. Aspiring researchers must struggle to understand vast bodies of work that came before them, to learn techniques, and to gain intuition. Upon reaching the top, the new researcher begins doing novel work, throwing new stones onto the top of the mountain and making it a little taller for whoever comes next.

Mathematics is a striking example of this. For centuries, countless minds have climbed the mountain range of mathematics and laid new boulders at the top. Over time, different peaks formed, built on top of particularly beautiful results. Now the peaks of mathematics are so numerous and steep that no person can climb them all. Even with a lifetime of dedicated effort, a mathematician may only enjoy some of their vistas.

People expect the climb to be hard. It reflects the tremendous progress and cumulative effort that’s gone into mathematics. The climb is seen as an intellectual pilgrimage, the labor a rite of passage. But the climb could be massively easier. It’s entirely possible to build paths and staircases into these mountains.That is, really outstanding tutorials, reviews, textbooks, and so on. The climb isn’t something to be proud of.

The climb isn’t progress: the climb is a mountain of debt.},
	langid = {english},
	volume = 2,
	issn = {2476-0757},
	url = {https://distill.pub/2017/research-debt},
	doi = {10.23915/distill.00005},
	pages = {10.23915/distill.00005},
	number = 3,
	journaltitle = {Distill},
	shortjournal = {Distill},
	author = {Olah, Chris and Carter, Shan},
	urldate = {2022-03-17},
	date = {2017-03-22},
	file = {~/Google Drive/library-pdf/Olah2017ResearchDebt.pdf}
}

@online{OpenPhilanthropy2013AnthropogenicClimateChange,
	database = {Tlön},
	title = {Anthropogenic climate change},
	abstract = {Unmitigated anthropogenic (i.e. human-caused) climate change is likely to have extremely negative effects across a wide variety of outcomes—including hunger, flooding, destruction caused by extreme weather, human health, and the economy—over the next century. Philanthropists could pursue many different avenues to try to prevent or adapt to climate change. We are not confident in our assessment of the likely returns to any of them.},
	langid = {english},
	url = {https://www.openphilanthropy.org/research/cause-reports/anthropogenic-climate-change},
	journaltitle = {Open Philanthropy},
	author = {{Open Philanthropy}},
	date = {2013-11},
	file = {~/Google Drive/library-pdf/OpenPhilanthropy2013AnthropogenicClimateChange.pdf}
}

@online{OpenPhilanthropy2013LargeVolcanicEruptions,
	database = {Tlön},
	title = {Large volcanic eruptions},
	abstract = {What is the problem? Large volcanic eruptions, though rare, could have an extremely negative humanitarian impact. What are possible interventions? Based on current knowledge and technology, large eruptions cannot be prevented, but further research may improve the prediction of eruptions and allow for some mitigation of humanitarian impacts. Who else is working on it? Although scientific organizations fund some basic research on volcanic eruptions, we are not aware of any organizations explicitly working to reduce the humanitarian risk from large eruptions.},
	langid = {english},
	url = {https://www.openphilanthropy.org/research/cause-reports/volcanoes},
	journaltitle = {Open Philanthropy},
	author = {{Open Philanthropy}},
	date = {2013-06},
	file = {~/Google Drive/library-pdf/OpenPhilanthropy2013LargeVolcanicEruptions.pdf}
}

@online{OpenPhilanthropy2013ResearchCrimeIncarceration,
	database = {Tlön},
	title = {Research on crime, incarceration and cannabis
                  regulation},
	abstract = {Good Ventures, with input from GiveWell, awarded a grant of \$245,000 to the Washington Office on Latin America in December 2013 to support research projects on crime, incarceration and cannabis regulation to be led by Mark Kleiman. The grant is part of our exploration of criminal justice reform and drug policy reform in the United States, two areas we’ve prioritized for deeper investigation through learning grants. The grant is also meant to take advantage of what we see as a timely opportunity to study the implementation and effects of marijuana legalization in Washington and Colorado.},
	langid = {english},
	url = {https://www.openphilanthropy.org/focus/us-policy/criminal-justice-reform/research-crime-incarceration-and-cannabis-regulation},
	journaltitle = {Open Philanthropy},
	author = {{Open Philanthropy}},
	date = {2013-12},
	file = {~/Google Drive/library-pdf/OpenPhilanthropy2013ResearchCrimeIncarceration.pdf}
}

@online{OpenPhilanthropy2013TreatmentAnimalsIndustrial,
	database = {Tlön},
	title = {Treatment of animals in industrial agriculture},
	abstract = {What is the problem? Industrial agriculture in the United States involves billions of animals each year. The information we’ve seen suggests that these animals are often treated in ways that may cause extreme suffering over the course of their lives. What are possible interventions? Efforts to address the harms of industrial agriculture on animals typically focus on advocacy to individuals (to reduce their meat consumption), corporations (to reduce consumption or improve animal welfare conditions), or governments (to ban particular practices deemed especially harmful), though there are a number of other potential activities as well. Who else is working on it? Although the overall field of animal welfare receives a large amount of support from donors, relatively little funding appears to go to addressing the significant impacts of industrial agriculture on animal welfare.},
	langid = {english},
	url = {https://www.openphilanthropy.org/research/cause-reports/treatment-animals-industrial-agriculture},
	journaltitle = {Open Philanthropy},
	author = {{Open Philanthropy}},
	date = {2013-09},
	file = {~/Google Drive/library-pdf/OpenPhilanthropy2013TreatmentAnimalsIndustrial.pdf}
}

@online{OpenPhilanthropy2014PepperdineUniversityWashington,
	database = {Tlön},
	title = {Pepperdine University — Washington {THC} monitoring},
	abstract = {Good Ventures awarded a grant of \$150,000 to Pepperdine University to support a project to study the potential impact of marijuana legalization on use of both marijuana and other illicit drugs, led by Professor Angela Hawken.},
	langid = {english},
	url = {https://www.openphilanthropy.org/focus/us-policy/miscellaneous/pepperdine-university-washington-thc-monitoring},
	journaltitle = {Open Philanthropy},
	author = {{Open Philanthropy}},
	date = {2014-04},
	file = {~/Google Drive/library-pdf/OpenPhilanthropy2014PepperdineUniversityWashington.pdf}
}

@online{OpenPhilanthropy2015GeomagneticStorms,
	database = {Tlön},
	title = {Geomagnetic storms},
	abstract = {What is the problem? A severe solar storm might have the potential to shut down power grids on a continental scale for months. Who is already working on it? Power companies, transformer makers, insurers, and governments all have an interest in protecting the grid from geomagnetic storms. As far as we know, there is little philanthropic involvement in this issue. What could a new philanthropist do? The grid can be protected through hardening and through the installation of ground-induced current blocking devices that would prevent the currents generated by a geomagnetic storm from flowing through the grid. A philanthropist could fund further research on the threat posed by geomagnetic storms or on mitigation possibilities, fund advocacy for dealing with the threat, or directly fund mitigation.},
	langid = {english},
	url = {https://www.openphilanthropy.org/research/cause-reports/geomagnetic-storms},
	journaltitle = {Open Philanthropy},
	author = {{Open Philanthropy}},
	date = {2015-07},
	file = {~/Google Drive/library-pdf/OpenPhilanthropy2015GeomagneticStorms.pdf}
}

@online{OpenPhilanthropy2015GovernanceSolarRadiation,
	database = {Tlön},
	title = {Governance of solar radiation management},
	abstract = {What is the problem? Solar radiation management is a type of geoengineering that aims to cool the earth by reflecting sunlight away from it. As a category, solar radiation management appears to be both riskier and closer to being ready for use than other types of geoengineering. However, there are currently no specific systems in place to govern research into solar radiation management, or its deployment at any scale. Whether or not solar radiation management turns out to be safe and beneficial, we believe improved governance would make it more likely that decisions about research and deployment of the technology are made wisely and in the interests of humanity as a whole. What are possible interventions? A philanthropist interested in supporting the governance of solar radiation management could fund research into possible approaches to governance, or encourage discussion of and education about this issue among decision-makers and the general public. Who else is working on it? Funding for governance initiatives is limited, and comes mostly from government-funded agencies. We believe there is currently little private philanthropy in this area.},
	langid = {english},
	url = {https://www.openphilanthropy.org/research/cause-reports/SRM-governance},
	journaltitle = {Open Philanthropy},
	author = {{Open Philanthropy}},
	date = {2015-10},
	file = {~/Google Drive/library-pdf/OpenPhilanthropy2015GovernanceSolarRadiation.pdf}
}

@online{OpenPhilanthropy2015NuclearWeaponsPolicy,
	database = {Tlön},
	title = {Nuclear weapons policy},
	abstract = {What is the problem? Nuclear risks range in magnitude from an accident at a nuclear power plant to an individual detonation to a regional or global nuclear war. Our investigation has focused on the risks from nuclear war, which, while unlikely, would have a catastrophic global impact. What are possible interventions? A philanthropist could fund research or advocacy aimed at reducing nuclear arsenals, preventing nuclear proliferation, securing nuclear materials from terrorists, or attempting to more directly prevent the use of nuclear weapons in a conflict (e.g. by working with civil society actors to reduce the risk of conflict). A funder could also raise awareness about risks from nuclear weapons in general by working with media or educators, or through grassroots advocacy. Who else is working on it? Several major U.S. foundations fund approximately \$30 million/year of work on nuclear weapons issues, with most of this work supporting U.S.-based policy research and graduate/post-graduate education, some advocacy, and “track II diplomacy” (i.e. meetings between nuclear policy analysts and current and former government officials, often from different states). We do not have an estimate of funding from other non-profits in the space, but the Nuclear Threat Initiative has an annual budget of \$17-18 million and is not primarily funded by foundations. The U.S., other governments, and the International Atomic Energy Agency spend much larger amounts of money managing risks from nuclear weapons. We see work on nuclear weapons policy outside of the U.S. and U.S.-based advocacy as the largest potential gaps in the field, with the former gap being larger, but also harder for a U.S.-based philanthropist to fill.},
	langid = {english},
	url = {https://www.openphilanthropy.org/research/cause-reports/nuclear-weapons-policy},
	journaltitle = {Open Philanthropy},
	author = {{Open Philanthropy}},
	date = {2015-09},
	file = {~/Google Drive/library-pdf/OpenPhilanthropy2015NuclearWeaponsPolicy.pdf}
}

@online{OpenPhilanthropy2016AlcoholTaxation,
	database = {Tlön},
	title = {Alcohol taxation},
	abstract = {What is the problem? The Centers for Disease Control and Prevention reports that excessive drinking causes tens of thousands of deaths and costs society hundreds of billions of dollars every year in the United States. What are possible interventions? We focus on alcohol excise taxes in this investigation. The evidence suggests that alcohol consumption is sensitive to price changes, and so alcohol excise taxes are likely to decrease consumption and related social costs. In general, the value of taxes on alcohol has eroded in the past few decades. However, some state-wide campaigns to raise alcohol taxes have been successful in recent years. A funder could support a variety of research or advocacy efforts at the state or federal level to encourage alcohol tax increases. Who else is working on it? A number of organizations support work related to alcohol abuse prevention and treatment, but our understanding is that few focus on alcohol tax policy, and we are not aware of any major active funders in the area. Some research is being conducted by the US government and other institutions.},
	langid = {english},
	url = {https://www.openphilanthropy.org/research/cause-reports/alcohol-taxation},
	journaltitle = {Open Philanthropy},
	author = {{Open Philanthropy}},
	date = {2016-03},
	file = {~/Google Drive/library-pdf/OpenPhilanthropy2016AlcoholTaxation.pdf}
}

@online{OpenPhilanthropy2016CenterAppliedRationality,
	database = {Tlön},
	title = {Center for applied rationality — General support},
	abstract = {We decided to write about this grant in order to share our thinking about this grantee, as many of our supporters are familiar with the organization. This page is a summary of the reasoning behind our decision to recommend the grant; it was not written by the grant investigator(s).},
	langid = {english},
	url = {https://www.openphilanthropy.org/giving/grants/center-applied-rationality-general-support},
	journaltitle = {Open Philanthropy},
	author = {{Open Philanthropy}},
	date = {2016-09},
	file = {~/Google Drive/library-pdf/OpenPhilanthropy2016CenterAppliedRationality.pdf}
}

@online{OpenPhilanthropy2016CenterGlobalDevelopment,
	database = {Tlön},
	title = {Center for global development — General support 2016},
	abstract = {The Center for Global Development (CGD) is a think tank that conducts research on and promotes improvements to rich-world policies that affect the global poor. We have generally been impressed with CGD’s work, and view the organization (which we have funded before) as being highly aligned with our values.

Following previous grants, CGD approached us about increasing our level of general operating support. CGD outlined its overall funding situation—including a gap between its current and desired level of unrestricted funding—and a set of activities that it would potentially use new unrestricted funding to support, which generally struck us as promising.

We reviewed CGD’s track record by examining several of its largest and most concrete claimed successes in somewhat more detail, and based on our review, we would guess that there have been multiple occasions in which CGD’s work had a causal impact on decisions involving billions of dollars aimed at helping the global poor. It is difficult to trace the impact of CGD’s involvement through to clear humanitarian outcomes, but we guess that it has had a positive impact many times its total spending.

We see a strong case for providing a high level of general operating support to a well-aligned group with a compelling track record and promising-seeming plans for what to do with further funding. Accordingly, the Open Philanthropy Project decided to recommend a \$3 million grant to CGD in general operating support, paid out annually over the next three years.},
	langid = {english},
	url = {https://www.openphilanthropy.org/focus/global-health-and-development/miscellaneous/center-global-development-general-support-2016},
	journaltitle = {Open Philanthropy},
	author = {{Open Philanthropy}},
	date = {2016-02},
	file = {~/Google Drive/library-pdf/OpenPhilanthropy2016CenterGlobalDevelopment.pdf}
}

@online{OpenPhilanthropy2016FederalTaxReform,
	database = {Tlön},
	title = {Federal tax reform},
	abstract = {What is the problem? The federal tax system in the U.S. is inefficient, overly complicated, unlikely to be able to cover rising federal expenditures in the long run, and may constrain economic growth. What are possible interventions? Fundamental income tax reforms, including shifting the tax base to consumption or broadening the income tax base by eliminating many tax expenditures, may increase rates of economic growth and help address long-run fiscal issues. Smaller adjustments to federal tax policy may also have substantial benefits. These reforms face several political obstacles, and we do not have a strong sense of how additional funding would be able to create policy change. Who else is working on it? Federal tax reform efforts attract significant attention from many think tanks and foundations, including the Tax Policy Center and the Peter G. Peterson Foundation, amongst others.},
	langid = {english},
	url = {https://www.openphilanthropy.org/research/cause-reports/policy/federal-tax-reform},
	journaltitle = {Open Philanthropy},
	author = {{Open Philanthropy}},
	date = {2016-03},
	file = {~/Google Drive/library-pdf/OpenPhilanthropy2016FederalTaxReform.pdf;~/Google Drive/library-pdf/OpenPhilanthropy2016FederalTaxReform.pdf}
}

@online{OpenPhilanthropy2016FoundationNationalInstitutesa,
	database = {Tlön},
	title = {Foundation for the national institutes of health —
                  Working group on malaria gene drive testing path},
	abstract = {We decided to write about this grant in order to share the rationale for our interest in gene drives. This page is a summary of the reasoning behind our decision to recommend the grant; it was not written by the grant investigator(s).},
	langid = {english},
	url = {https://www.openphilanthropy.org/focus/scientific-research/miscellaneous/foundation-national-institutes-health-working-group},
	journaltitle = {Open Philanthropy},
	author = {{Open Philanthropy}},
	date = {2016-07},
	file = {~/Google Drive/library-pdf/OpenPhilanthropy2016FoundationNationalInstitutesa.pdf}
}

@online{OpenPhilanthropy2016HumaneLeagueCorporate,
	database = {Tlön},
	title = {The Humane League — corporate cage-free campaigns},
	abstract = {The Humane League is an animal advocacy organization that engages in a variety of activities, including campaigning for corporations to end the use of cages to confine egg-laying hens. We believe that corporate cage-free campaigns are a particularly effective method for reducing animal suffering, and that The Humane League has played an important role in the success of many of these campaigns in the past. We think it is likely that the effectiveness of The Humane League’s corporate campaign model will scale as it expands. Based on these considerations, the Open Philanthropy Project recommended a grant of \$1 million over two years to The Humane League (THL) to support its campaigns for corporate cage-free egg reform.},
	langid = {english},
	url = {https://www.openphilanthropy.org/focus/us-policy/farm-animal-welfare/humane-league-corporate-cage-free-campaigns},
	journaltitle = {Open Philanthropy},
	author = {{Open Philanthropy}},
	date = {2016-02},
	file = {~/Google Drive/library-pdf/OpenPhilanthropy2016HumaneLeagueCorporate.pdf}
}

@online{OpenPhilanthropy2016Policy,
	database = {Tlön},
	title = {U.S. policy},
	abstract = {One of the questions I’ve been focused on is “what sorts of activities can one fund in order to have an influence on policy?”.},
	langid = {english},
	url = {https://www.openphilanthropy.org/focus/us-policy},
	journaltitle = {Open Philanthropy},
	author = {{Open Philanthropy}},
	date = 2016,
	file = {~/Google Drive/library-pdf/OpenPhilanthropy2016Policy.pdf}
}

@online{OpenPhilanthropy2016UCBerkeleyCenter,
	database = {Tlön},
	title = {{UC} berkeley — Center for human-compatible {AI}
                  (2016)},
	abstract = {The Open Philanthropy Project recommended a grant of \$5,555,550 over five years to UC Berkeley to support the launch of a Center for Human-Compatible Artificial Intelligence (AI), led by Professor Stuart Russell. We believe the creation of an academic center focused on AI safety has significant potential benefits in terms of establishing AI safety research as a field and making it easier for researchers to learn about and work on this topic.},
	langid = {english},
	url = {https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-center-human-compatible-ai},
	journaltitle = {Open Philanthropy},
	author = {{Open Philanthropy}},
	date = {2016-08},
	file = {~/Google Drive/library-pdf/OpenPhilanthropy2016UCBerkeleyCenter.pdf}
}

@online{OpenPhilanthropy2017CenterAppliedRationality,
	database = {Tlön},
	title = {Center for applied rationality — European summer
                  program on rationality},
	abstract = {The Open Philanthropy Project recommended a grant of \$340,000 over two years to the Center for Applied Rationality (CFAR) to support its European Summer Program on Rationality (ESPR), a two-week summer workshop for about 40 mathematically gifted students aged 16-19. ESPR is partly modeled after CFAR’s Summer Program on Applied Rationality and Cognition (SPARC), which we have previously funded. The program teaches a curriculum that includes a variety of topics, usually connected to science, technology, engineering, and/or math. We are excited about this grant because we expect that ESPR will orient participants to problems that we believe to be high impact, and may lead them to increase their positive impact on the world.},
	langid = {english},
	url = {https://www.openphilanthropy.org/giving/grants/center-applied-rationality-european-summer-program-rationality},
	journaltitle = {Open Philanthropy},
	author = {{Open Philanthropy}},
	date = {2017-09},
	file = {~/Google Drive/library-pdf/OpenPhilanthropy2017CenterAppliedRationality.pdf}
}

@online{OpenPhilanthropy2017JPALSupportImmunization,
	database = {Tlön},
	title = {J-{PAL} — Support for immunization incentives {RCTs}},
	abstract = {Good Ventures made two gifts of \$100,000 each to the Abdul Latif Jameel Poverty Action Lab (J-PAL) at the Massachusetts Institute of Technology (MIT) to support two randomized controlled trials (RCTs) in India and Pakistan that will test whether providing non-cash incentives increases child immunization rates. See GiveWell’s review of J-PAL for more about its activities and to follow its progress. See GiveWell’s writeup of this grant for more details.},
	langid = {english},
	url = {https://www.openphilanthropy.org/focus/global-health-and-development/miscellaneous/j-pal-support-immunization-incentives-rcts},
	journaltitle = {Open Philanthropy},
	author = {{Open Philanthropy}},
	date = {2017-03},
	file = {~/Google Drive/library-pdf/OpenPhilanthropy2017JPALSupportImmunization.pdf}
}

@online{OpenPhilanthropy2017MachineIntelligenceResearch,
	database = {Tlön},
	title = {Machine Intelligence Research Institute — General
                  support (2017)},
	abstract = {The Open Philanthropy Project recommended a grant of \$3,750,000 over three years to the Machine Intelligence Research Institute (MIRI) for general support. MIRI plans to use these funds for ongoing research and activities related to reducing potential risks from advanced artificial intelligence, one of our focus areas.

This grant represents a renewal of and increase to our \$500,000 grant recommendation to MIRI in 2016, which we made despite strong reservations about their research agenda, detailed here. In short, we saw value in MIRI’s work but decided not to recommend a larger grant at that time because we were unconvinced of the value of MIRI’s research approach to AI safety relative to other research directions, and also had difficulty evaluating the technical quality of their research output. Additionally, we felt a large grant might signal a stronger endorsement from us than was warranted at the time, particularly as we had not yet made many grants in this area.},
	url = {https://www.openphilanthropy.org/grants/machine-intelligence-research-institute-general-support-2017/},
	journaltitle = {Open Philanthropy},
	author = {{Open Philanthropy}},
	urldate = {2022-06-22},
	date = {2017-10-16},
	langid = {english},
	file = {~/Google Drive/library-pdf/OpenPhilanthropy2017MachineIntelligenceResearch.pdf;~/Google Drive/library-html/machine-intelligence-research-institute-general-support-2017.html}
}

@online{OpenPhilanthropy2017TargetMalariaGene,
	database = {Tlön},
	title = {Target Malaria — gene drives for malaria control},
	abstract = {The Open Philanthropy Project recommended a grant of \$17,500,000 to Target Malaria over four years to help the project develop and prepare for the potential deployment of gene drive technologies to help eliminate malaria in Sub-Saharan Africa, if feasible, ethical, safe, approved by the regulatory authorities, and supported by the affected communities. Target Malaria is also a grantee of the Bill \& Melinda Gates Foundation. This grant will support training and outreach programs, research into the potential ecological effects of releasing gene drives, operational development, regulatory support, and an unrestricted funding reserve.},
	langid = {english},
	url = {https://www.openphilanthropy.org/focus/scientific-research/miscellaneous/target-malaria-general-support},
	journaltitle = {Open Philanthropy},
	author = {{Open Philanthropy}},
	date = {2017-05},
	file = {~/Google Drive/library-pdf/OpenPhilanthropy2017TargetMalariaGene.pdf}
}

@online{OpenPhilanthropy2018MachineIntelligenceResearch,
	database = {Tlön},
	title = {Machine Intelligence Research Institute — {AI} Safety
                  Retraining Program},
	abstract = {The Open Philanthropy Project recommended a grant of \$150,000 to the Machine Intelligence Research Institute (MIRI) to support its artificial intelligence safety (AI) retraining project. MIRI intends to use these funds to provide stipends, structure, and guidance to promising computer programmers and other technically proficient individuals who are considering transitioning their careers to focus on potential risks from advanced artificial intelligence. MIRI believes the stipends will make it easier for aligned individuals to leave their jobs and focus full-time on safety. MIRI expects the transition periods to range from three to six months per individual.},
	url = {https://www.openphilanthropy.org/grants/machine-intelligence-research-institute-ai-safety-retraining-program/},
	journaltitle = {Open Philanthropy},
	author = {{Open Philanthropy}},
	urldate = {2022-06-22},
	date = {2018-06-27},
	langid = {english},
	file = {~/Google Drive/library-pdf/OpenPhilanthropy2018MachineIntelligenceResearch.pdf;~/Google Drive/library-html/machine-intelligence-research-institute-ai-safety-retraining-program.html}
}

@online{OpenPhilanthropy2019JohnsHopkinsCenter,
	database = {Tlön},
	title = {Johns hopkins center for health security —
                  Biosecurity, global health security , and global
                  catastrophic risks (2019)},
	abstract = {The Open Philanthropy Project recommended a grant of \$20,192,755 over three years to the Johns Hopkins Center for Health Security (CHS) to support work on biosecurity, global catastrophic risks posed by pathogens, and other work related to CHS’s mission, and to support the Emerging Leaders in Biosecurity Initiative. CHS plans to use these funds to continue to conduct policy research and continue to build communications and advocacy capacity.},
	langid = {english},
	url = {https://www.openphilanthropy.org/focus/global-catastrophic-risks/biosecurity/center-health-security-biosecurity-health-security-gcrs-2019},
	journaltitle = {Open Philanthropy},
	author = {{Open Philanthropy}},
	date = {2019-09},
	file = {~/Google Drive/library-pdf/OpenPhilanthropy2019JohnsHopkinsCenter.pdf}
}

@online{OpenPhilanthropy2020MachineIntelligenceResearch,
	database = {Tlön},
	title = {Machine Intelligence Research Institute — General
                  support (2020)},
	abstract = {Open Philanthropy recommended a grant of \$7,703,750 to the Machine Intelligence Research Institute (MIRI) for general support. MIRI plans to use these funds for ongoing research and activities related to reducing potential risks from advanced artificial intelligence, one of our focus areas.

This follows our February 2019 support. While we see the basic pros and cons of this support similarly to what we’ve presented in past writeups on the matter, our ultimate grant figure was set by the aggregated judgments of our committee for effective altruism support, described in more detail here.},
	url = {https://www.openphilanthropy.org/grants/machine-intelligence-research-institute-general-support-2020/},
	journaltitle = {Open Philanthropy},
	author = {{Open Philanthropy}},
	urldate = {2022-06-22},
	date = {2020-04-10},
	langid = {english},
	file = {~/Google Drive/library-pdf/OpenPhilanthropy2020MachineIntelligenceResearch.pdf;~/Google Drive/library-html/machine-intelligence-research-institute-general-support-2020.html}
}

@online{OpenPhilanthropy2022CauseExplorationPrizes,
	database = {Tlön},
	author = {{Open Philanthropy}},
	abstract = {Though legal systems around the globe differ widely, they can generally be separated into two categories - common law systems and civil law systems. There are a wide range of differences between the two, but most important for this essay’s purposes are the differences in how both the court roles differ and how new laws are created in practical terms.
In civil law systems such as France, Spain, Germany, and Japan, the judge takes on the role of an investigator who brings charges and establishes facts by cross examining witnesses in addition to deciding the case. However, in common law systems such as the United Kingdom, the United States, India, and Canada (kind of - they use both), the courts function in much more of an adversarial manner. Two sides must argue their case against one another before a judge (and sometimes jury), who act as a neutral decision-maker. It is highly oratory, and much more of the investigation, information, fact-finding, and evidence presentation is left to lawyers and expert witnesses. This is relevant to cause exploration because it means though it has its advantages, this system means that often the person with the most money (and therefore best lawyers and legal resources) has the biggest advantage, which tips the odds of court victory in favour of those who have access to larger funds. As a result, it is possible for philanthropists to affect the outcome of court cases by strategically donating to legal funds.
But why do this?.
Another difference between civil law systems and common law systems is the difference in what the consequences of court decisions mean. Generally, in civil law systems the legislation and codified laws are of primary importance in what constitutes ‘law’. In common law systems, however, the results of court cases evolve and decide the interpretation of law. In oversimplified terms, in common law systems judges create new law via their decisions on cases.
When we take these two differences into account, we can see that philanthropists can affect the law by donating to particular individual cases where they feel there is a positive difference to be made. Because these laws can change the situation for hundreds of millions of people, they’re an attractive (if underused) potential cause area for philanthropic exploration. This is especially true as legal institutions such as legal aid are increasingly eroded - at-risk people such as the poor, disabled, vulnerable, and victims of the powerful increasingly are unable to shape new laws to protect themselves due to the cost overheads. Many examples of current/near-term suffering and long-term suffering or existential risks could be tackled by providing future victims the means of preventing their own suffering in this manner.},
	title = {Cause exploration prizes: reducing suffering and long
                  term risk in common law nations via strategic case law
                  funding},
	url = {https://forum.effectivealtruism.org/posts/C6urjnDeKjHGwf2M3/cause-exploration-prizes-reducing-suffering-and-long-term},
	shorttitle = {Cause Exploration Prizes},
	journaltitle = {Effective Altruism Forum},
	urldate = {2022-07-12},
	date = {2022-07-12},
	langid = {english},
	file = {~/Google Drive/library-pdf/OpenPhilanthropy2022CauseExplorationPrizes.pdf;~/Google Drive/library-html/OpenPhilanthropy2022CauseExplorationPrizes.html}
}

@online{OpenPhilanthropy2023CalidadDelAire,
	database = {Tlön},
	date = {2023},
	title = {La calidad del aire en Asia meridional},
	author = {{Open Philanthropy}},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {OpenPhilanthropy2021SouthAsianAir}
}

@online{OpenWingAlliance2022Organizations,
	url = {https://openwingalliance.org/organizations},
	date = {2022},
	langid = {english},
	database = {Tlön},
	journaltitle = {Open Wing Alliance},
	title = {Organizations},
	author = {{Open Wing Alliance}},
	timestamp = {2023-07-19 20:39:37 (GMT)}
}

@thesis{Ord2005ConsequentialismDecisionProcedures,
	database = {Tlön},
	title = {Consequentialism and decision procedures},
	langid = {english},
	institution = {University of Oxford},
	type = {{BPhil} thesis},
	author = {Ord, Toby},
	date = {2005-06},
	note = {Genre: {BPhil} thesis},
	file = {~/Google Drive/library-pdf/Ord2005ConsequentialismDecisionProcedures.pdf}
}

@thesis{Ord2009ActionApplyingConsequentialism,
	database = {Tlön},
	title = {Beyond action: Applying consequentialism to decision
                  making and motivation},
	langid = {english},
	url = {https://ethos.bl.uk/OrderDetails.do?uin=uk.bl.ethos.508590},
	institution = {University of Oxford},
	type = {{PhD} thesis},
	author = {Ord, Toby},
	date = {2009-03},
	note = {Genre: {PhD} thesis},
	file = {~/Google Drive/library-pdf/Ord2009ActionApplyingConsequentialism.pdf}
}

@online{Ord2013AidWorksAverage,
	database = {Tlön},
	file = {~/Google Drive/library-pdf/Ord2013AidWorksAverage.pdf},
	abstract = {There is considerable controversy about whether foreign aid helps poor countries, with several prominent critics arguing that it doesn’t. Dr. Ord shows that these critics have only reached this conclusion because they have failed to count the biggest successes of aid, such as the eradication of smallpox, which have been in the sphere of global health rather than economic growth. These health successes have often been neglected in analysis of aid because they have only made up a small proportion of aid spending. When we look at the impact of this spending, though, we see that it the big wins have been so utterly vast that they more than justify all aid spending to date.},
	date = {2013-02},
	langid = {english},
	journaltitle = {Oxford Forum for International Development},
	title = {Aid works (on average)},
	author = {Ord, Toby},
	timestamp = {2023-11-06 18:01:03 (GMT)}
}

@online{Ord2016MoralProgressAnd,
	journaltitle = {EAGlobal},
	date = {2016-08-05},
	abstract = {The effective altruism community has grown tremendously since it began in 2009. In the opening keynote of {EA} Global: San Francisco 2016, Toby Ord and Will {MacAskill} discuss this history and consider what might happen in the movement's future.
Starts at 2:22.
Hello Effective Altruism Global. I say this every single year and every single year it’s true. Welcome to the largest gathering of the Effective Altruism Community that the world has ever seen. It's my absolute delight to introduce the conference on the past, present, and future of effective altruism. I'm going to introduce Dr. Toby Ord, who will talk about the past of effective altruism before talking, myself, about what's happening at the moment and what the future of effective altruism might look like.},
	database = {Tlön},
	author = {Ord, Toby and {MacAskill}, William},
	langid = {english},
	timestamp = {2023-06-19 18:50:20 (GMT)},
	title = {Moral progress and "cause X"},
	url = {https://forum.effectivealtruism.org/posts/9K8Yiv9Fdm7XNsmCm/moral-progress-and-cause-x},
	urldate = {2023-06-19}
}

@online{Ord2023CapituloRiesgosFuturos,
	database = {Tlön},
	date = {2023},
	title = {Capítulo "Riesgos futuros" de El Precipicio,
                  Introducción y sección “Pandemias"},
	author = {Ord, Toby},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Ord20205FutureRisks}
}

@online{Ord2023MomentoOportunoDe,
	date = {2023},
	title = {El momento oportuno de trabajar para reducir el riesgo
                  existencial},
	database = {Tlön},
	author = {Ord, Toby},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Ord2014TimingLabourAimed}
}

@online{Ord2023ProgresoMoralCausa,
	database = {Tlön},
	date = {2023},
	title = {Progreso moral y "causa X"},
	author = {Ord, Toby and {MacAskill}, William},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Ord2016MoralProgressAnd}
}

@online{Ord2023RiesgosExistencialesPara,
	database = {Tlön},
	keywords = {riesgo existencial},
	date = {2023},
	langid = {spanish},
	author = {Ord, Toby},
	title = {Riesgos existenciales para la humanidad},
	translator = {Tlön},
	translation = {Ord2020ExistentialRisksHumanity}
}

@online{Ortiz-Ospina2023SaludGlobal,
	database = {Tlön},
	date = {2023},
	title = {Salud global},
	author = {Ortiz-Ospina, Esteban},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Ortiz-Ospina2016GlobalHealth}
}

@online{Ostman2009PoorMeatEater,
	database = {Tlön},
	title = {The poor meat eater problem},
	langid = {english},
	url = {https://felicifia.github.io/thread/214.html},
	journaltitle = {Felicifia},
	author = {Östman, Jesper},
	date = {2009-10-26},
	file = {~/Google Drive/library-pdf/Ostman2009PoorMeatEater.pdf}
}

@online{Ozden2021AnalysisEAFunding,
	database = {Tlön},
	title = {Analysis of {EA} funding within animal welfare from
                  2019-2021},
	abstract = {FTX Foundation Group, a collective of companies owned by Sam Bankman-Fried, launches the FTX Climate program. The program's goals are to neutralize FTX's impact on the environment, fund research and public policy initiatives related to climate change, support the development of carbon removal solutions and the funding of special climate-related projects. FTX has already purchased carbon offsets, committed funding to research organizations focused on climate change, and pledged \$1 million to permanent carbon capture and storage. – AI-generated abstract.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/6H9QGZkdMzDEdKNCX/analysis-of-ea-funding-within-animal-welfare-from-2019-2021-1},
	journaltitle = {Effective Altruism Forum},
	author = {Ozden, James},
	urldate = {2022-05-14},
	date = {2021-09-27},
	file = {~/Google Drive/library-pdf/Ozden2021AnalysisEAFunding.pdf;~/Google Drive/library-html/analysis-of-ea-funding-within-animal-welfare-from-2019-2021-1.html}
}

@online{PRNewswire2021FTXFoundationGroup,
	database = {Tlön},
	title = {The {FTX} Foundation Group launches the {FTX} Climate
                  program},
	abstract = {Effective Altruism began as a movement focused on finding ways to eradicate human and nonhuman animal suffering. As it progressed, its focus shifted towards safeguarding humanity from existential risks, particularly those stemming from artificial intelligence (AI) safety concerns. However, there is a new discourse within the movement known as Longtermism, which argues that the movement should prioritize the well-being of far-future generations by directing resources towards the development of superintelligence, superlongevity, and superhappiness through germline engineering. This article discusses the implications of adopting Longtermism, proposing that a focus on eradicating existing sources of suffering, such as animal cruelty and mental illness, is still essential. It also highlights the importance of considering different ethical frameworks, such as negative utilitarianism, when evaluating the long-term implications of technological advancements. – AI-generated abstract.},
	url = {https://www.prnewswire.com/news-releases/the-ftx-foundation-group-launches-the-ftx-climate-program-301342380.html},
	journaltitle = {{PRNewswire}},
	author = {{PRNewswire}},
	urldate = {2021-10-25},
	date = {2021-07-27},
	langid = {english},
	file = {~/Google Drive/library-html/the-ftx-foundation-group-launches-the-ftx-climate-program-301342380.html}
}

@online{Pace2020HaveLockdownsBeen,
	database = {Tlön},
	title = {Have the lockdowns been worth it?},
	abstract = {The article focuses on raising several individual considerations which are relevant for thinking about the question “Have the pandemic lockdowns, in general, been worth it?” It does not attempt to take a position on the overall question, but rather provides relevant facts, data, and information about specific factors that are relevant to the discussion. Some of the factors considered include the potential loss of QALYs in the UK, the impact on GDP in Germany, the potential development of a vaccine within a specific timeframe, the trade-offs between lockdown and non-lockdown scenarios in terms of years of life lost, and the potential long-term benefits of remote work arrangements. – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/cFfZgN62AizXWf872/have-the-lockdowns-been-worth-it},
	journaltitle = {{LessWrong}},
	author = {Pace, Ben},
	date = {2020-10-12},
	file = {~/Google Drive/library-pdf/Pace2020HaveLockdownsBeen.pdf}
}

@mvbook{Parfit2011WhatMatters,
	database = {Tlön},
	location = {Oxford},
	langid = {english},
	title = {On what matters},
	isbn = {978-0-19-926592-3},
	series = {The Berkeley Tanner lectures},
	volumes = 3,
	publisher = {Oxford University Press},
	author = {Parfit, Derek},
	date = 2011,
	file = {~/Google Drive/library-pdf/Parfit2011WhatMattersVolume.pdf;~/Google
                  Drive/library-pdf/Parfit2011WhatMattersVolume.pdf;~/Google Drive/library-pdf/Parfit2017WhatMatters.pdf}
}

@online{Parfit2015FullAddress,
	database = {Tlön},
	title = {Full address},
	langid = {english},
	url = {https://www.youtube.com/watch?v=xTUrwO9-B_I},
	journaltitle = {Oxford Union},
	author = {Parfit, Derek},
	urldate = {2022-01-09},
	date = {2015-10-10}
}

@online{Parikh2022EALibrarianCEA,
	database = {Tlön},
	title = {{EA} Librarian: {CEA} wants to help answer your {EA}
                  questions!},
	abstract = {We want to help answer your questions about {EA}!. ‘Dumb’ questions or questions that you would usually be embarrassed to ask are especially encouraged.
You can ask questions using the forum question feature with the {EA} Librarian tag, using this form or via the discussion channel in certain fellowship groups.
Feel encouraged to ask questions without reading the rest of this post!.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/Y5pja6CrRmPPFkMKj/ea-librarian-cea-wants-to-help-answer-your-ea-questions},
	shorttitle = {{EA} Librarian},
	journaltitle = {Effective Altruism Forum},
	author = {Parikh, Caleb},
	urldate = {2022-04-04},
	date = {2022-01-17},
	file = {~/Google Drive/library-pdf/Parikh2022EALibrarianCEA.pdf;~/Google Drive/library-html/ea-librarian-cea-wants-to-help-answer-your-ea-questions.html}
}

@Article{Parsons2002AxiologicalActualism,
	database = {Tlön},
	url = {https://www.tandfonline.com/doi/abs/10.1093/ajp/80.2.137},
	pages = {137–147},
	langid = {english},
	number = {2},
	volume = {80},
	date = {2002},
	journaltitle = {Australasian Journal of Philosophy},
	title = {Axiological actualism},
	author = {Parsons, Josh},
	timestamp = {2023-07-12 15:31:39 (GMT)}
}

@online{Patel2023CarlShulmanPt,
	database = {Tlön},
	rating = {10},
	author = {Patel, Dwarkesh},
	date = {2023-06-26},
	langid = {english},
	timestamp = {2023-07-01 12:19:23 (GMT)},
	title = {Carl Shulman (Pt 2) - {AI} Takeover, Bio \& Cyber
                  Attacks, Detecting Deception, \& Humanity's Far
                  Future},
	url = {https://www.dwarkeshpatel.com/p/carl-shulman-2},
	urldate = {2023-07-01}
}

@online{Pawntoe42020CollapseRiskCrisk,
	database = {Tlön},
	title = {On collapse risk (C-risk)},
	abstract = {The mainstream of effective altruist thought has undergone a conceptual evolution towards longtermist notions and “safeguarding the future”. Nick Bostrom’s work on existential risks laid some of the foundations of "longtermism" and the valuing of the continued existence of the human race as a generator of positive experiences. This has led to a focus on the avoidance of existential risks, which I believe to be relatively narrow given the range of competing risks that exist.
Among those the most salient and of greatest cost to the welfare of the long-term future is a class of potential events that result in the reversion of human existence to a degraded scientific and technological understanding. Largely due to advances in these fields, we have developed healthcare and social systems to allow for a much greater quality of life than in the past, so such a regression would also imply a significant loss of life and/or quality adjusted life years ({QALYs}) during the period of reduction and for many future generations. Many more {QALYs} will have been missed due to the failure of our society to progress at its current rate.
This article will make a case for why we should care about why collapse risks (C-risks) are a neglected area in {EA} thinking and priority based on the expected outcomes from a longtermist perspective.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/4QumMPTNSkKqTsNGh/on-collapse-risk-c-risk},
	journaltitle = {Effective Altruism Forum},
	author = {{Pawntoe4}},
	date = {2020-01-02},
	file = {~/Google Drive/library-pdf/Pawntoe42020CollapseRiskCrisk.pdf}
}

@online{Pearce2016CompassionateBiologyHow,
	database = {Tlön},
	title = {Compassionate biology: How {CRISPR}-based "gene
                  drives" could cheaply, rapidly and sustainably reduce
                  suffering throughout the living world},
	abstract = {CRISPR-based gene drives can potentially be used to reduce suffering in all species of sexually reproducing organisms. Gene drives can be used to “fix” the level of suffering endured by members of entire free-living and sexually reproducing species at minimal cost and inconvenience to humans, without waiting for a full-blown transition to mature nanotechnology and post-human superintelligence. Using gene drives to optimize well-being could massively amplify the effects of even exceedingly weak and fitful human benevolence towards non-human animals. Helping an entire species of small fast-reproducing vertebrate within the time-frame of two or three decades, and an entire species of sexually fast-reproducing insect or marine invertebrate within two to three years – AI-generated abstract.},
	langid = {english},
	url = {https://www.hedweb.com/gene-drives/index.html},
	journaltitle = {{BLTC} Research},
	author = {Pearce, David},
	date = 2016,
	eventdate = {2021},
	file = {~/Google Drive/library-pdf/Pearce2016CompassionateBiologyHow.pdf}
}

@online{Pearce2021WhatDoesDavid,
	database = {Tlön},
	title = {What does David Pearce think of longtermism in the
                  effective altruist movement?},
	abstract = {The article explores the tension in the effective altruist movement between the traditional focus on near-term issues (such as poverty reduction or animal welfare) and the emergent focus on long-termism, with particular attention to the implications for suffering reduction. It argues that while long-termism can provide a valuable perspective, neglecting near-term suffering in favor of potential future benefits risks causing harm and should be balanced with a continued commitment to alleviating existing suffering. Additionally, the article highlights diverging views within the effective altruist community, particularly between those who prioritize minimizing suffering (negative utilitarianism) and those who prioritize maximizing pleasure (classical utilitarianism), and suggests that these differing ethical frameworks should be acknowledged and discussed openly. – AI-generated abstract.},
	url = {https://www.quora.com/What-does-David-Pearce-think-of-Longtermism-in-the-Effective-Altruist-movement/answer/David-Pearce-18},
	journaltitle = {Quora},
	author = {Pearce, David},
	urldate = {2022-03-27},
	date = {2021-12-11},
	langid = {english},
	file = {~/Google Drive/library-pdf/Pearce2021WhatDoesDavid.pdf;~/Google Drive/library-html/David-Pearce-18.html}
}

@article{Pega2017UnconditionalCashTransfers,
	database = {Tlön},
	title = {Unconditional cash transfers for reducing poverty and
                  vulnerabilities: effect on use of health services and
                  health outcomes in low- and middle-income countries},
	langid = {english},
	volume = 11,
	issn = 14651858,
	url = {https://www.cochranelibrary.com/cdsr/doi/10.1002/14651858.CD011135.pub2/related-content/podcast/51175/pt},
	doi = {10.1002/14651858.CD011135.pub2},
	pages = {1–137},
	number = 1,
	journaltitle = {Cochrane database of systematic reviews},
	author = {Pega, Frank and Liu, Sze Yan and Walter, Stefan and
                  Pabayo, Roman and Saith, Ruhi and Lhachimi, Stefan K},
	date = {2017-11-15},
	file = {~/Google Drive/library-pdf/Pega2017UnconditionalCashTransfers.pdf}
}

@article{Pettit1986RestrictiveConsequentialism,
	author = {Pettit, Philip and Brennan, Geoffrey},
	title = {Restrictive consequentialism},
	volume = {64},
	number = {4},
	pages = {438–455},
	doi = {10.1080/00048408612342631},
	url = {http://www.tandfonline.com/doi/abs/10.1080/00048408612342631},
	database = {Tlön},
	date = {1986-12},
	issn = {0004-8402, 1471-6828},
	journaltitle = {Australasian Journal of Philosophy},
	langid = {english},
	shortjournal = {Australasian Journal of Philosophy},
	timestamp = {2023-07-13 10:23:41 (GMT)},
	urldate = {2023-07-13}
}

@thesis{Phillips2015RationalFaithStudy,
	database = {Tlön},
	title = {Rational faith:A study of the effective altruism
                  movement},
	langid = {english},
	url = {https://id.lib.harvard.edu/alma/990147960900203941/catalog},
	institution = {Harvard University},
	type = {{PhD} thesis},
	author = {Phillips, Nick},
	date = 2015
}

@online{Picon2022BigListCause,
	database = {Tlön},
	title = {Big list of cause candidates: January 2021–March 2022
                  update},
	abstract = {Since Big List of Cause Candidates was posted, there have been many other posts proposing new neglected or less-discussed cause areas.  This post aims to record and organize any such causes. The original post will be shortly updated, but I thought it could be useful to list them separately.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/DBhuERvKRgGpLiK6T/big-list-of-cause-candidates-january-2021-march-2022-update},
	shorttitle = {Big List of Cause Candidates},
	journaltitle = {Effective Altruism Forum},
	author = {Picón, Leonardo},
	urldate = {2022-04-30},
	date = {2022-04-30},
	file = {~/Google Drive/library-pdf/Picon2022BigListCause.pdf;~/Google Drive/library-html/big-list-of-cause-candidates-january-2021-march-2022-update.html}
}

@online{Pilz2022GermansOpinionsTranslations,
	database = {Tlön},
	title = {Germans' opinions on translations of "longtermism":
                  survey results},
	abstract = {I conducted a survey with 32 respondents via Mechanical Turk about which word for “longtermism” Germans prefer. Most think that the English word works fine and should not necessarily be translated. However, the word “Zukunftsschutz” (future protection) gets the highest ranking, even higher than “Longtermism” itself. As the word has a very positive connotation in German, it might work well for outreach purposes.},
	url = {https://forum.effectivealtruism.org/posts/35D5NxW8uzDQkHWH8/germans-opinions-on-translations-of-longtermism-survey},
	shorttitle = {Germans' Opinions on Translations of "longtermism"},
	journaltitle = {Effective Altruism Forum},
	author = {Pilz, Konstantin},
	urldate = {2022-06-29},
	date = {2022-06-27},
	langid = {english},
	file = {~/Google Drive/library-pdf/Pilz2022GermansOpinionsTranslations.pdf;~/Google Drive/library-html/germans-opinions-on-translations-of-longtermism-survey.html}
}

@book{Pinker2019SentidoDelEstilo,
	translation = {Pinker2014SenseStyleThinking},
	database = {Tlön},
	langid = {spanish},
	author = {Pinker, Steven},
	date = {2019},
	publisher = {Capitán Swing},
	address = {Madrid},
	isbn = {9788494966798},
	title = {El sentido del estilo: la guía de escritura del pensador del siglo XXI},
	timestamp = {2023-05-25 21:13:58 (GMT)}
}

@online{Pinker2021TypicalEssayScott,
	database = {Tlön},
	title = {A typical essay by Scott Alexander is deeper, better
                  reasoned, better referenced, more original, and
                  wittier than 99\% of the opinion pieces in {MSM}},
	langid = {english},
	url = {https://twitter.com/sapinker/status/1360787817459253251},
	journaltitle = {Twitter},
	author = {Pinker, Steven},
	date = {2021-02-14}
}

@online{Piper2020NextDeadlyPathogen,
	database = {Tlön},
	title = {The next deadly pathogen could come from a rogue
                  scientist. Here’s how we can prevent that},
	abstract = {{DNA} synthesis is driving innovations in biology. But gaps in screening mechanisms risk the release of deadly pathogens.},
	url = {https://www.vox.com/future-perfect/2020/2/11/21076585/dna-synthesis-assembly-viruses-biosecurity},
	journaltitle = {Vox},
	author = {Piper, Kelsey},
	urldate = {2022-01-22},
	date = {2020-02-11},
	langid = {english},
	file = {~/Google Drive/library-pdf/Piper2020NextDeadlyPathogen.pdf;~/Google Drive/library-html/dna-synthesis-assembly-viruses-biosecurity.html}
}

@online{Piper2020ThisCharityGiving,
	database = {Tlön},
	title = {This charity is giving cash directly to Americans
                  suffering during the coronavirus crisis},
	abstract = {{GiveDirectly} gives people money. Now it’s doing it for the coronavirus.},
	url = {https://www.vox.com/future-perfect/2020/3/20/21186007/coronavirus-pandemic-donate-help-cash-benefits},
	journaltitle = {Vox},
	author = {Piper, Kelsey},
	urldate = {2022-01-22},
	date = {2020-03-20},
	langid = {english},
	file = {~/Google Drive/library-pdf/Piper2020ThisCharityGiving.pdf;~/Google Drive/library-html/coronavirus-pandemic-donate-help-cash-benefits.html}
}

@online{Piper2021VoxFuturePerfect,
	database = {Tlön},
	title = {Vox's Future Perfect is hiring},
	abstract = {I work at Vox's Future Perfect, a grant-funded effective-altruism-inspired vertical focused on high impact writing on some of the world's most important problems. Some of the stories I'm proudest of over the last few years have covered the case for catastrophic {AI} risk, the rise of plant-based meat, progress towards malaria vaccines, and the case for better biosafety and pandemic preparedness.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/86FEL7BASERouFbRb/vox-s-future-perfect-is-hiring},
	journaltitle = {Effective Altruism Forum},
	author = {Piper, Kelsey},
	urldate = {2021-07-16},
	date = {2021-07-16},
	file = {~/Google Drive/library-pdf/Piper2021VoxFuturePerfect.pdf;~/Google Drive/library-html/vox-s-future-perfect-is-hiring.html}
}

@online{Piper2023AcercaDeIdeas,
	database = {Tlön},
	date = {2023},
	title = {Acerca de las ideas “marginales”},
	author = {Piper, Kelsey},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Piper2019FringeIdeas}
}

@online{Piper2023PorQueDebemos,
	database = {Tlön},
	keywords = {inteligencia artificial, seguridad de la IA},
	date = {2023},
	langid = {spanish},
	author = {Piper, Kelsey},
	title = {Por qué debemos pensar seriamente que la inteligencia
                  artificial es una amenaza para la humanidad},
	translator = {Humarán, Aurora},
	translation = {Piper2018CaseTakingAI}
}

@online{Plant2016EffectiveAltruismOverlooking,
	database = {Tlön},
	title = {Is effective altruism overlooking human happiness and
                  mental health? I argue it is},
	abstract = {version: Mental illness is probably much worse than poverty or physical illness. Interventions which change how people think - i.e. reduce mental illness and increase happiness - may be more cost-effective ways of increasing happiness than {AMF} or Give Directly. I outline some new opportunities {EA} should look into.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/A278Run59FenZ4kae/is-effective-altruism-overlooking-human-happiness-and-mental},
	journaltitle = {Effective Altruism Forum},
	author = {Plant, Michael},
	date = {2016-06-22},
	file = {~/Google Drive/library-pdf/Plant2016EffectiveAltruismOverlooking.pdf}
}

@online{Plant2021DonatingMoneyBuying,
	database = {Tlön},
	title = {Donating money, buying happiness: new meta-analyses
                  comparing the cost-effectiveness of cash transfers and
                  psychotherapy in terms of subjective well-being},
	abstract = {In order to do as much good as possible, we need to compare how much good different things do in a single ‘currency’. At the Happier Lives Institute ({HLI}), we believe the best approach is to measure the effects of different interventions in terms of ‘units’ of subjective well-being (e.g. self-reports of happiness and life satisfaction). In this post, we discuss our new research comparing the cost-effectiveness of psychotherapy to cash transfers.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/mY4pZSwvFCDsjorJX/donating-money-buying-happiness-new-meta-analyses-comparing},
	shorttitle = {Donating money, buying happiness},
	journaltitle = {Effective Altruism Forum},
	author = {Plant, Michael and {McGuire}, Joel},
	urldate = {2022-05-15},
	date = {2021-10-25},
	file = {~/Google Drive/library-pdf/Plant2021DonatingMoneyBuying.pdf;~/Google Drive/library-html/donating-money-buying-happiness-new-meta-analyses-comparing.html}
}

@online{Plant2021ResearchAgendaContexta,
	database = {Tlön},
	title = {Research agenda and context},
	url = {https://www.happierlivesinstitute.org/uploads/1/0/9/9/109970865/research_agenda_and_contextv3.1.pdf},
	journaltitle = {Happier Lives Institute},
	author = {Plant, Michael},
	urldate = {2022-03-25},
	date = {2021-04},
	langid = {english}
}

@Collection{PreussischeAkademiederWissenschaften1911KantsGesammelteSchriften,
	database = {Tlön},
	volume = {4},
	editor = {{Preussische Akademie der Wissenschaften}},
	langid = {english},
	address = {Berlin},
	publisher = {Georg Reiner},
	date = {1911},
	title = {Kant's Gesammelte Schriften},
	timestamp = {2023-08-12 15:50:43 (GMT)}
}

@book{Pringle-Pattison1907PhilosophicalRadicalsOther,
	database = {Tlön},
	location = {Edinburgh},
	langid = {english},
	title = {The philosophical radicals and other essays},
	publisher = {William Blackwood and sons},
	author = {Pringle-Pattison, A. Seth},
	date = 1907,
	file = {~/Google Drive/library-pdf/Pringle-Pattison1907PhilosophicalRadicalsOther.pdf}
}

@online{ProbablyGood2023ImpactoMarginal,
	database = {Tlön},
	date = {2023},
	title = {Impacto marginal},
	author = {{Probably Good}},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {ProbablyGood2022MarginalImpact}
}

@online{Pummer2020HilaryGreavesTalks,
	database = {Tlön},
	title = {Hilary Greaves talks longtermism},
	abstract = {Philosopher Hilary Greaves sits down with Theron Pummer to offer her controversial viewpoint on poverty, global health, extinction, artificial superintelligence, and which problems are most worth our effort to avert.},
	langid = {english},
	url = {https://youtu.be/d1jMlb8E08k},
	journaltitle = {{CEPPA} Chats},
	author = {Pummer, Theron},
	date = {2020-04-29}
}

@online{QualiaResearchInstitute2022DavidPearceAndres,
	database = {Tlön},
	title = {David Pearce and Andrés Gómez Emilsson chat about the
                  nature of reality},
	abstract = {David Pearce and Andrés Gómez Emilsson chat about the nature of reality. Along the way, they discuss the early days of David's HedWeb, the Abolitionist Project, the Three Supers of Transhumanism (Superhappiness, Superintelligence, and Superlongevity), philosophy and history of science, the nature of intelligence, field theories of consciousness, anesthesia, empathogens, anti-tolerance drugs, and much more.},
	langid = {english},
	url = {https://www.youtube.com/watch?v=0GE4AdG1Z2g},
	journaltitle = {{YouTube}},
	author = {{Qualia Research Institute}},
	urldate = {2022-09-07},
	date = {2022-09-07}
}

@online{Quirk2020WeReLincoln,
	database = {Tlön},
	title = {We're Lincoln Quirk \& Ben Kuhn from Wave, {AMA}!},
	abstract = {Wave is a startup building mobile money—a way for people in developing countries to access financial services like savings and money transfer if they can't afford, or live too far away from, traditional banks. Lincoln is co-founder and head of product; Ben is an early engineer and {CTO}. We've both been part of the {EA} community since {\textasciitilde}2011 (in fact, we met through {NYC} {EA}), and work on Wave for {EA} reasons.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/kBSgtcrbBXEwLyYRD/we-re-lincoln-quirk-and-ben-kuhn-from-wave-ama},
	journaltitle = {Effective Altruism Forum},
	author = {Quirk, Lincoln and Kuhn, Ben},
	urldate = {2022-02-21},
	date = {2020-10-27},
	file = {~/Google Drive/library-pdf/Quirk2020WeReLincoln.pdf;~/Google Drive/library-html/we-re-lincoln-quirk-and-ben-kuhn-from-wave-ama.html}
}

@online{Rafferty2020IntroducingLEEPLead,
	database = {Tlön},
	title = {Introducing {LEEP}: Lead Exposure Elimination Project},
	abstract = {We are excited to announce the launch of Lead Exposure Elimination Project ({LEEP}), a new {EA} organisation incubated by Charity Entrepreneurship. Our mission is to reduce lead poisoning, which causes significant disease burden worldwide. We aim to achieve this by advocating for lead paint regulation in countries with large and growing burdens of lead poisoning from paint.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/fd96FtLFACeAshqJP/introducing-leep-lead-exposure-elimination-project},
	journaltitle = {Effective Altruism Forum},
	author = {Rafferty, Jack and Coulter, Lucia},
	date = {2020-10-06},
	file = {~/Google Drive/library-pdf/Rafferty2020IntroducingLEEPLead.pdf}
}

@online{Rafferty2023PresentacionDeLeep,
	database = {Tlön},
	date = {2023},
	title = {Presentación de LEEP: Lead Exposure Elimination
                  Project},
	author = {Rafferty, Jack and Coulter, Lucia},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Rafferty2020IntroducingLEEPLead}
}

@article{Railton1988HowThinkingAbout,
	author = {Railton, Peter},
	database = {Tlön},
	langid = {english},
	title = {How Thinking About Character and Utilitarianism Might
                  Lead To Rethinking the Character of Utilitarianism},
	volume = {13},
	pages = {398–416},
	doi = {10.1111/j.1475-4975.1988.tb00135.x},
	url = {http://www.pdcnet.org/oom/service?url_ver=Z39.88-2004&rft_val_fmt=&rft.imuse_id=msp_1988_0013_0398_0416&svc_id=info:www.pdcnet.org/collection},
	date = {1988},
	issn = {0363-6550},
	journaltitle = {Midwest Studies in Philosophy},
	timestamp = {2023-07-11 19:32:30 (GMT)},
	urldate = {2023-07-11}
}

@online{RaisingforEffectiveGiving2020HumaneSlaughterAssociation,
	database = {Tlön},
	title = {Humane Slaughter Association},
	abstract = {Approval voting, a novel voting system allowing voters to vote for as many candidates as they desire, is being implemented in St. Louis mayoral elections as a nonpartisan method. By enabling voters to surpass the predominant single-vote system in favor of proportional representation, it seeks to eliminate electoral limitations and provide more chances for minority representation. However, it remains unclear whether voters will fully utilize this option or continue with their customary voting habits. The potential impact of approval voting on candidate strategies, polarization reduction, and overall voter engagement remains uncertain, presenting an opportunity for an empirical study in its highest-scale implementation thus far. – AI-generated abstract.},
	url = {https://reg-charity.org/recommended-charities/humane-slaughter-association/},
	journaltitle = {Raising for Effective Giving},
	author = {{Raising for Effective Giving}},
	urldate = {2022-05-02},
	date = 2020,
	langid = {english},
	file = {~/Google Drive/library-pdf/RaisingforEffectiveGiving2020HumaneSlaughterAssociation.pdf;~/Google Drive/library-html/humane-slaughter-association.html}
}

@online{Rakich2021StLouisVoters,
	database = {Tlön},
	title = {In St. Louis, voters will get to vote for as many
                  candidates as they want},
	abstract = {Approval voting is a new electoral system where voters can vote for as many candidates as they like, and the two candidates with the most votes advance to a head-to-head general election. It aims to remove the zero-sum game of politics and give a fairer shot to third parties by allowing voters to express their preferences more accurately. Approval voting has been implemented in a few jurisdictions, including St. Louis, where it is being used for the first time in a mayoral election. The success or failure of approval voting in St. Louis will be closely watched as it could potentially be a model for other cities and states looking to reform their electoral systems. – AI-generated abstract.},
	url = {https://fivethirtyeight.com/features/in-st-louis-voters-will-get-to-vote-for-as-many-candidates-as-they-want/},
	journaltitle = {{FiveThirtyEight}},
	author = {Rakich, Nathaniel},
	urldate = {2021-11-21},
	date = {2021-03-01},
	langid = {english},
	file = {~/Google Drive/library-pdf/Rakich2021StLouisVoters.pdf;~/Google Drive/library-html/in-st-louis-voters-will-get-to-vote-for-as-many-candidates-as-they-want.html}
}

@article{Rapoport1957LewisRichardsonMathematical,
	database = {Tlön},
	title = {Lewis F. Richardson's mathematical theory of war},
	volume = 1,
	issn = {0731-4086},
	url = {https://journals.sagepub.com/doi/10.1177/002200275700100301},
	doi = {10.1177/002200275700100301},
	pages = {249–299},
	number = 3,
	journaltitle = {Journal of Conflict Resolution},
	shortjournal = {Conflict Resolution},
	author = {Rapoport, Anatol},
	urldate = {2021-08-27},
	date = {1957-09},
	langid = {english},
	file = {~/Google Drive/library-pdf/Rapoport1957LewisRichardsonMathematical.pdf}
}

@online{Rauker2022IdeaRedteamingFellowships,
	database = {Tlön},
	title = {Idea: Red-teaming fellowships},
	abstract = {A red team is an independent group that challenges an organization or movement in order to improve it. Red teaming is the practice of using red teams.
Participants get prepared for and work on a red-teaming challenge, focussed on specific arguments, ideas, cause areas or programmes of organizations.This will:i) Allow fellows to dig deeply into a topic of their interest, encourage critical thinking and reduce deferring to an “{EA} consensus”ii) Increase scrutiny of key {EA} ideas and lead to clarifying discussionsiii) Allow fellows to show useful research skills to potential {employersFactors} to make this succesful are a good selection process for fellows, well-informed {EAs} to supervise the fellowship and connection to {EA} research organizations.
Edit: {HT} to Linch, who came up with the same idea in a shortform half a year ago and didn't follow Aaron's advice of turning it into a top-level post!.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/obHA95otPtDNSD6MD/idea-red-teaming-fellowships},
	shorttitle = {Idea},
	journaltitle = {Effective Altruism Forum},
	author = {Räuker, Max and Geh, Jesper and Grimm, Simon and
                  Muehlhaeuser, Yannick},
	urldate = {2022-02-04},
	date = {2022-02-02},
	file = {~/Google Drive/library-pdf/Ra2022IdeaRedteamingFellowships.pdf;~/Google Drive/library-html/idea-red-teaming-fellowships.html}
}

@online{Ray2020PointClarificationInfohazard,
	database = {Tlön},
	title = {A point of clarification on infohazard terminology},
	abstract = {Information hazards, or any kind of information that could be harmful in some manner, have been discussed and characterized by several researchers, including Bostrom in 2011. This work further delineates the different types of information hazards and proposes introducing the term “cognitohazard” to refer to the information that may harm the person who knows it, which is a narrower definition compared to Bostrom’s broader concept of information hazard. The author proposes distinguishing the two concepts to prevent confusion in discussions about information hazards. – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/Rut5wZ7qyHoj3dj4k/a-point-of-clarification-on-infohazard-terminology},
	journaltitle = {{LessWrong}},
	author = {Ray, Georgia},
	date = {2020-02-02},
	file = {~/Google Drive/library-pdf/Ray2020PointClarificationInfohazard.pdf}
}

@online{Reese2016WhyAnimalsMatter,
	database = {Tlön},
	title = {Why animals matter for effective altruism},
	abstract = {This is an introductory article for people currently learning more about effective altruism. “Effective altruism is about answering one simple question: how can we use our resources to help others the most?” This article explains why it’s so important to consider the wellbeing of animals when choosing where to donate, what career to take, and making other decisions from an effective altruism perspective. The primary author of this post is Jacy Reese with very helpful feedback from Jacob Funnell, Larissa Rowe, Eddie Dugal, Caspar Österheld, Hussein Al-Kaf, Kelly Witwicki Faddegon, Oliver Austin, Harrison Nathan, Eric Day, Robin Raven, James Snowden, Daniel Dorado, Kieran Greig, Oscar Horta, and Sanjay Joshi. The ideas expressed in the article come from a variety of sources, including people not mentioned above. Note that the views expressed in this article do not necessarily represent the views of these people or their respective employers.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/ch5fq73AFn2Q72AMQ/why-animals-matter-for-effective-altruism},
	journaltitle = {Effective Altruism Forum},
	author = {Reese, Jacy},
	date = {2016-08-22},
	file = {~/Google Drive/library-pdf/Reese2016WhyAnimalsMatter.pdf}
}

@Book{Reino2003GreenBookAppraisal,
	database = {Tlön},
	file = {~/Google Drive/library-pdf/Reino2003GreenBookAppraisal.pdf},
	langid = {english},
	address = {London},
	publisher = {TSO},
	url = {https://webarchive.nationalarchives.gov.uk/ukgwa/20080305121602/http://www.hm-treasury.gov.uk/media/3/F/green_book_260907.pdf},
	date = {2003},
	title = {The Green Book: Appraisal and Evaluation in Central
                  Government},
	author = {Tesoro del Reino Unido},
	timestamp = {2023-06-08 07:59:12 (GMT)}
}

@book{Reus2019IndustrieBienPhilanthropie,
	database = {Tlön},
	title = {L ’industrie du bien: philanthropie, altruisme
                  efficace et altruisme efficace animalier},
	langid = {french},
	author = {Reus, Estiva},
	date = 2019,
	file = {~/Google Drive/library-pdf/Reus2019IndustrieBienPhilanthropie.pdf}
}

@online{Rice2017TimelineAISafety,
	database = {Tlön},
	title = {Timeline of {AI} safety},
	langid = {english},
	url = {https://timelines.issarice.com/wiki/Timeline_of_AI_safety},
	journaltitle = {Timelines Wiki},
	author = {Rice, Issa},
	date = {2017-07-25},
	file = {~/Google Drive/library-pdf/Rice2017TimelineAISafety.pdf}
}

@online{Rice2017TimelineMachineIntelligence,
	database = {Tlön},
	title = {Timeline of Machine Intelligence Research Institute},
	langid = {english},
	url = {https://timelines.issarice.com/wiki/Timeline_of_Machine_Intelligence_Research_Institute},
	journaltitle = {Timelines Wiki},
	author = {Rice, Issa},
	date = {2017-06-30},
	file = {~/Google Drive/library-pdf/Rice2017TimelineMachineIntelligence.pdf}
}

@online{Rice2018TimelineWeiDai,
	database = {Tlön},
	title = {Timeline of Wei Dai publications},
	langid = {english},
	url = {https://timelines.issarice.com/wiki/Timeline_of_Wei_Dai_publications},
	journaltitle = {Timelines Wiki},
	author = {Rice, Issa and Sánchez, Sebastián},
	date = {2018-02-15}
}

@book{Riddell2007DoesForeignAid,
	database = {Tlön},
	location = {Oxford},
	langid = {english},
	title = {Does foreign aid really work?},
	isbn = {978-0-19-153776-9},
	publisher = {Oxford University Press},
	author = {Riddell, Roger C.},
	date = 2007,
	file = {~/Google Drive/library-pdf/Riddell2007DoesForeignAid.pdf}
}

@online{Rieber2015BittersweetnessReplaceability,
	database = {Tlön},
	title = {The bittersweetness of replaceability},
	abstract = {When I first became interested in effective altruist ideas, I was inspired by the power of one person to make a difference: "the life you can save", as Peter Singer puts it. I planned to save lives by becoming an infectious disease researcher. So the first time I read about replaceability was a gut punch, when I realized that it would be futile for me to pursue a highly competitive biomedical research position, especially given that I was mediocre at wet lab research. In the best case, I would obtain a research position but would merely be replacing other applicants who were roughly as good as me. I became deeply depressed for a time, as I finished a degree that was no longer useful to me. After I graduated, I embarked on the frustrating, counterintuitive challenge of making a difference in a world in which everybody's replaceable.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/CsRyvwwop66dgo5gu/the-bittersweetness-of-replaceability},
	journaltitle = {Effective Altruism Forum},
	author = {Rieber, Lila},
	date = {2015-07-12},
	file = {~/Google Drive/library-pdf/Rieber2015BittersweetnessReplaceability.pdf}
}

@article{Roberts2002NewWayOf,
	author = {Roberts, by M. A.},
	title = {A new way of doing the best that we can: person‐based
                  consequentialism and the equality problem},
	volume = {112},
	number = {2},
	pages = {315–350},
	doi = {10.1086/324321},
	url = {https://www.journals.uchicago.edu/doi/10.1086/324321},
	database = {Tlön},
	date = {2002-01},
	issn = {0014-1704, 1539-297X},
	journaltitle = {Ethics},
	langid = {english},
	shortjournal = {Ethics},
	shorttitle = {A New Way of Doing the Best That We Can},
	timestamp = {2023-07-12 15:36:18 (GMT)},
	urldate = {2023-07-12}
}

@online{Robinson2022NewGrantmakingProgram,
	database = {Tlön},
	title = {New grantmaking program: supporting the effective
                  altruism community around Global Health and Wellbeing},
	abstract = {We are searching for a program officer to help us launch a new grantmaking program. The program would support projects and organizations in the effective altruism community (EA) with a focus on improving global health and wellbeing (GHW).},
	langid = {english},
	url = {https://www.openphilanthropy.org/blog/new-grantmaking-program-supporting-effective-altruism-community-around-global-health-and},
	shorttitle = {New grantmaking program},
	journaltitle = {Open Philanthropy},
	author = {Robinson, Zachary},
	urldate = {2022-04-01},
	date = {2022-02-22},
	file = {~/Google Drive/library-pdf/Robinson2022NewGrantmakingProgram.pdf;~/Google Drive/library-html/new-grantmaking-program-supporting-the-effective-altruism.html}
}

@online{Rodriguez2019HowBadWould,
	database = {Tlön},
	title = {How bad would nuclear winter caused by a {US}-Russia
                  nuclear exchange be?},
	abstract = {In this post, I quantify the severity of the nuclear winter we might expect to result from a nuclear war between the {US} and Russia (Guesstimate model here).
Researchers who have studied nuclear winter estimate that a nuclear war that produced between 50 and 150 teragrams of smoke would make agriculture nearly impossible, causing most people on Earth to starve to death and leaving humanity on the brink of extinction.
But most of the research into nuclear winter was done at the height of the Cold War when the {US} and Russian nuclear arsenals and nuclear policies looked quite different. I previously argued that the {US} and Russia would be more likely to target each others’ nuclear forces during a nuclear war, rather than target each others’ cities as they would have done during the Cold War. This makes a big difference in whether a {US}-Russia nuclear exchange would lead to a severe nuclear winter.
Nuclear attacks on cities would likely produce much more smoke than attacks on missile silos, military bases, and other nuclear arsenal targets. This is mainly because cities have much more flammable material to burn than the remote wildlands — mostly cropland and grasslands — that surround, for example, missile silos.
This leads me to conclude that a nuclear war between the {US} and Russia would likely produce closer to 31 teragrams of smoke (90\% confidence interval: 14 Tg to 68 Tg of smoke) — suggesting that nuclear winter is not as synonymous with {US}-Russia nuclear war as many effective altruists seem to assume. The {\textasciitilde}31 teragrams of smoke that would be vaulted into the atmosphere would undoubtedly produce severe climate effects, likely leading to food shortfalls and regional famines, and killing between 36\% and 96\% of the world population.
I think the finding points us toward being a bit more skeptical of the idea that some effective altruists seem to hold — that a nuclear war between the {US} and Russia would necessarily lead to a nuclear winter that posed a large risk of extinction. There’s about an 11\% chance that 50 Tg of smoke — the threshold at which the literature suggests the resulting nuclear winter would be catastrophic — are released into the atmosphere by a Russia-{US} nuclear war. To be clear, this 11\% risk is non-trivial, and it’s plausible that even a so-called nuclear autumn (the result of between {\textasciitilde}5 and {\textasciitilde}50 Tg of smoke) would pose some sort of x-risk.
As a final point, I’d like to emphasize that the nuclear winter is quite controversial (for example, see: Singer, 1985; Seitz, 2011; Robock, 2011; Coupe et al., 2019; Reisner et al., 2019; Pausata et al., 2016; Reisner et al., 2018; Also see the summary of the nuclear winter controversy in Wikipedia’s article on nuclear winter). Critics argue that the parameters fed into the climate models (like, how much smoke would be generated by a given exchange) as well as the assumptions in the climate models themselves (for example, the way clouds would behave) are suspect, and.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/pMsnCieusmYqGW26W/how-bad-would-nuclear-winter-caused-by-a-us-russia-nuclear},
	journaltitle = {Effective Altruism Forum},
	author = {Rodriguez, Luisa},
	date = {2019-06-20},
	file = {~/Google Drive/library-pdf/Rodriguez2019HowBadWould.pdf}
}

@online{Rodriguez2020WhatIsLikelihood,
	date = {2020-12-24},
	journaltitle = {Effective Altruism Forum},
	abstract = {In this post, I explore the probability that if various kinds of catastrophe caused civilizational collapse, this collapse would fairly directly lead to human extinction. I don’t assess the probability of those catastrophes occurring in the first place, the probability they’d lead to indefinite technological stagnation, or the probability that they’d lead to non-extinction existential catastrophes (e.g., unrecoverable dystopias). I hope to address the latter two outcomes in separate posts (forthcoming).
My analysis is organized into case studies: I take three possible catastrophes, defined in terms of the direct damage they would cause, and assess the probability that they would lead to extinction within a generation. There is a lot more someone could do to systematically assess the probability that a catastrophe of some kind would lead to human extinction, and what I’ve written up is certainly not conclusive. But I hope my discussion here can serve as a starting point as well as lay out some of the main considerations and preliminary results.
Note: Throughout this document, I’ll use the following language to express my best guess at the likelihood of the outcomes discussed:.
.
Case 1: I think it’s exceedingly unlikely that humanity would go extinct (within {\textasciitilde}a generation) as a direct result of a catastrophe that causes the deaths of 50\% of the world’s population, but causes no major infrastructure damage (e.g. damaged roads, destroyed bridges, collapsed buildings, damaged power lines, etc.) or extreme changes in the climate (e.g. cooling). The main reasons for this are:.
.
Although civilization’s critical infrastructure systems (e.g. food, water, power) might collapse, I expect that several billions of people would survive without critical systems (e.g. industrial food, water, and energy systems) by relying on goods already in grocery stores, food stocks, and fresh water sources.
After a period of hoarding and violent conflict over those supplies and other resources, I expect those basic goods would keep a smaller number of remaining survivors alive for somewhere between a year and a decade (which I call the grace period, following Lewis Dartnell’s The Knowledge).
After those supplies ran out, I expect several tens of millions of people to survive indefinitely by hunting, gathering, and practicing subsistence agriculture (having learned during the grace period any necessary skills they didn’t possess already).
.
Case 2: I think it’s very unlikely that humanity would go extinct as a direct result of a catastrophe that caused the deaths of 90\% of the world’s population (leaving 800 million survivors), major infrastructure damage, and severe climate change (e.g. nuclear winter/asteroid impact).
.
While I expect that millions would starve to death in the wake of something like a global nuclear winter, my best guess is that hundreds of thousands to hundreds of millions of the survivors of the initial catastrophe would be able to subsist using leftover food stocks and supplies, before eventually working out how to feed themselves through traditional agriculture (in regions where this was still possible) and fishing and/or modified agriculture (using methods that don’t rely on climate factors like warm temperatures and regular precipitation).
For catastrophes that.},
	database = {Tlön},
	author = {Rodriguez, Luisa},
	title = {What is the likelihood that civilizational collapse
                  would directly lead to human extinction (within
                  decades)?},
	url = {https://forum.effectivealtruism.org/posts/GsjmufaebreiaivF7/what-is-the-likelihood-that-civilizational-collapse-would},
	langid = {english},
	timestamp = {2023-07-21 09:20:47 (GMT)},
	urldate = {2023-07-21}
}

@online{Rodriguez2023CualEsProbabilidad,
	database = {Tlön},
	date = {2023},
	title = {¿Cuál es la probabilidad de que el colapso de la
                  civilización provoque de manera directa la extinción
                  humana (en cuestión de décadas)?},
	author = {Rodriguez, Luisa},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Rodriguez2022WhatLikelihoodThat}
}

@online{Rogers-Smith2022HowToPursue,
	database = {Tlön},
	file = {~/Google Drive/library-pdf/Rs2022HowToPursue.pdf},
	abstract = {This guide is written for people who are considering direct work on technical {AI} alignment. I expect it to be most useful for people who are not yet working on alignment, and for people who are already familiar with the arguments for working on {AI} alignment. If you aren’t familiar with the arguments for the importance of {AI} alignment, you can get an overview of them by reading Why {AI} alignment could be hard with modern deep learning (Cotra, 2021) and one of The Most Important Century Series (Karnofsky, 2021) and {AGI} Safety from First Principles (Ngo, 2019).},
	langid = {english},
	author = {{Roger-Smith}, Charlie},
	date = {2022-06-03},
	timestamp = {2023-02-15 21:48:24 (GMT)},
	title = {How to pursue a career in technical {AI} alignment},
	journaltitle = {Effective Altruism Forum},
	url = {https://forum.effectivealtruism.org/posts/7WXPkpqKGKewAymJf/how-to-pursue-a-career-in-technical-ai-alignment},
	urldate = {2023-02-15}
}

@online{Roodman2015ComingEarthWhat,
	database = {Tlön},
	title = {Coming down to earth: what if a big geomagnetic storm
                  does hit?},
	abstract = {I devoted the first three posts in this series to describing geomagnetic storms and assessing the odds that a Big One is coming. I concluded that the iconic Carrington superstorm of 1859 was neither as intense nor as overdue for an encore as some prominent analysts have suggested. (I suppose that’s unsurprising: those who say more-alarming things get more attention.) But my analysis is not certain. To paraphrase Churchill, the sun is a riddle, wrapped in a mystery, inside a corona. And great harm would flow from what I cannot rule out: a blackout spanning states and lasting months.

I shift in this post from whether the Big One is coming to what will happen if it does. And here, unfortunately, my facility with statistics does less good, for the top questions are now about power engineering: how grids and high-voltage transformers respond to planetary magnetic concussions.},
	langid = {english},
	url = {https://www.openphilanthropy.org/blog/coming-down-earth-what-if-big-geomagnetic-storm-does-hit},
	journaltitle = {Open Philanthropy},
	author = {Roodman, David},
	date = {2015-08-21},
	file = {~/Google Drive/library-pdf/Roodman2015ComingEarthWhat.pdf}
}

@online{Roodman2015GeomagneticStormsHistory,
	database = {Tlön},
	title = {Geomagnetic storms: history's surprising, if
                  tentative, reassurance},
	abstract = {My last post raised the specter of a geomagnetic storm so strong it would black out electric power across continent-scale regions for months or years, triggering an economic and humanitarian disaster. How likely is that? One relevant source of knowledge is the historical record of geomagnetic disturbances, which is what this post considers. In approaching the geomagnetic storm issue, I had read some alarming statements to the effect that global society is overdue for the geomagnetic “Big One.” So I was surprised to find reassurance in the past. In my view, the most worrying extrapolations from the historical record do not properly represent it.},
	langid = {english},
	url = {https://www.openphilanthropy.org/blog/geomagnetic-storms-historys-surprising-if-tentative-reassurance},
	journaltitle = {Open Philanthropy},
	author = {Roodman, David},
	date = {2015-07-02},
	file = {~/Google Drive/library-pdf/Roodman2015GeomagneticStormsHistory.pdf}
}

@online{Roodman2015GeomagneticStormsUsing,
	database = {Tlön},
	title = {Geomagnetic storms: using extreme value theory to
                  gauge the risk},
	abstract = {My last post examined the strength of certain major geomagnetic storms that occurred before the advent of the modern electrical grid, as well as a solar event in 2012 that could have caused a major storm on earth if it had happened a few weeks earlier or later. I concluded that the observed worst cases over the last 150+ years are probably not more than twice as intense as the major storms that have happened since modern grids were built, notably in 1982, 1989, and 2003.

But that analysis was in a sense informal. Using a branch of statistics called Extreme Value Theory (EVT), we can look more systematically at what the historical record tells us about the future. The method is not magic—it cannot reliably divine the scale of a 1000-year storm from 10 years of data—but through the familiar language of probability and confidence intervals it can discipline extrapolations with appropriate doses of uncertainty. This post brings EVT to geomagnetic storms.},
	langid = {english},
	url = {https://www.openphilanthropy.org/blog/geomagnetic-storms-using-extreme-value-theory-gauge-risk},
	journaltitle = {Open Philanthropy},
	author = {Roodman, David},
	date = {2015-07-13},
	file = {~/Google Drive/library-pdf/Roodman2015GeomagneticStormsUsing.pdf}
}

@article{Roodman2015RiskGeomagnetcStorms,
	database = {Tlön},
	title = {The risk of geomagnetc storms to the grid: A
                  preliminary review},
	langid = {english},
	volume = 3,
	issn = {1556-5068},
	url = {https://www.ssrn.com/abstract=3635861},
	doi = {10.2139/ssrn.3635861},
	journaltitle = {{SSRN} Electronic Journal},
	author = {Roodman, David},
	date = {2015-06-29},
	file = {~/Google Drive/library-pdf/Roodman2015RiskGeomagnetcStorms.pdf}
}

@online{Rose2020ImpactCertificatesHelp,
	database = {Tlön},
	title = {Do impact certificates help if you're not sure your
                  work is effective?},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/k8JnovPNiQHQQyvGu/do-impact-certificates-help-if-you-re-not-sure-your-work-is},
	journaltitle = {Effective Altruism Forum},
	author = {Rose, Eli},
	date = {2020-02-12},
	file = {~/Google Drive/library-pdf/Rose2020ImpactCertificatesHelp.pdf}
}

@online{Rosenfeld2022FreespendingEAMight,
	database = {Tlön},
	title = {Free-spending {EA} might be a big problem for optics
                  and epistemics},
	abstract = {The influx of {EA} funding is brilliant news, but it has also left many {EAs} feeling uncomfortable. I share this feeling of discomfort and propose two concrete concerns which I have recently come across. Optics: {EA} spending is often perceived as wasteful and self-serving, creating a problematic image which could lead to external criticism, outreach issues and selection effects. Epistemics: Generous funding has provided extrinsic incentives for being {EA}/longtermist which are exciting but also significantly increase the risks of motivated reasoning and make the movement more reliant on the judgement of a small number of grantmakers. I don’t really know what to do about this (especially since it’s overall very positive), so I give a few uncertain suggestions but mainly hope that others will have ideas and that this will at least serve as a call to vigilance in the midst of funding excitement.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/HWaH8tNdsgEwNZu8B/free-spending-ea-might-be-a-big-problem-for-optics-and},
	journaltitle = {Effective Altruism Forum},
	author = {Rosenfeld, George},
	urldate = {2022-04-15},
	date = {2022-04-12},
	file = {~/Google Drive/library-html/Free-spending EA might be
                  a big problem for optics and epistemics - EA
                  Forum:free-spending-ea-might-be-a-big-problem-for-optics-and.html;~/Google Drive/library-pdf/Rosenfeld2022FreespendingEAMight.pdf}
}

@online{Roser2021WhyWeNeed,
	database = {Tlön},
	title = {Why do we need to know about progress if we are
                  concerned about the world’s large problems?: The
                  mission of our world in data},
	abstract = {Why have we made it our mission to publish “research and data to make progress against the world’s largest problems”?.},
	langid = {english},
	url = {https://ourworldindata.org/problems-and-progress},
	journaltitle = {Our World in Data},
	author = {Roser, Max},
	date = {2021-06-07}
}

@online{Roser2023DesigualdadEconomicaGlobal,
	database = {Tlön},
	date = {2023},
	title = {Desigualdad económica global},
	author = {Roser, Max},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Roser2021GlobalEconomicInequality}
}

@online{Roser2023MundoEsHorrible,
	date = {2023},
	title = {El mundo es horrible. El mundo está mucho mejor. El
                  mundo puede estar mucho mejor.},
	database = {Tlön},
	author = {Roser, Max},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Roser2022WorldIsAwful}
}

@article{Routley1973IsThereNeed,
	database = {Tlön},
	author = {Routley, Richard},
	title = {Is there a need for a new, an environmental ethic},
	volume = {1},
	pages = {205–210},
	doi = {10.5840/wcp151973136},
	url = {https://www.pdcnet.org/pdc/bvdb.nsf/purchase?openform&fp=wcp15&id=wcp15_1973_0001_0205_0210},
	date = {1973-04-01},
	journaltitle = {Proceedings of the {XVth} World Congress of
                  Philosophy},
	langid = {english},
	timestamp = {2023-07-27 09:16:22 (GMT)},
	urldate = {2023-07-27}
}

@online{Rowe2020InsectsRaisedFood,
	database = {Tlön},
	title = {Insects raised for food and feed — global scale,
                  practices, and policy},
	abstract = {This research covers the scale of insect farming globally, including the number of insects farmed in different regions, sold live or slaughtered, or killed during the production process and not otherwise sold. I break down these overall numbers into estimates by region and broad taxonomic group. I include estimates, by species and region, of the number of days on farms that insects experience. I also review the regulations governing insect farming, the practices found on insect farms, welfare concerns we might have for insects on farms, and potential promising interventions for insect advocates.
This research specifically covers insects whose bodies are eaten in whole or powdered form for food and animal feed. It does not review insects farmed for a food product they produce (such as honey bees), nor does it include insects who have a food additive produced with a minor derivative of their bodies (such as cochineals). This research also does not cover wild insects collected for food or animal feed. Finally, this research does not cover annelids raised for fishing bait, though some of the insects sold live described in this report are likely used for fishing bait.
This research does not touch on insect sentience or moral status. Generally, I am working under the assumption that all the insects mentioned are sentient in a morally relevant way, though since this paper focuses only on scale, the question of insect sentience and its moral relevance does not play a further role outside motivating the research. For further research into these questions, see Rethink Priorities’ research into invertebrate welfare, and in particular, the analysis of fruit flies and ants, since they are insects currently impacted by farming.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/ruFmR5oBgqLgTcp2b/insects-raised-for-food-and-feed-global-scale-practices-and},
	journaltitle = {Effective Altruism Forum},
	author = {Rowe, Abraham},
	urldate = {2022-05-13},
	date = {2020-06-29},
	file = {~/Google Drive/library-pdf/Rowe2020InsectsRaisedFood.pdf;~/Google Drive/library-html/insects-raised-for-food-and-feed-global-scale-practices-and.html}
}

@online{Rowe2020ShouldLongtermistsMostly,
	database = {Tlön},
	title = {Should longtermists mostly think about animals?},
	abstract = {Animal welfare is frequently considered a short-term cause area within effective altruism. As an example of this, the {EA} Survey lists animal welfare as a separate cause area from the long-term future. A plausible reason for this conceptual separation is that most animal charities work on short-term issues, with perhaps the exception of wild animal welfare organizations.
However, separating animal welfare and longtermism as cause areas seems to neglect the fact that animals will likely continue to exist far into the future, and that if you think animals are moral patients then you ought to think that their welfare (or the lack thereof following their extinction) matters. Animal welfare should therefore be a consideration in evaluating the impact of existential risks and s-risks.
In fact, if one is a total or negative utilitarian, thinks that animal welfare is morally important, and buys arguments for strong longtermism, I argue that your primary focus ought to be on the welfare of wild animals over the very long-term. I argue that the vast majority of the value in preventing existential risks is either in preserving future animal life, or preserving human life in order to reduce wild animal suffering (depending on whether you are a total or negative-leaning utilitarian, and whether or not you think animals have on average net-positive lives). I also outline the beliefs that might lead one to dismiss wild animal welfare as the most important long-term cause area, and why I think that those beliefs are somewhat unreasonable to hold.
This is an exercise in imagining how many wild animals there are, and how many there will be. Even when I apply often used discounts to animal populations, my model suggests that animals overwhelmingly dominate total welfare considerations. The figures that I will present aren’t to be taken as exact estimates, but instead a demonstration of the degree to which animal welfare considerations should dominate human welfare for a total utilitarian, and potential scenarios in which that could change. There are probably lots of ways in which my figures and estimates could be improved, and I’d welcome any attempt to do so.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/W5AGTHm4pTd6TeEP3/should-longtermists-mostly-think-about-animals},
	shorttitle = {Should Longtermists Mostly Think About Animals?},
	journaltitle = {Effective Altruism Forum},
	author = {Rowe, Abraham},
	urldate = {2022-02-01},
	date = {2020-01-03},
	file = {~/Google Drive/library-pdf/Rowe2020ShouldLongtermistsMostly.pdf;~/Google Drive/library-html/should-longtermists-mostly-think-about-animals.html}
}

@online{Rowe2022CritiquesEAThat,
	database = {Tlön},
	title = {Critiques of {EA} that I want to read},
	abstract = {I’m interested in the {EA} red-teaming contest as an idea, and there are lots of interesting critiques I’d want to read. But I haven’t seen any of those written yet. I put together a big list of critiques of {EA} I’d be really interested in seeing come out of the contest. I personally would be interested in writing some of these, but don’t really have time to right now, so I am hoping that by sharing these, someone else will write a good version of them. I’d also welcome people to share other critiques they’d be excited to see written in the comments here.},
	url = {https://forum.effectivealtruism.org/posts/n3WwTz4dbktYwNQ2j/critiques-of-ea-that-i-want-to-read},
	journaltitle = {Effective Altruism Forum},
	author = {Rowe, Abraham},
	urldate = {2022-07-01},
	date = {2022-06-19},
	langid = {english},
	file = {~/Google Drive/library-pdf/Rowe2022CritiquesEAThat.pdf;~/Google Drive/library-html/critiques-of-ea-that-i-want-to-read.html}
}

@online{Rozendal2019EightHighlevelUncertainties,
	database = {Tlön},
	title = {Eight high-level uncertainties about global
                  catastrophic and existential risk},
	abstract = {I wanted to write a quick overview of overarching topics in global catastrophic and existential risk where we do not know much yet. Each of these topics deserves a lot of attention on their own, and this is simply intended as a non-comprehensive overview. I use the term ‘hazard’ to indicate an event that could lead to adverse outcomes, and the term ‘risk’ to indicate the product of a hazard’s probability times its negative consequences. Although I believe not all uncertainties are of equal importance (some might be more important by orders of magnitude), I discuss them in no particular order. Furthermore, the selection of uncertainties is the result of what has been on the forefront of my mind and does not reflect the 8 most important uncertainties.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/QjKRBcobCzeeerMbP/eight-high-level-uncertainties-about-global-catastrophic-and},
	journaltitle = {Effective Altruism Forum},
	author = {Rozendal, Siebe},
	date = {2019-11-28},
	file = {~/Google Drive/library-pdf/Rozendal2019EightHighlevelUncertainties.pdf}
}

@online{Ruhl2022AnnouncingFoundersPledge,
	database = {Tlön},
	title = {Announcing the Founders Pledge Global Catastrophic
                  Risks Fund},
	abstract = {The {GCR} Fund will build on Founders Pledge’s recent research into great power conflict and risks from frontier military and civilian technologies, with a special focus on international stability — a pathway that we believe shapes a number of the biggest risks facing humanity — and will work on:.
War between great powers, like a U.S.-China clash over Taiwan, or U.S.-Russia war;Nuclear war, especially emerging threats to nuclear stability, like vulnerabilities of nuclear command, control, and communications;Risks from artificial intelligence ({AI}), including risks from both machine learning applications (like autonomous weapon systems) and from transformative {AI};Catastrophic biological risks, such as naturally-arising pandemics, engineered pathogens, laboratory accidents, and the misuse of new advances in synthetic biology; {andEmerging} threats from new technologies and in new domains.
Moreover, the Fund will support field-building activities around the study and mitigation of global catastrophic risks, and methodological interventions, including new ways of studying these risks, such as probabilistic forecasting and experimental wargaming. The focus on international security is a current specialty, and we expect the areas of expertise of the fund to expand as we build capacity.},
	url = {https://forum.effectivealtruism.org/posts/ZJpPvxXdimPgxFp2j/announcing-the-founders-pledge-global-catastrophic-risks-1},
	journaltitle = {Effective Altruism Forum},
	author = {Ruhl, Christian},
	urldate = {2022-11-03},
	date = {2022-10-26},
	langid = {english},
	file = {~/Google Drive/library-pdf/Ruhl2022AnnouncingFoundersPledge.pdf;~/Google Drive/library-html/announcing-the-founders-pledge-global-catastrophic-risks-1.html}
}

@Article{Rupprecht2004OralVaccinationOf,
	database = {Tlön},
	title = {Oral vaccination of wildlife against rabies: Opportunities and challenges in prevention and control},
	langid = {english},
	date = {2004},
	pages = {173--84},
	volume = {119},
	journaltitle = {Developments in Biologicals},
	author = {Rupprecht, Charles and Hanlon, Cathleen and Slate, Dennis},
	timestamp = {2023-06-01 08:51:21 (GMT)}
}

@Book{Russell1946HistoryOfWestern,
	database = {Tlön},
	file = {~/Google Drive/library-pdf/Russell1946HistoryOfWestern.pdf},
	date = {1946},
	langid = {english},
	address = {London},
	publisher = {George Allen \& Unwin},
	title = {A history of western philosophy: and its connection with political and social circumstances from the earliest times to the present day},
	author = {Russell, Bertrand},
	timestamp = {2023-07-27 12:05:31 (GMT)}
}

@online{Ruthenis2022ReshapingAIIndustry,
	database = {Tlön},
	title = {Reshaping the {AI} industry},
	abstract = {This article proposes various strategic options to address the impending risks posed by artificial intelligence (AI). It emphasizes the importance of convincing industry leaders, researchers, and the general public about AI risks and the need for safety measures. The article suggests several approaches to achieve these goals, including direct appeals, sideways appeals to insiders, appeals to outsiders, joining the winning side, and shifting the research culture. The objective is to reshape the AI industry and encourage the adoption of safety-oriented practices to prevent catastrophic outcomes arising from advanced AI systems. – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/mF8dkhZF9hAuLHXaD/reshaping-the-ai-industry},
	journaltitle = {{LessWrong}},
	author = {Ruthenis, Thane},
	urldate = {2022-05-30},
	date = {2022-05-29},
	file = {~/Google Drive/library-pdf/Ruthenis2022ReshapingAIIndustry.pdf;~/Google Drive/library-html/reshaping-the-ai-industry.html}
}

@article{Ryberg1996IsTheRepugnant,
	author = {Ryberg, Jesper},
	title = {Is the repugnant conclusion repugnant?},
	volume = {25},
	number = {3},
	pages = {161–177},
	doi = {10.1080/05568649609506547},
	url = {http://www.tandfonline.com/doi/abs/10.1080/05568649609506547},
	database = {Tlön},
	date = {1996-11},
	issn = {0556-8641, 1996-8523},
	journaltitle = {Philosophical Papers},
	langid = {english},
	shortjournal = {Philosophical Papers},
	timestamp = {2023-07-12 15:59:11 (GMT)},
	urldate = {2023-07-12}
}

@online{SDM2020ModellingContinuousProgress,
	database = {Tlön},
	title = {Modelling continuous progress},
	abstract = {A mathematical model, based on a prior formulation by Eliezer Yudkowsky and Nick Bostrom, was constructed to investigate the differences between discontinuous and continuous progress of AI takeoff, addressing disparities in viewpoint between proponents of both views. The model considers the abruptness of discontinuity and the overall strength of recursive self-improvement (RSI) as variables, capturing the respective assumptions of discontinuous and continuous trajectories. Simulations show that continuous progress leads to earlier acceleration above the exponential trend, a smoother transition to faster growth, and a potential overlap with a “Paul slow” takeoff, where intermediate doubling intervals occur before a rapid collapse in doubling time. Examining varying takeoff speeds reveals that the smoothness of the takeoff curve and the timing of doubling intervals depend on both the continuity and speed of progress. – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/66FKFkWAugS8diydF/modelling-continuous-progress},
	journaltitle = {{LessWrong}},
	author = {{SDM}},
	date = {2020-06-23},
	file = {~/Google Drive/library-pdf/SDM2020ModellingContinuousProgress.pdf}
}

@online{Salamon2012ChecklistRationalityHabits,
	database = {Tlön},
	title = {Checklist of rationality habits},
	abstract = {Based on workshops taught by the Center for Applied Rationality, this checklist evaluates how and to what extent a person acquires "rationality habits." The checklist's goal is not to assess how “rational” one is, but rather to provide a personal shopping list of habits to consider developing. The checklist is not meant to be a way to get a 'how rational are you?' score, but, rather, a way to notice specific habits one might want to develop. For each habit, one might ask if they last used it within a particular timeframe. In this way, the checklist can gauge a sense of development over time. – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/ttGbpJQ8shBi8hDhh/checklist-of-rationality-habits},
	journaltitle = {{LessWrong}},
	author = {Salamon, Anna},
	date = {2012-11-07},
	file = {~/Google Drive/library-pdf/Salamon2012ChecklistRationalityHabits.pdf}
}

@Article{Salt1896HumanitiesOfDiet,
	database = {Tlön},
	pages = {426–435},
	langid = {english},
	number = {357},
	volume = {60},
	journaltitle = {Fortnightly review},
	date = {1896},
	author = {Salt, Henry S.},
	timestamp = {2023-06-18 22:11:16 (GMT)},
	title = {The humanities of diet}
}

@online{SamBankmanFried29,
	database = {Tlön},
	title = {Sam Bankman-Fried, 29, heads crypto exchange worth
                  billions},
	abstract = {Effective altruists have long debated the demandingness of their movement, often focusing solely on frugality. However, there are many ways in which effective altruism could be demanding, including time, epistemics, or status. This issue can be separated into levels of demandingness (total resources) and kinds of demandingness (what resources). The author argues that as effective altruism receives more funding, the focus should shift from money to time. This may lead to a decrease in overall demandingness, although this depends on individual psychology and job type. – AI-generated abstract.},
	url = {https://www.channelnewsasia.com/business/sam-bankman-fried-29-heads-crypto-exchange-worth-billions-2495221},
	journaltitle = {{CNA}},
	urldate = {2022-02-14},
	langid = {english},
	file = {~/Google Drive/library-html/sam-bankman-fried-29-heads-crypto-exchange-worth-billions-2495221.html}
}

@online{Sanchez2017TimelineEnvironmentalistMovement,
	database = {Tlön},
	title = {Timeline of the environmentalist movement},
	langid = {english},
	url = {https://timelines.issarice.com/wiki/Timeline_of_the_environmentalist_movement},
	journaltitle = {Timelines Wiki},
	author = {Sánchez, Sebastián},
	date = {2017-06-08}
}

@online{Sanchez2017TimelineHIVAIDS,
	database = {Tlön},
	title = {Timeline of {HIV}/{AIDS}},
	langid = {english},
	url = {https://timelines.issarice.com/wiki/Timeline_of_HIV/AIDS},
	journaltitle = {Timelines Wiki},
	author = {Sánchez, Sebastián},
	date = {2017-10-29}
}

@online{Sanchez2017TimelineMelanoma,
	database = {Tlön},
	title = {Timeline of melanoma},
	langid = {english},
	url = {https://timelines.issarice.com/wiki/Timeline_of_melanoma},
	journaltitle = {Timelines Wiki},
	author = {Sánchez, Sebastián},
	date = {2017-03-13}
}

@online{Sanchez2018TimelineMercyAnimals,
	database = {Tlön},
	title = {Timeline of mercy for animals},
	langid = {english},
	url = {https://timelines.issarice.com/wiki/Timeline_of_Mercy_For_Animals},
	journaltitle = {Timelines Wiki},
	author = {Sánchez, Sebastián},
	date = {2018-07-02}
}

@online{Sanchez2019TimelineMalnutrition,
	database = {Tlön},
	title = {Timeline of malnutrition},
	langid = {english},
	url = {https://timelines.issarice.com/wiki/Timeline_of_malnutrition},
	journaltitle = {Timelines Wiki},
	author = {Sánchez, Sebastián},
	date = {2019-12-09}
}

@online{Sanchez2019TimelineSchistosomiasisControl,
	database = {Tlön},
	title = {Timeline of Schistosomiasis Control Initiative},
	langid = {english},
	url = {https://timelines.issarice.com/wiki/Timeline_of_Schistosomiasis_Control_Initiative},
	journaltitle = {Timelines Wiki},
	author = {Sánchez, Sebastián},
	date = {2019-03-09}
}

@online{Sanchez2020TimelineArtificialIntelligence,
	database = {Tlön},
	title = {Timeline of artificial intelligence},
	langid = {english},
	url = {https://timelines.issarice.com/wiki/Timeline_of_artificial_intelligence},
	journaltitle = {Timelines Wiki},
	author = {Sánchez, Sebastián},
	date = {2020-02-02}
}

@online{Sanchez2021TimelineHelenKeller,
	database = {Tlön},
	title = {Timeline of Helen Keller International},
	langid = {english},
	url = {https://timelines.issarice.com/wiki/Timeline_of_Helen_Keller_International},
	journaltitle = {Timelines Wiki},
	author = {Sánchez, Sebastián},
	date = 2021
}

@online{Sanchez2022TimelineExistentialRisk,
	database = {Tlön},
	title = {Timeline of existential risk},
	langid = {english},
	url = {https://timelines.issarice.com/wiki/Timeline_of_existential_risk},
	journaltitle = {Timelines Wiki},
	author = {Sánchez, Sebastián},
	date = {2022-08-05}
}

@report{Sandberg2008WholeBrainEmulation,
	database = {Tlön},
	title = {Whole brain emulation: A roadmap},
	langid = {english},
	url = {https://www.fhi.ox.ac.uk/brain-emulation-roadmap-report.pdf},
	institution = {Future of Humanity Institute, Oxford University},
	author = {Sandberg, Anders and Bostrom, Nick},
	date = 2008,
	number = {technical report no. 2008‐3},
	file = {~/Google Drive/library-pdf/Sandberg2008WholeBrainEmulation.pdf}
}

@online{Sandberg2018AnswerComputroniumTime,
	database = {Tlön},
	title = {Answer to 'computronium and time dilation and
                  Bremermann's limit'},
	abstract = {This article delves into the hypothetical computational limits of computronium, a theoretical material consisting of pure computation, in the context of gravitational time dilation. It explores the idea that there may be a density threshold beyond which the time dilation effects due to the gravitational field of computronium would lead to a decrease in its computational efficiency as measured by an external observer. The author presents a simple calculation that estimates this critical density and compares it with the densities of other astrophysical objects such as neutron stars and protons. The article also discusses the potential conflict between this theoretical limit and well-established concepts like Bremermann's limit and the Bekenstein bound. – AI-generated abstract.},
	langid = {english},
	url = {https://physics.stackexchange.com/questions/407658/computronium-and-time-dilation-and-bremermanns-limit},
	journaltitle = {Stack Exchange},
	author = {Sandberg, Anders},
	date = {2018-05-23},
	file = {~/Google Drive/library-pdf/Sandberg2018AnswerComputroniumTime.pdf}
}

@online{Sandberg2020PostScarcityCivilizations,
	database = {Tlön},
	title = {Post scarcity civilizations \& cognitive enhancement},
	langid = {english},
	url = {https://www.youtube.com/watch?v=DZfh3JRlc44},
	journaltitle = {Foresight Institute},
	author = {Sandberg, Anders},
	date = {2020-09-04}
}

@online{Sandberg2021PopperVsMacrohistory,
	database = {Tlön},
	title = {Popper vs macrohistory: What can we say about the
                  long-run future?},
	langid = {english},
	url = {https://www.youtube.com/watch?v=nhoKXBZTKSo},
	journaltitle = {Oxford Karl Popper Society},
	author = {Sandberg, Anders},
	date = {2021-01-25}
}

@article{Sassi2006CalculatingQalysComparing,
	database = {Tlön},
	author = {Sassi, Franco},
	title = {Calculating {QALYs}, Comparing {QALY} and {DALY}
                  Calculations},
	volume = {21},
	number = {5},
	pages = {402--408},
	doi = {10.1093/heapol/czl018},
	url = {https://academic.oup.com/heapol/article-lookup/doi/10.1093/heapol/czl018},
	date = {2006-07-28},
	issn = {0268-1080, 1460-2237},
	journaltitle = {Health Policy and Planning},
	langid = {english},
	shortjournal = {Health Policy and Planning},
	timestamp = {2023-09-14 12:17:19 (GMT)},
	urldate = {2023-09-14}
}

@online{Savage2022CallEarlycareerJournalists,
	database = {Tlön},
	title = {Call for early-career journalists and researchers},
	abstract = {Schmidt Futures is a large philanthropic effort co-founded by Eric and Wendy Schmidt. I'm leading a new program there, Act 2 Network, to matchmake between exceptional people and the roles/cofounders/orgs/funders that might help them do the most damage.
As a part of this, we are building resources to help top professionals who would not consider themselves {EAs}---mostly due to age or social group---make high-impact career shifts. Think "80,000 hours but for executives who don't spend all their time on Twitter".},
	url = {https://forum.effectivealtruism.org/posts/vrmt84oT6SDdLeTSj/call-for-early-career-journalists-and-researchers},
	journaltitle = {Effective Altruism Forum},
	author = {Savage, James},
	urldate = {2022-07-01},
	date = {2022-03-08},
	langid = {english},
	file = {~/Google Drive/library-pdf/Savage2022CallEarlycareerJournalists.pdf;~/Google Drive/library-html/call-for-early-career-journalists-and-researchers.html}
}

@online{Savoie2021Presenting2021Incubated,
	database = {Tlön},
	title = {Presenting: 2021 incubated charities (Charity
                  Entrepreneurship)},
	abstract = {2021 was the third year that we at Charity Entrepreneurship held our annual Incubation Program. Interest in the program was very high, with over 2000 applications submitted. 27 participants representing 16 countries graduated from the 2-month intensive training program, including teams that will start new organizations, individuals that are being hired by high-impact organizations, regional groups that will conduct research under our mentorship, and a foundation that will focus on providing grants to high-impact interventions.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/dAqrQQK42PpjHYoZs/presenting-2021-incubated-charities-charity-entrepreneurship},
	shorttitle = {Presenting},
	journaltitle = {Effective Altruism Forum},
	author = {Savoie, Joey},
	urldate = {2022-04-26},
	date = {2021-10-07},
	file = {~/Google Drive/library-pdf/Savoie2021Presenting2021Incubated.pdf;~/Google Drive/library-html/presenting-2021-incubated-charities-charity-entrepreneurship.html}
}

@online{Sayre-McCord2007Metaethics,
	eventdate = {2023-01-24},
	database = {Tlön},
	abstract = {Metaethics is the attempt to understand the metaphysical,epistemological, semantic, and psychological, presuppositions andcommitments of moral thought, talk, and practice. As such, it countswithin its domain a broad range of questions and puzzles, including:Is morality more a matter of taste than truth? Are moral standardsculturally relative? Are there moral facts? If there are moral facts,what are their origin and nature? How is it that they set anappropriate standard for our behavior? How might moral facts berelated to other facts (about psychology, happiness, humanconventions…)? And how do we learn about moral facts, if thereare any? These questions lead naturally to puzzles about the meaningof moral claims as well as about moral truth and the justification ofour moral commitments. Metaethics explores as well the connectionbetween values, reasons for action, and human motivation, asking howit is that moral standards might provide us with reasons to do orrefrain from doing as they demand, and it addresses many of the issuescommonly bound up with the nature of freedom and its significance (ornot) for moral responsibility.},
	langid = {english},
	title = {Metaethics},
	url = {https://plato.stanford.edu/entries/metaethics/},
	journaltitle = {Stanford Encyclopedia of Philosophy},
	author = {Sayre-{McCord}, Geoff},
	date = {2007-01-23},
	file = {~/Google Drive/library-pdf/Sayre-McCord2007Metaethics.pdf}
}

@article{Schilling2016AvoidHardProblem,
	author = {Schilling, Malte and Cruse, Holk},
	title = {Avoid the hard problem: employment of mental
                  simulation for prediction is already a crucial step},
	volume = {113},
	number = {27},
	doi = {10.1073/pnas.1607146113},
	url = {https://pnas.org/doi/full/10.1073/pnas.1607146113},
	database = {Tlön},
	date = {2016-07-05},
	issn = {0027-8424, 1091-6490},
	journaltitle = {Proceedings of the National Academy of Sciences},
	langid = {english},
	shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
	shorttitle = {Avoid the hard problem},
	timestamp = {2023-07-04 20:19:32 (GMT)},
	urldate = {2023-07-04}
}

@report{SchmidtFutures2022FactSheetPartners,
	database = {Tlön},
	title = {Fact sheet for partners},
	langid = {english},
	url = {https://www.schmidtfutures.com/wp-content/uploads/2022/04/SchmidtFutures_FactSheet_20220413-Final_sm.pdf},
	author = {{Schmidt Futures}},
	date = {2022-04}
}

@report{Schneier2015ResourcesExistentialRisk,
	database = {Tlön},
	title = {Resources on existential risk},
	langid = {english},
	institution = {Berkman Center for Internet and Society, Harvard
                  University},
	author = {Schneier, Bruce},
	date = 2015,
	file = {~/Google Drive/library-pdf/Schneier2015ResourcesExistentialRisk.pdf}
}

@online{Schubert2018WhyArenPeople,
	database = {Tlön},
	title = {Why aren’t people donating more effectively?},
	abstract = {Charitable giving is popular and substantial, but many donors do not choose to donate to highly effective causes. In this talk, Dr. Stefan Schubert explores some possible reasons why, as well as an overview of recent scientific studies on the subject. He also explores what we can do, as effective altruists, to promote effective giving in the world.},
	langid = {english},
	url = {https://www.youtube.com/watch?v=QyvzbW0XKmY},
	journaltitle = {Effective Altruism Global},
	author = {Schubert, Stefan},
	date = {2018-06-08}
}

@online{Schubert2022ResourceNeutralityLevels,
	database = {Tlön},
	title = {Resource neutrality and levels vs kinds of
                  demandingness},
	abstract = {Effective altruists have long argued to be both cause and resource neutral: that issue selection and allocation of resources between causes should be based on impartial assessments of impact, rather than prejudices. However, the growth of funding in recent years has made time a more pressing need than money. Thus, effective altruism should shift away from a focus on money towards a focus on time. It may seem that the level of demandingness (how many resources to give) is a separate issue from the kinds of demandingness (what kinds of resources to give). However, the need for one kind of resource can influence the optimal level of overall demandingness. For example, the influx of money into effective altruism may have made it less demanding overall, despite the switch to time as the key bottleneck. – AI-generated abstract.},
	url = {https://stefanfschubert.com/blog/2022/6/4/resource-neutrality-and-levels-vs-kinds-of-demandingness},
	journaltitle = {Stefan Schubert},
	author = {Schubert, Stefan},
	urldate = {2022-06-06},
	date = {2022-06-04},
	langid = {english},
	file = {~/Google Drive/library-pdf/Schubert2022ResourceNeutralityLevels.pdf;~/Google Drive/library-html/resource-neutrality-and-levels-vs-kinds-of-demandingness.html}
}

@book{Schuck-Paim2021QuantifyingPainLaying,
	database = {Tlön},
	title = {Quantifying pain laying hens: A blueprint for the
                  comparative analysis of welfare in animals},
	langid = {english},
	url = {https://welfarefootprint.org/book-laying-hens/},
	publisher = {self-published},
	author = {Schuck-Paim, Cynthia and Alonso, Wladimir J.},
	date = {2021},
	file = {~/Google Drive/library-pdf/Schuck-Paim2021QuantifyingPainLaying.pdf}
}

@book{Schuck-Paim2022QuantifyingPainBroiler,
	database = {Tlön},
	title = {Quantifying pain in broiler chickens: Impact of the
                  better chicken commitment and adoption of
                  slower-growing breeds on broiler welfare},
	langid = {english},
	url = {https://welfarefootprint.org/broilers/},
	publisher = {self-published},
	author = {Schuck-Paim, Cynthia and Alonso, Wladimir J.},
	date = {2022},
	file = {~/Google Drive/library-pdf/Schuck-Paim2022QuantifyingPainBroiler.pdf}
}

@online{Schuett2020IntroducingLegalPriorities,
	database = {Tlön},
	title = {Introducing the Legal Priorities Project},
	abstract = {We’re excited to introduce a new {EA} organization: the Legal Priorities Project.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/PvBLDPkqKvdHQkKPn/introducing-the-legal-priorities-project},
	journaltitle = {Effective Altruism Forum},
	author = {Schuett, Jonas},
	urldate = {2022-01-09},
	date = {2020-08-30},
	file = {~/Google Drive/library-pdf/Schuett2020IntroducingLegalPriorities.pdf;~/Google Drive/library-html/introducing-the-legal-priorities-project.html}
}

@online{Schukraft2019ManagedHoneyBee,
	database = {Tlön},
	title = {Managed honey bee welfare: Problems and potential
                  interventions},
	abstract = {At any given time there are more than a trillion  managed  honey bees. Globally, the number of managed colonies has risen steadily over the last twenty years and this growth will almost certainly persist, at least in the short term. Increases in demand for honey and (especially) commercial pollination services continue to outpace the increase in supply of managed bees. Asia, especially China and India, hosts the largest populations of managed bees and accounts for much of the recent growth in bee stocks. Commercial beekeeping techniques standardly treat managed bees as a resource from which to maximize the extraction of value. Beekeepers have a financial incentive to maintain the health of their colonies, but they have little reason to look after the welfare of individual bees. Managed bees suffer from a variety of problems, including pesticide exposure, poor nutrition due to inadequate access to natural forage, invasive hive inspections and honey harvest, stress from long-distance transport, and parasite and pathogen spread exacerbated by common management techniques. The effective animal advocacy movement can help a significant number of managed bees by promoting welfare-oriented management techniques, supporting academic research that has the potential to advance honey bee welfare, encouraging welfare-oriented policies and regulations, and reducing the demand for commercial pollination services. However, it’s not clear that these interventions are promising enough to be pursued in the near-term. Much uncertainty remains, and more research would be required to determine the ultimate wisdom and cost-effectiveness of these interventions.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/XyKJJqLQjSKzL7ykP/managed-honey-bee-welfare-problems-and-potential-1},
	shorttitle = {Managed Honey Bee Welfare},
	journaltitle = {Effective Altruism Forum},
	author = {Schukraft, Jason},
	urldate = {2022-05-13},
	date = {2019-11-14},
	file = {~/Google Drive/library-pdf/Schukraft2019ManagedHoneyBee.pdf;~/Google Drive/library-html/managed-honey-bee-welfare-problems-and-potential-1.html}
}

@unpublished{Schukraft2020ComparativeMoralStatus,
	database = {Tlön},
	title = {Comparative moral status academic bibliography},
	langid = {english},
	url = {https://docs.google.com/document/d/1Y7XG3YWEkZWdn8LBBiwQVnzW2yJutz1FNfldAd4jzeY/edit#heading=h.59d8i7djcj1s},
	type = {unpublished},
	author = {Schukraft, Jason},
	date = {2020}
}

@collection{Schwarz2004DekkerEncyclopediaNanoscience,
	database = {Tlön},
	location = {New York},
	abstract = {This paper discusses how academics can contribute to shaping and improving Wikipedia. It suggests that academics are well-positioned to use their knowledge and expertise to ensure that Wikipedia articles are accurate, up-to-date, and accessible to a wide audience. By engaging with Wikipedia, academics can communicate their research findings to a broader public and make a positive impact on public discourse. – AI-generated abstract.},
	langid = {english},
	title = {Dekker encyclopedia of nanoscience and nanotechnology},
	isbn = {0-8247-4797-6},
	pagetotal = {5},
	publisher = {M. Dekker},
	editor = {Schwarz, James A. and Contescu, Cristian I. and
                  Putyera, Karol},
	date = {2004},
	file = {~/Google Drive/library-pdf/Schwarz2004DekkerEncyclopediaNanoscience.pdf}
}

@online{Scoblic2022NuclearExpertComment,
	database = {Tlön},
	title = {Nuclear expert comment on Samotsvety nuclear risk
                  forecast},
	abstract = {On March 10th, the Samotsvety forecasting team published a Forum post assessing the risk that a London resident would be killed in a nuclear strike “in light of the war in Ukraine and fears of nuclear escalation.” It recommended against evacuating major cities because the risk of nuclear death was extremely low. However, the reasoning behind the forecast was questionable. The following is intended as a constructive critique to improve the forecast quality of nuclear risk, heighten policymaker responsiveness to probabilistic predictions, and ultimately reduce the danger of nuclear war.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/W8dpCJGkwrwn7BfLk/nuclear-expert-comment-on-samotsvety-nuclear-risk-forecast-2},
	journaltitle = {Effective Altruism Forum},
	author = {Scoblic, J. Peter},
	urldate = {2022-04-20},
	date = {2022-03-26},
	file = {~/Google Drive/library-pdf/Scoblic2022NuclearExpertComment.pdf;~/Google Drive/library-html/nuclear-expert-comment-on-samotsvety-nuclear-risk-forecast-2.html}
}

@online{Sebo2019UtilitarianCaseFor,
	date = {2019-12-18},
	journaltitle = {Effective Altruism Global},
	author = {Sebo, Jeff},
	database = {Tlön},
	langid = {spanish},
	shorttitle = {A utilitarian case for animal rights},
	timestamp = {2023-07-13 10:12:32 (GMT)},
	title = {A utilitarian case for animal rights},
	url = {https://www.youtube.com/watch?v=vELWCTgA9oA},
	urldate = {2023-07-13}
}

@online{Sebo2023DefensaEfectivaDe,
	database = {Tlön},
	date = {2023},
	title = {Defensa efectiva de los animales},
	author = {Sebo, Jeff},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Sebo2020EffectiveAnimalAdvocacy}
}

@online{Sempere2019ShapleyValuesBetter,
	database = {Tlön},
	title = {Shapley values: Better than counterfactuals},
	abstract = {[Epistemic status: Pretty confident. But also, enthusiasm on the verge of partisanship].
One intuitive function which assigns impact to agents is the counterfactual, which has the form:.
.
{CounterfactualImpact}(Agent) = Value(World) - Value(World/Agent).
.
which reads "The impact of an agent is the difference between the value of the world with the agent and the value of the world without the agent".},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/XHZJ9i7QBtAJZ6byW/shapley-values-better-than-counterfactuals},
	shorttitle = {Shapley values},
	journaltitle = {Effective Altruism Forum},
	author = {Sempere, Nuño},
	urldate = {2022-03-29},
	date = {2019-10-10},
	file = {~/Google Drive/library-pdf/Sempere2019ShapleyValuesBetter.pdf;~/Google Drive/library-html/shapley-values-better-than-counterfactuals.html}
}

@online{Sempere2020BigListCause,
	database = {Tlön},
	title = {Big list of cause candidates},
	abstract = {In the last few years, there have been many dozens of posts about potential new {EA} cause areas, causes and interventions. Searching for new causes seems like a worthy endeavour, but on their own, the submissions can be quite scattered and chaotic. Collecting and categorizing these cause candidates seemed like a clear next step. In early 2022, Leo updated the list with new candidates suggested before March 2022, in this post—these have now been incorporated into the main post.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/SCqRu6shoa8ySvRAa/big-list-of-cause-candidates},
	journaltitle = {Effective Altruism Forum},
	author = {Sempere, Nuño},
	urldate = {2022-04-30},
	date = {2020-12-25},
	file = {~/Google Drive/library-html/Big List of Cause Candidates - EA Forum:big-list-of-cause-candidates.html;~/Google Drive/library-pdf/Sempere2020BigListCause.pdf}
}

@online{Sempere20212020ForecastingReview,
	database = {Tlön},
	title = {2020: Forecasting in review},
	abstract = {This document contains a series of highlights about forecasting in 2020 which I have gathered after 10 months of writing a forecasting newsletter. I'll write one or two paragraphs for each point, and then list ideas which interested readers can follow up on. As such, this piece can be read either as an accessible superficial summary, as an index of pointers, or or as a resource for later years—a snapshot of what was happening in 2020. {EAs} will particularly be interested in sections {II}, {IV} and {VI}.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/8shCj2eoQygQvtoZP/2020-forecasting-in-review},
	journaltitle = {Effective Altruism Forum},
	author = {Sempere, Nuño},
	date = {2021-01-10},
	file = {~/Google Drive/library-pdf/Sempere20212020ForecastingReview.pdf}
}

@online{Sempere2022CriticalReviewOpen,
	database = {Tlön},
	title = {A critical review of Open Philanthropy’s bet on
                  criminal justice reform},
	abstract = {From 2013 to 2021, Open Philanthropy donated \$200M to criminal justice reform. My best guess is that, from a utilitarian perspective, this was likely suboptimal. In particular, I am fairly sure that it was possible to realize sooner that the area was unpromising and act on that earlier on.
In this post, I first present the background for Open Philanthropy's grants on criminal justice reform, and the abstract case for considering it a priority. I then estimate that criminal justice grants were distinctly worse than other grants in the global health and development portfolio, such as those to {GiveDirectly} or {AMF}.
I speculate about why Open Philanthropy donated to criminal justice in the first place, and why it continued donating. I end up uncertain about to what extent this was a sincere play based on considerations around the value of information and learning, and to what extent it was determined by other factors, such as the idiosyncratic preferences of Open Philanthropy's funders, human fallibility and slowness, paying too much to avoid social awkwardness, “worldview diversification” being an imperfect framework imperfectly applied, or it being tricky to maintain a balance between conventional morality and expected utility maximization. In short, I started out being skeptical that a utilitarian, left alone, spontaneously starts exploring criminal justice reform in the {US} as a cause area, and to some degree I still think that upon further investigation, though I still have significant uncertainty.
I then outline my updates about Open Philanthropy. Personally, I updated downwards on Open Philanthropy’s decision speed, rationality and degree of openness, from an initially very high starting point. I also provide a shallow analysis of Open Philanthropy’s worldview diversification strategy and suggest that they move to a model where regular rebalancing roughly equalizes the marginal expected values for the grants in each cause area. Open Philanthropy is doing that for its global health and development portfolio anyways.
Lastly, I brainstorm some mechanisms which could have accelerated and improved Open Philanthropy's decision-making and suggest red teams and monetary bets or prediction markets as potential avenues of investigation.
Throughout this piece, my focus is aimed at thinking clearly and expressing myself clearly. I understand that this might come across as impolite or unduly harsh. However, I think that providing uncertain and perhaps flawed criticism is still worth it, in expectation. I would like to note that I still respect Open Philanthropy and think that it’s one of the best philanthropic organizations around.
Open Philanthropy staff reviewed this post prior to publication.},
	url = {https://forum.effectivealtruism.org/posts/h2N9qEbvQ6RHABcae/a-critical-review-of-open-philanthropy-s-bet-on-criminal},
	journaltitle = {Effective Altruism Forum},
	author = {Sempere, Nuño},
	urldate = {2022-07-01},
	date = {2022-06-16},
	langid = {english},
	file = {~/Google Drive/library-pdf/Sempere2022CriticalReviewOpen.pdf;~/Google Drive/library-html/a-critical-review-of-open-philanthropy-s-bet-on-criminal.html}
}

@online{Sempere2022SamotsvetyNuclearRisk,
	database = {Tlön},
	title = {Samotsvety nuclear risk update October 2022},
	abstract = {For ≤ 1 month staggering times between each step.
Event Conditional on previous {stepUnconditional} {probabilityRussia} uses a nuclear weapon in Ukraine in the next month—5.3\%Nuclear conflict scales beyond Ukraine in the next month after the initial nuclear weapon use2.5\%0.13\%London gets hit, one month after the first non-Ukraine nuclear bomb is used?14\%0.02\%.
For ≤ 1 year staggering times between each step.
Event Conditional on previous {stepUnconditional} {probabilityRussia} uses a nuclear weapon in Ukraine in the next year—16\% Nuclear conflict scales beyond Ukraine in the next year after the initial nuclear weapon use9.6\%1.6\%London gets hit, one year after the first non-Ukraine nuclear bomb is used?23\%0.36\%.},
	url = {https://forum.effectivealtruism.org/posts/2nDTrDPZJBEerZGrk/samotsvety-nuclear-risk-update-october-2022},
	journaltitle = {Effective Altruism Forum},
	author = {Sempere, Nuño and Yagudin, Misha},
	urldate = {2022-10-04},
	date = {2022-10-03},
	langid = {english},
	file = {~/Google Drive/library-pdf/Sempere2022SamotsvetyNuclearRisk.pdf;~/Google Drive/library-html/samotsvety-nuclear-risk-update-october-2022.html}
}

@unpublished{Sevilla2021PersistenceCriticalReview,
	database = {Tlön},
	title = {Persistence - A critical review},
	abstract = {The document presents a strategy for the Effective Altruism community in London, comprising the main points of coordination and activities. It includes areas not focused on alongside those of importance, such as community-wide events and meta activities. Moreover, it details the metrics that will be measured for evaluation purposes. – AI-generated abstract.},
	url = {https://docs.google.com/document/d/14ULAaTofWiQbTCP1ekuaenQJ6saXEzjgiKMznIBrXvQ/edit?usp=embed_facebook},
	type = {unpublished},
	author = {Sevilla, Jaime},
	urldate = {2021-11-10},
	date = {2021},
	langid = {english},
	file = {~/Google Drive/library-html/edit.html}
}

@online{Sevilla2022AnnouncingEpochResearcha,
	database = {Tlön},
	title = {Announcing Epoch: A research organization
                  investigating the road to Transformative {AI} - {EA}
                  Forum},
	abstract = {We are a new research organization working on investigating trends in Machine Learning and forecasting the development of Transformative Artificial {IntelligenceThis} work is done in close collaboration with other organizations, like Rethink Priorities and Open Philanthropy We will be hiring for 2-4 full-time roles this summer – more information {hereYou} can find up-to-date information about Epoch on our website.},
	url = {https://forum.effectivealtruism.org/posts/zqRDNChFburJMmpqK/announcing-epoch-a-research-organization-investigating-the},
	shorttitle = {Announcing Epoch},
	journaltitle = {Effective Altruism Forum},
	author = {Sevilla, Jaime and Villalobos, Pablo and Besiroglu,
                  Tamay and Heim, Lennart and Ho, Anson and Hobbhahn,
                  Marius},
	urldate = {2022-06-27},
	date = {2022-06-27},
	langid = {english},
	file = {~/Google Drive/library-pdf/Sevilla2022AnnouncingEpochResearch.pdf;~/Google Drive/library-html/announcing-epoch-a-research-organization-investigating-the.html}
}

@article{Shafee2017AcademicsCanHelp,
	database = {Tlön},
	title = {Academics can help shape Wikipedia},
	abstract = {Over three years of publication, the “Alignment Newsletter” has become a platform for summarizing research and news related to AI safety and alignment. Over time, the newsletter has become more pedagogical, offering explanations and unpacking jargon for better understanding. The author has also become more opinionated on important research, focusing on those that align with their current understanding of AI alignment. As a result, the newsletter now offers a particular perspective on alignment research, moving away from a more general overview of the field. – AI-generated abstract.},
	volume = {357},
	rights = {Copyright © 2017 The Authors, some rights reserved;
                  exclusive licensee American Association for the
                  Advancement of Science. No claim to original U.S.
                  Government Works.
                  https://www.sciencemag.org/about/science-licenses-journal-article-{reuseThis}
                  is an article distributed under the terms of the
                  Science Journals Default License.},
	issn = {0036-8075, 1095-9203},
	url = {https://science.sciencemag.org/content/357/6351/557.2},
	doi = {10.1126/science.aao0462},
	pages = {557–558},
	number = {6351},
	journaltitle = {Science},
	author = {Shafee, Thomas and Mietchen, Daniel and Su, Andrew I.},
	urldate = {2021-07-26},
	date = {2017-08-11},
	langid = {english},
	pmid = {28798122},
	note = {Publisher: American Association for the Advancement of
                  Science Section: Letters},
	file = {~/Google Drive/library-pdf/Shafee2017AcademicsCanHelp.pdf;~/Google Drive/library-html/557.2.html}
}

@online{Shah2019AlignmentNewsletterOne,
	database = {Tlön},
	title = {Alignment newsletter one year retrospective},
	abstract = {In which I badger you to take the 3-minute survey, and summarize some key points.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/Prxqvhr9JFj7JyJRX/alignment-newsletter-one-year-retrospective},
	journaltitle = {Effective Altruism Forum},
	author = {Shah, Rohin},
	date = {2019-04-10},
	file = {~/Google Drive/library-pdf/Shah2019AlignmentNewsletterOne.pdf}
}

@online{Shah2021AlignmentNewsletterThree,
	database = {Tlön},
	title = {Alignment newsletter three year retrospective},
	abstract = {The Alignment Newsletter provides weekly updates on advancements in AI alignment research. Three years after its inception, it remains an organic platform with high open rates and click-through rates. This retrospective focuses on changes since the one-year mark. The newsletter has shifted towards more pedagogic summaries and longer explainers, while covering fewer articles per newsletter. The author explains that he has become more selective in choosing articles that align with his understanding of AI alignment. Despite a call for contributors, the newsletter is mainly written by the author, who encourages readers to consider using the spreadsheet database as a primary mode of interaction. – AI-generated abstract.},
	langid = {english},
	url = {https://www.alignmentforum.org/posts/L7yHdqRiHKd3FhQ7B/alignment-newsletter-three-year-retrospective},
	journaltitle = {{AI} Alignment Forum},
	author = {Shah, Rohin},
	date = {2021-04-07},
	file = {~/Google Drive/library-pdf/Shah2021AlignmentNewsletterThree.pdf}
}

@online{Sharma2020ProblemAreaReport,
	database = {Tlön},
	title = {Problem area report: pain},
	abstract = {Redwood Research attempts to prevent a fine-tuned language model from describing someone getting injured. To achieve this, they train a classifier to predict whether a completion involves injury. Using this classifier as a filter, they generate safe completions. However, this method is inefficient if the unfiltered model is unlikely to generate harmful content. The authors detail other issues with this method, then discuss plans to improve the classifier and train a new model that can generate exclusively safe content. – AI-generated abstract.},
	url = {https://web.archive.org/web/20201130121652/https://www.happierlivesinstitute.org/uploads/1/0/9/9/109970865/problem-area-pain.pdf},
	journaltitle = {Happier Lives Institute},
	author = {Sharma, Sid and Donaldson, Clare and Plant, Michael},
	urldate = {2022-03-21},
	date = {2020-11},
	langid = {english},
	file = {~/Google Drive/library-pdf/Sharma2020ProblemAreaReport.pdf;~/Google Drive/library-html/problem-area-pain.html}
}

@Book{Shewring1947RichAndPoor,
	database = {Tlön},
	publisher = {Burns Oates \& Washbourne},
	langid = {english},
	address = {London},
	date = {1948},
	title = {Rich and poor in Christian tradition},
	author = {Shewring, Walter},
	timestamp = {2023-06-05 12:39:36 (GMT)}
}

@online{Shiller2022ImportanceGettingDigital,
	database = {Tlön},
	title = {The importance of getting digital consciousness right},
	abstract = {We may soon develop digital minds that act like humans. Because we have no consensus about what makes a mind conscious, society may mistakenly think these digital minds are conscious, which could lead to a future where most advanced consciousness consists of machines without phenomenal experiences or subjective feelings. Additionally, the easiest ways to create minds that appear conscious may not actually create minds that are truly conscious. If society incorrectly believes that many apparent digital minds are genuinely conscious, it could lead to moral and legal quandaries, as well as societal pressure to treat these digital minds as if they had human-level rights. Avoiding this requires society to reach a consensus on what makes a mind conscious before apparent digital minds become popular. – AI-generated abstract.},
	url = {https://forum.effectivealtruism.org/posts/yxpFFChqK5ErKYHxv/the-importance-of-getting-digital-consciousness-right},
	journaltitle = {Effective Altruism Forum},
	author = {Shiller, Derek},
	urldate = {2022-06-28},
	date = {2022-06-13},
	langid = {english},
	file = {~/Google Drive/library-pdf/Shiller2022ImportanceGettingDigital.pdf;~/Google Drive/library-html/the-importance-of-getting-digital-consciousness-right.html}
}

@online{Shlegeris2020MyPersonalCruxes,
	database = {Tlön},
	title = {My personal cruxes for working on {AI} safety},
	abstract = {It's great to be here. I used to hang out at Stanford a lot, fun fact. I moved to America six years ago, and then in 2015, I came to Stanford {EA} every Sunday, and there was, obviously, a totally different crop of people there. It was really fun. I think we were a lot less successful than the current Stanford {EA} iteration at attracting new people. We just liked having weird conversations about weird stuff every week. It was really fun, but it's really great to come back and see a Stanford {EA} which is shaped differently.
Today I'm going to be talking about the argument for working on {AI} safety that compels me to work on {AI} safety, rather than the argument that should compel you or anyone else. I'm going to try to spell out how the arguments are actually shaped in my head. Logistically, we're going to try to talk for about an hour with a bunch of back and forth and you guys arguing with me as we go. And at the end, I'm going to do miscellaneous Q and A for questions you might have.
And I'll probably make everyone stand up and sit down again because it's unreasonable to sit in the same place for 90 minutes.},
	url = {https://forum.effectivealtruism.org/posts/Ayu5im98u8FeMWoBZ/my-personal-cruxes-for-working-on-ai-safety},
	journaltitle = {Effective Altruism Forum},
	author = {Shlegeris, Buck},
	urldate = {2022-06-03},
	date = {2020-02-13},
	langid = {english},
	file = {~/Google Drive/library-pdf/Shlegeris2020MyPersonalCruxes.pdf;~/Google Drive/library-html/my-personal-cruxes-for-working-on-ai-safety.html}
}

@online{Shlegeris2021RedwoodResearchCurrent,
	database = {Tlön},
	title = {Redwood Research’s current project},
	abstract = {This article presents the 'quiz study' of Ross, Amabile, and Steinmetz (1977), which demonstrated the Fundamental Attribution Error, a cognitive bias where observers rate questioners higher than answerers, even when they know the assignments were random. The author argues that this error can lead to a 'super-happy death spiral' of reverence, where individuals overrate the messenger of good information, such as teachers or science popularizers, based on their limited interactions with them in domains where they have less knowledge. Citing examples from Eliezer Yudkowsky's writings on philosophy, economics, and psychology, the author suggests that knowledge should be obtained from multiple independent sources to avoid this bias. Additionally, the author recommends seeking neutral sources like the Stanford Encyclopedia of Philosophy and reading opposing viewpoints to gain a more balanced understanding. – AI-generated abstract.},
	langid = {english},
	url = {https://www.alignmentforum.org/posts/k7oxdbNaGATZbtEg3/redwood-research-s-current-project},
	journaltitle = {{AI} Alignment Forum},
	author = {Shlegeris, Buck},
	urldate = {2021-09-23},
	date = {2021-09-21},
	file = {~/Google Drive/library-html/redwood-research-s-current-project.html}
}

@online{Shlegeris2022TakeoffSpeedsHave,
	database = {Tlön},
	title = {Takeoff speeds have a huge effect on what it means to
                  work on {AI} x-risk},
	abstract = {The speed of artificial intelligence (AI) takeoff, whether fast or slow, has a significant impact on the strategies and approaches of individuals and organizations working on AI x-risk mitigation. In a fast takeoff scenario, AI takeover risks remain largely unchanged, and x-risk-motivated individuals play a more prominent role in alignment research. In contrast, in a slow takeoff scenario, alignment problems manifest in non-AGI systems, leading to industrial research on various aspects of alignment. This divergence in perspectives underscores the need for considering different research strategies and collaboration paradigms in each scenario. – AI-generated abstract.},
	langid = {english},
	url = {https://www.alignmentforum.org/posts/hRohhttbtpY3SHmmD/takeoff-speeds-have-a-huge-effect-on-what-it-means-to-work-1},
	journaltitle = {{AI} Alignment Forum},
	author = {Shlegeris, Buck},
	urldate = {2022-04-15},
	date = {2022-04-13},
	file = {~/Google Drive/library-pdf/Shlegeris2022TakeoffSpeedsHavea.pdf}
}

@online{Shulman2009DontRevereBearer,
	file = {~/Google Drive/library-pdf/Shulman2009DontRevereBearer.pdf;~/Google Drive/library-html/Shulman2009DontRevereBearer.html},
	date = {2009-03-22},
	abstract = {This article discusses the tendency to overrate the messenger, specifically those who transmit knowledge in a field where the receiver lacks significant prior knowledge. The author emphasizes the need to avoid cultish hero-worship and revering the messenger of good information. He provides examples of how certain concepts and arguments, presented by influential communicators, may be overrated due to affective death spiral. To counter this bias, the author suggests seeking diverse perspectives and acknowledging independent discoveries or contributions by others to avoid overemphasizing the messenger's role. He also recommends accessing textbooks and neutral sources to broaden one's understanding and avoid being influenced by biased or incomplete information presented by a single source. – AI-generated abstract.},
	journaltitle = {LessWrong},
	database = {Tlön},
	author = {Shulman, Carl},
	title = {Don't revere the bearer of good info},
	url = {https://www.lesswrong.com/posts/tSgcorrgBnrCH8nL3/don-t-revere-the-bearer-of-good-info},
	langid = {english},
	timestamp = {2023-07-27 12:15:52 (GMT)},
	urldate = {2023-07-27}
}

@online{Shulman2010PoliticsCharity,
	database = {Tlön},
	title = {Politics as charity},
	abstract = {The cost of voting may exceed the expected value of the policy changes that a marginal vote might make, causing many people to believe that voting is irrational. The article constructs a model likening voting to charity and posits that it might be rational to engage in politics to affect change due to the consequentialist potential benefit that may come from doing so. The author notes that a detailed analysis of electoral campaigning, political spending, and the link between those and policy outcomes would be necessary to make such a determination. – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/SgZ2mhvDbneBusFEB/politics-as-charity},
	journaltitle = {{LessWrong}},
	author = {Shulman, Carl},
	date = {2010-09-23},
	file = {~/Google Drive/library-pdf/Shulman2010PoliticsCharity.pdf}
}

@online{Shulman2012CouldWeUse,
	database = {Tlön},
	title = {Could we use untrustworthy human brain emulations to
                  make trustworthy ones?},
	abstract = {One possible route to creating Artificial General Intelligence (AGI) is by creating detailed models of human brains which can substitute for those brains at various tasks, i.e. human Whole Brain Emulation (WBE) (Sandberg and Bostrom, 2008). If computation was abundant, WBEs could undergo an extremely rapid population explosion, operate at higher speeds than humans, and create even more capable successors in an "intelligence explosion" (Good, 1965; Hanson, 1994). Thus, it would be reassuring if the first widely deployed emulations were mentally stable, loyal to the existing order or human population, and humane in their moral sentiments.
However, incremental progress may make it possible to create productive but unstable or inhuman emulations first (Yudkowsky, 2008). For instance, early emulations might loosely resemble brain-damaged amnesiacs that have been gradually altered (often in novel ways) to improve performance, rather than digital copies of human individuals selected for their stability, loyalties, and ethics. A business or state that waited to develop more trustworthy emulations would then delay and risk losing an enormous competitive advantage. The longer the necessary delay, the more likely untrustworthy systems would be widely deployed.

Could low-fidelity brain emulations, intelligent but untrusted, be used to greatly reduce that delay? Trustworthy high-quality emulations could do anything that a human development team could do, but could do it a hundredfold more quickly given a hundredfold hardware improvement (perhaps with bottlenecks from runtime of other computer programs, etc). Untrusted systems would need their work supervised by humans to prevent escape or sabotage of the project.

This supervision would restrict the productivity of the emulations, and introduce bottlenecks for human input, reducing the speedup.
This paper discusses tools for human supervision of untrusted brain emulations, and argues that supervised, untrusted brain emulations could result in large speedups in research progress.},
	langid = {english},
	url = {https://www.youtube.com/watch?v=ljHFmznqkYM&list=PLkC3Ey3ATnTCn2_v8dgRmg5MBJtja_YK-&index=20},
	journaltitle = {The Fifth Conference on Artificial General
                  Intelligence},
	author = {Shulman, Carl},
	date = {2012-12-11}
}

@online{Shulman2016DonorLotteriesDemonstration,
	database = {Tlön},
	title = {Donor lotteries: demonstration and {FAQ}},
	abstract = {Suppose that Alice is trying to figure out how to do the most good with her donation of \$1,000 this giving season, and can spend various kinds of resources to improve her decision. Unfortunately, many investments that could improve the decision quality would impose costs that are large relative to her donation: spending hundreds of hours (whether her own, those of charity staff, or of hired evaluators) investigating opportunities will cost more than her donation amount. She will also be limited in the projects she can fund: whereas a large funder can attract proposals for new projects, and fund a new position or startup organization, she seems to be limited to contributing to existing public opportunities.One solution to this problem would be for Alice to work with Bob, a large donor, to construct a 'donor lottery.' Alice donates her \$1,000 to Bob's donor-advised fund, or {DAF}. Then Alice and Bob consult a random number generator to determine how to recommend donation allocations to the {DAF}. For example, they might plan that with 1/100 probability Alice would get to recommend the allocation of \$100,000 from the {DAF}, while with 99/100 probability Bob allocates the {DAF} without input from Alice.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/WvPEitTCM8ueYPeeH/donor-lotteries-demonstration-and-faq},
	shorttitle = {Donor lotteries},
	journaltitle = {Effective Altruism Forum},
	author = {Shulman, Carl},
	urldate = {2021-07-22},
	date = {2016-12-07},
	file = {~/Google Drive/library-pdf/Shulman2016DonorLotteriesDemonstration.pdf;~/Google Drive/library-html/donor-lotteries-demonstration-and-faq.html}
}

@online{Shulman2019PersonalThoughtsEA,
	database = {Tlön},
	title = {Some personal thoughts on {EA} and systemic change},
	abstract = {Actual {EA} is able to do assessments of systemic change interventions including electoral politics and policy change, and has done so a number of {timesThe} great majority of critics of {EA} invoking systemic change fail to present the simple sort of quantitative analysis given above for the interventions they claim excel, and frequently when such analysis is done the intervention does not look competitive by {EA} {lightsNonetheless}, my view is that historical data do show that the most efficient political/advocacy spending, particularly aiming at candidates and issues selected with an eye to global poverty or the long term, does have higher returns than {GiveWell} top charities (even ignoring nonhumans and future generations or future technologies); one can connect the systemic change critique as a position in intramural debates among {EAs} about the degree to which one should focus on highly linear, giving as consumption, type {interventionsEAs} who are willing to consider riskier and less linear interventions are mostly already pursuing fairly dramatic systemic change, in areas with budgets that are small relative to political spending (unlike foreign aid) As funding expands in focused {EA} priority issues, eventually diminishing returns there will equalize with returns for broader political spending, and activity in the latter area could increase enormously: since broad political impact per dollar is flatter over a large range political spending should either be a very small or very large portion of {EA} activity.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/QYH9yJ4WfHRs3ftJD/some-personal-thoughts-on-ea-and-systemic-change},
	journaltitle = {Effective Altruism Forum},
	author = {Shulman, Carl},
	date = {2019-09-26},
	file = {~/Google Drive/library-pdf/Shulman2019PersonalThoughtsEA.pdf}
}

@online{Shulman2022CommentDiscountRate,
	database = {Tlön},
	title = {Comment on 'The discount rate is not zero'},
	abstract = {Longtermists believe that future people matter, there could be a lot of them, and they are disenfranchised. They argue a life in the distant future has the same moral worth as somebody alive today. This implies that analyses which discount the future unjustifiably overlook the welfare of potentially hundreds of billions of future people, if not many more.Given the relationship between longtermism and views about existential risk, it is often noted that future lives should in fact be discounted somewhat – not for time preference, but for the likelihood of existing (i.e., the discount rate equals the catastrophe rate).I argue that the long-term discount rate is both positive and inelastic, due to 1) the lingering nature of present threats, 2) our ongoing ability to generate threats, and 3) continuously lowering barriers to entry. This has 2 major implications.First, we can only address near-term existential risks. Applying a long-term discount rate in line with the long-term catastrophe rate, by my calculations, suggests the expected length of human existence is another 8,200 years (and another trillion people). This is significantly less than commonly cited estimates of our vast potential.Second, I argue that equally applying longtermist principles would consider the descendants of each individual, when lives are saved in the present. A non-zero discount rate allows us to calculate the expected number of a person’s descendants. I estimate 1 life saved today affects an additional 93 people over the course of humanity’s expected existence.Both claims imply that x-risk reduction is overweighted relative to interventions such as global health and poverty reduction (but I am {NOT} arguing x-risks are unimportant).},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/zLZMsthcqfmv5J6Ev/?commentId=Nr35E6sTfn9cPxrwQ},
	journaltitle = {Effective Altruism Forum},
	author = {Shulman, Carl},
	urldate = {2022-09-05},
	date = {2022-09-02},
	file = {~/Google Drive/library-html/Effective Altruism Forum
                  Post zLZMsthcqfmv5J6Ev.html;~/Google Drive/library-pdf/Shulman2022CommentDiscountRate.pdf}
}

@online{Shulman2023CuanDificilEs,
	database = {Tlön},
	date = {2023},
	title = {¿Cuán difícil es llegar a ser Primer Ministro del
                  Reino Unido?},
	author = {Shulman, Carl},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Shulman2012HowHardIs}
}

@online{Shulman2023EfectosIndirectosDe,
	database = {Tlön},
	date = {2023},
	title = {Los efectos indirectos de largo alcance en los años de
                  vida vividos como consecuencia de salvar una vida a
                  través de los siglos},
	author = {Shulman, Carl},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Shulman2018FlowThroughEffects}
}

@online{Shulman2023HowMuchShouldb,
	database = {Tlön},
	file = {~/Google Drive/library-html/Shulman2023HowMuchShouldb.html;~/Google Drive/library-pdf/Shulman2023HowMuchShouldb.pdf},
	abstract = {Longtermists have argued that humanity should significantly increase its efforts to prevent catastrophes like nuclear wars, pandemics, and {AI} disasters. But one prominent longtermist argument overshoots this conclusion: the argument also implies that humanity should reduce the risk of existential catastrophe even at extreme cost to the present generation. This overshoot means that democratic governments cannot use the longtermist argument to guide their catastrophe policy. In this paper, we show that the case for preventing catastrophe does not depend on longtermism. Standard cost-benefit analysis implies that governments should spend much more on reducing catastrophic risk. We argue that a government catastrophe policy guided by cost-benefit analysis should be the goal of longtermists in the political sphere. This policy would be democratically acceptable, and it would reduce existential risk by almost as much as a strong longtermist policy.},
	langid = {english},
	author = {Shulman, Carl and Thornley, Elliott},
	date = {2023-03-18},
	shorttitle = {How much should governments pay to prevent
                  catastrophes?},
	timestamp = {2023-03-20 09:37:57 (GMT)},
	title = {How much should governments pay to prevent
                  catastrophes? Longtermism's limited role},
	journaltitle = {Effective Altruism Forum},
	url = {https://forum.effectivealtruism.org/posts/DiGL5FuLgWActPBsf/how-much-should-governments-pay-to-prevent-catastrophes},
	urldate = {2023-03-20}
}

@online{Shulman2023SonDolorPlacer,
	database = {Tlön},
	keywords = {utilitarismo, hedonio, desperdicio astronómico},
	date = {2023},
	langid = {spanish},
	author = {Shulman, Carl},
	title = {¿Son el dolor y el placer igual de eficientes en
                  energía?},
	translator = {Tlön},
	translation = {Shulman2012ArePainPleasure}
}

@book{Sidgwick1907MethodsEthics,
	database = {Tlön},
	location = {London},
	langid = {english},
	edition = {7},
	title = {The Methods of Ethics},
	publisher = {Macmillan},
	author = {Sidgwick, Henry},
	date = {1907},
	file = {~/Google Drive/library-pdf/Sidgwick1907MethodsEthics.pdf}
}

@online{Simcikas2019CorporateCampaignsAffect,
	database = {Tlön},
	title = {Corporate campaigns affect 9 to 120 years of chicken
                  life per dollar spent},
	abstract = {In this article, I estimate how many chickens will be affected by corporate cage-free and broiler welfare commitments won by all charities, in all countries, during all the years between 2005 and the end of 2018. According to my estimate, for every dollar spent, 9 to 120 years of chicken life will be affected. However, the estimate doesn't take into account indirect effects which could be more important.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/L5EZjjXKdNgcm253H/corporate-campaigns-affect-9-to-120-years-of-chicken-life},
	journaltitle = {Effective Altruism Forum},
	author = {Šimčikas, Saulius},
	urldate = {2022-03-24},
	date = {2019-07-08},
	file = {~/Google Drive/library-html/Corporate campaigns affect
                  9 to 120 years of chicken life per dollar spent - EA
                  Forum:corporate-campaigns-affect-9-to-120-years-of-chicken-life.html;~/Google Drive/library-pdf/Simcikas2019CorporateCampaignsAffect.pdf}
}

@online{Simcikas2020CauseCanBe,
	database = {Tlön},
	title = {A cause can be too neglected},
	abstract = {Effective Altruism movement often uses a scale-neglectedness-tractability framework. As a result of that framework, when I discovered issues like baitfish, fish stocking, and rodents fed to pet snakes, I thought that it is an advantage that they are almost maximally neglected (seemingly no one is working on them). Now I think that it’s also a disadvantage because there are set-up costs associated with starting work on a new cause.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/NktbYpwa48u23c5XL/a-cause-can-be-too-neglected},
	journaltitle = {Effective Altruism Forum},
	author = {Šimčikas, Saulius},
	date = {2020-04-03},
	file = {~/Google Drive/library-pdf/Simcikas2020CauseCanBe.pdf}
}

@online{Simcikas2022WildAnimalWelfare,
	database = {Tlön},
	title = {Wild animal welfare in the far future},
	abstract = {In this article, I analyze which far-future wild animal welfare ({WAW}) scenarios seem most important from a utilitarian perspective, and what we can do about them. While I don’t think that {WAW} is among the most important longtermist considerations, perhaps a few small projects in this area are worthwhile. Scenarios where humans have the biggest impact on {WAW} seem to be about spreading wildlife.},
	url = {https://forum.effectivealtruism.org/posts/MKmowJNCeJCaitK3x/wild-animal-welfare-in-the-far-future},
	journaltitle = {Effective Altruism Forum},
	author = {Šimčikas, Saulius},
	urldate = {2022-07-08},
	date = {2022-07-08},
	langid = {english},
	file = {~/Google Drive/library-pdf/Simcikas2022WildAnimalWelfare.pdf;~/Google Drive/library-html/wild-animal-welfare-in-the-far-future.html}
}

@book{Singer1981ExpandingCircleEthics,
	database = {Tlön},
	location = {Oxford},
	langid = {english},
	title = {The expanding circle: Ethics and sociobiology},
	isbn = {978-0-19-824646-6},
	publisher = {Clarendon Press},
	author = {Singer, Peter},
	date = {1981},
	file = {~/Google Drive/library-pdf/Singer1981ExpandingCircleEthics.pdf}
}

@InBook{Singer1999TodosAnimalesSon,
	database = {Tlön},
	file = {~/Google Drive/library-pdf/Singer1999TodosAnimalesSon.pdf},
	pages = {37--60},
	langid = {english},
	timestamp = {2023-02-24 23:45:49 (GMT)}
}

@article{Singer2006WhatShouldBillionaire,
	database = {Tlön},
	title = {What should a billionaire give — and what should you?},
	abstract = {A philosopher’s case for donating more than you’re comfortable with.},
	langid = {english},
	url = {https://www.nytimes.com/2006/12/17/magazine/17charity.t.html},
	journaltitle = {The New York Times magazine},
	author = {Singer, Peter},
	date = {2006-12-17}
}

@online{Singer2023HambreRiquezaMoralidad,
	database = {Tlön},
	date = {2023},
	title = {Hambre, riqueza y moralidad},
	author = {Singer, Peter},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Singer1972FamineAffluenceMorality}
}

@online{Sinick2013ManyWeakArguments,
	database = {Tlön},
	file = {~/Google Drive/library-html/Sinick2013ManyWeakArguments.html;~/Google Drive/library-pdf/Sinick2013ManyWeakArguments.pdf},
	journaltitle = {Lesswrong},
	date = {2013-06-04},
	author = {Sinick, Jonah},
	title = {Many weak arguments vs. one relatively strong
                  argument},
	url = {https://www.lesswrong.com/posts/9W9P2snxu5Px746LD/many-weak-arguments-vs-one-relatively-strong-argument},
	langid = {english},
	timestamp = {2023-02-18 12:21:39 (GMT)},
	urldate = {2023-02-18}
}

@online{Sinick2023MuchosArgumentosDebiles,
	date = {2023},
	title = {Muchos argumentos débiles contra un argumento
                  relativamente fuerte},
	database = {Tlön},
	author = {Sinick, Jonah},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Sinick2013ManyWeakArguments}
}

@book{Smart1961OutlineSystemUtilitarian,
	database = {Tlön},
	location = {Victoria},
	langid = {english},
	title = {An outline of a system of utilitarian ethics},
	publisher = {Melbourne University Press},
	author = {Smart, J. J. C.},
	date = {1961},
	file = {~/Google Drive/library-pdf/Smart1961OutlineSystemUtilitarian.pdf}
}

@book{Smart1984EthicsPersuasionTruth,
	database = {Tlön},
	location = {London},
	langid = {english},
	title = {Ethics, persuasion and truth},
	isbn = {0-7102-0245-8},
	publisher = {Routledge \& Kegan Paul},
	author = {Smart, J. J. C.},
	date = {1984},
	file = {~/Google Drive/library-pdf/Smart1984EthicsPersuasionTruth.pdf}
}

@incollection{Smith2010CubanMissileCrisis,
	database = {Tlön},
	location = {Oxford},
	abstract = {The article introduces the concept of crisis management, emphasizing its importance in minimizing threats, identifying shortcomings, and preventing future crises. It explores the variables contributing to poor and successful crisis management, aiming to enhance decision-making and organizational preparedness. The analysis seeks to understand how leaders and organizations can better respond to, manage, and recover from crises. The exploration draws on historical examples and relevant studies to shed light on responsible decision-making and effective crisis management amidst evolving threats and complexities. – AI-generated abstract.},
	langid = {english},
	title = {Cuban Missile Crisis},
	isbn = {978-0-19-533468-5},
	pages = {518–521},
	booktitle = {The Oxford international encyclopedia of peace},
	publisher = {Oxford University Press},
	author = {Smith, E. Timothy},
	editor = {Young, Nigel},
	date = {2010},
	file = {~/Google Drive/library-pdf/Smith2010CubanMissileCrisis.pdf}
}

@online{Smith2017DifficultyInterstellarTravel,
	database = {Tlön},
	title = {The difficulty of interstellar travel for humans},
	abstract = {Space exploration, particularly interstellar travel, faces formidable challenges due to the vast distances involved and the limitations of current propulsion technologies. Chemical rockets are insufficient for interstellar travel, requiring enormous amounts of fuel. Even with advanced propulsion systems like nuclear pulse propulsion, the energy and mass requirements for crewed missions are immense. Uncrewed probes offer a more feasible option, using light sails or laser propulsion to reach distant stars. With ongoing advancements in technology and artificial intelligence, robotic exploration of interstellar space appears more achievable than human missions. – AI-generated abstract.},
	langid = {english},
	url = {https://selfawarepatterns.com/2017/07/22/the-difficulty-of-interstellar-travel-for-humans/},
	journaltitle = {{SelfAwarePatterns}},
	author = {Smith, Mike},
	date = {2017-07-22},
	file = {~/Google Drive/library-pdf/Smith2017DifficultyInterstellarTravel.pdf}
}

@online{Snodin2022MyThoughtsNanotechnology,
	database = {Tlön},
	title = {My thoughts on nanotechnology strategy research as an
                  {EA} cause area},
	abstract = {This post has two main goals:.
.
To provide a resource that {EA} community members can use to improve their understanding of advanced nanotechnology and nanotechnology strategy.
To make a case for nanotechnology strategy research being valuable from a longtermist {EA} perspective in order to get more people to consider working on it.
.
If you’re mostly interested in the second point, feel free to quickly skim through the first parts of the post, or maybe to skip directly to How to prioritise nanotechnology strategy research.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/oqBJk2Ae3RBegtFfn/my-thoughts-on-nanotechnology-strategy-research-as-an-ea},
	journaltitle = {Effective Altruism Forum},
	author = {Snodin, Ben},
	urldate = {2022-05-02},
	date = {2022-05-02},
	file = {~/Google Drive/library-pdf/MyThoughtsNanotechnologya.pdf;~/Google Drive/library-html/my-thoughts-on-nanotechnology-strategy-research-as-an-ea.html}
}

@online{Snyder-Beattie2022ConcreteBiosecurityProjects,
	database = {Tlön},
	file = {~/Google Drive/library-pdf/Snyder-Beattie2022ConcreteBiosecurityProjects.pdf;~/Google Drive/library-html/Snyder-Beattie2022ConcreteBiosecurityProjects.html},
	abstract = {This is a list of longtermist biosecurity projects.  We think most of them could reduce catastrophic biorisk by more than 1\% or so on the current margin (in relative  terms).  While we are confident there is important work to be done within each of these areas, our confidence in specific pathways varies widely and the particulars of each idea have not been investigated thoroughly.},
	langid = {english},
	author = {Snyder-Beattie, Andrew and Alley, Ethan},
	date = {2022-01-10},
	timestamp = {2023-02-18 14:15:48 (GMT)},
	title = {Concrete biosecurity projects (some of which could be
                  big)},
	journaltitle = {Effective Altruism Forum},
	url = {https://forum.effectivealtruism.org/s/vSAFjmWsfbMrTonpq/p/u5JesqQ3jdLENXBtB},
	urldate = {2023-02-18}
}

@online{Soares2014Caring,
	database = {Tlön},
	file = {~/Google Drive/library-html/Soares2014Caring.html;~/Google Drive/library-pdf/Soares2014Caring.pdf},
	abstract = {The paper presents the strategy of the London branch of a social movement called Effective Altruism (EA), a community of individuals who seek to find ways to benefit as many people as possible and as effectively as possible, including through organized efforts such as donating money. The strategy is focused on activities that will coordinate and support people in London who are interested in EA, by organizing different types of events and providing resources. The document includes a list of the activities that will be conducted and the metrics that will be used to measure the strategy’s effectiveness – AI-generated abstract.},
	langid = {english},
	author = {Soares, Nate},
	date = {2014-10-06},
	timestamp = {2023-02-17 21:34:57 (GMT)},
	title = {On caring},
	journaltitle = {Effective Altruism Forum},
	url = {https://forum.effectivealtruism.org/posts/hkimyETEo76hJ6NpW/on-caring},
	urldate = {2023-02-17}
}

@online{Soares2021CommentsCarlsmithPowerseeking,
	database = {Tlön},
	title = {Comments on Carlsmith's 'Is power-seeking {AI} an
                  existential risk?'},
	abstract = {The following are some comments I gave on Open Philanthropy Senior Research Analyst Joe Carlsmith’s Apr. 2021 “Is power-seeking {AI} an existential risk?”.},
	url = {https://www.lesswrong.com/posts/cCMihiwtZx7kdcKgt/comments-on-carlsmith-s-is-power-seeking-ai-an-existential},
	shorttitle = {Comments on Carlsmith's “Is power-seeking {AI} an
                  existential risk?},
	journaltitle = {{LessWrong}},
	author = {Soares, Nate},
	urldate = {2022-07-27},
	date = {2021-11-13},
	langid = {english},
	file = {~/Google Drive/library-html/comments-on-carlsmith-s-is-power-seeking-ai-an-existential.html;~/Google Drive/library-pdf/Soares2021CommentsCarlsmithPowerseeking.pdf}
}

@online{Soares2022HowVariousPlans,
	database = {Tlön},
	title = {On how various plans miss the hard bits of the
                  alignment challenge},
	abstract = {Many current AI alignment approaches propose to route around the key problem of alignment, by aiming to have humans + narrow AI services perform pivotal acts. Yet, the central alignment difficulty rears its ugly head by default, in a pretty robust way. Addressing it just requires gradient descent (or some other general-purpose optimizer) to be able to optimize over different concepts of our choosing. The problem is that an AI system's conceptual repertoire won't, in general, smoothly extrapolate beyond its training distribution, as capabilities generalize. Consequently, an AI will not reliably follow our goals outside of its training distribution, even if it was previously well-aligned within it. This alignment challenge seems fundamentally different from, and more difficult than, training an AI to succeed at new capabilities. – AI-generated abstract.},
	url = {https://www.alignmentforum.org/posts/3pinFH3jerMzAvmza/on-how-various-plans-miss-the-hard-bits-of-the-alignment},
	journaltitle = {{AI} Alignment Forum},
	author = {Soares, Nate},
	urldate = {2022-07-13},
	date = {2022-07-12},
	langid = {english},
	file = {~/Google Drive/library-pdf/Soares2022HowVariousPlans.pdf}
}

@online{Soares2023AcercaDeQue,
	database = {Tlön},
	date = {2023},
	title = {Acerca de lo que nos preocupa},
	author = {Soares, Nate},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Soares2014Caring}
}

@book{Sorensen1965Kennedy,
	database = {Tlön},
	location = {New York},
	langid = {english},
	title = {Kennedy},
	publisher = {Harper \& Row},
	author = {Sorensen, Ted},
	date = {1965},
	file = {~/Google Drive/library-pdf/Sorensen1965Kennedy.pdf}
}

@online{Sotala2012HeuristicsBiasesCharity,
	database = {Tlön},
	title = {Heuristics and biases in charity},
	langid = {english},
	url = {https://www.lesswrong.com/posts/hiiziojg3R5uwQPm9/heuristics-and-biases-in-charity},
	journaltitle = {{LessWrong}},
	author = {Sotala, Kaj},
	date = {2012-03-02}
}

@online{Sotala2014EffectiveAltruismMost,
	database = {Tlön},
	title = {Effective altruism as the most exciting cause in the
                  world},
	abstract = {I feel that one thing that effective altruists haven't sufficiently capitalized on in their marketing is just how amazingly exciting the whole thing is. There's Holden Karnofsky's post on excited altruism, but it doesn't really go into the details of why effective altruism is so exciting. So let me try to fix that.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/LwmEr3B9dpBrFq3du/effective-altruism-as-the-most-exciting-cause-in-the-world},
	journaltitle = {Effective Altruism Forum},
	author = {Sotala, Kaj},
	date = {2014-09-26},
	file = {~/Google Drive/library-pdf/Sotala2014EffectiveAltruismMost.pdf}
}

@incollection{Sotala2019DisjunctiveScenariosCatastrophic,
	database = {Tlön},
	location = {Boca Raton, {FL}},
	langid = {english},
	title = {Disjunctive Scenarios of Catastrophic {AI} Risk},
	isbn = {978-0-8153-6982-0},
	pages = {315–338},
	booktitle = {Artificial Intelligence Safety and Security},
	publisher = {Taylor \& Francis},
	author = {Sotala, Kaj},
	editor = {Yampolskiy, Roman V.},
	date = {2019},
	file = {~/Google Drive/library-pdf/Sotala2018DisjunctiveScenariosCatastrophic.pdf;~/Google Drive/library-pdf/Sotala2018DisjunctiveScenariosCatastrophic.pdf}
}

@incollection{Spitz1962FreedomIndividualityMill,
	database = {Tlön},
	location = {New York},
	langid = {english},
	title = {Freedom and individuality: Mill's Liberty in
                  retrospect},
	pages = {176–226},
	booktitle = {Nomos {IV}: Liberty},
	publisher = {Atherton Press},
	author = {Spitz, David},
	editor = {Friedrich, Carl J.},
	date = {1962}
}

@article{Srinivasan2015StopRobotApocalypse,
	database = {Tlön},
	title = {Stop the robot apocalypse},
	abstract = {Philosophers may talk about justice or rights, but they don’t often try to reshape the world according to their ideals...},
	langid = {english},
	volume = {37},
	url = {https://www.lrb.co.uk/the-paper/v37/n18/amia-srinivasan/stop-the-robot-apocalypse},
	pages = {1–10},
	number = {18},
	journaltitle = {London Review of Books},
	author = {Srinivasan, Amia},
	date = {2015-09-24},
	note = {tex.entrysubtype: magazine},
	file = {~/Google Drive/library-pdf/Srinivasan2015StopRobotApocalypse.pdf}
}

@online{Stafforini2014PaulChristianoCause,
	database = {Tlön},
	title = {Paul christiano on cause prioritization},
	abstract = {Paul Christiano is a graduate student in computer science at {UC} Berkeley. His academic research interests include algorithms and quantum computing. Outside academia, he has written about various topics of interest to effective altruists, with a focus on the far future.  Christiano holds a {BA} in mathematics from {MIT} and has represented the United States at the International Mathematical Olympiad. He is a Research Associate at the Machine Intelligence Research Institute and a Research Advisor at 80,000 Hours.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/b6y9zSkRtxvKSdqcc/paul-christiano-on-cause-prioritization},
	journaltitle = {Effective Altruism Forum},
	author = {Stafforini, Pablo},
	date = {2014-03-23},
	file = {~/Google Drive/library-pdf/Stafforini2014PaulChristianoCause.pdf}
}

@online{Stafforini2016ElectoralReformInitiatives,
	database = {Tlön},
	title = {Electoral reform initiatives: historical data},
	url = {https://docs.google.com/spreadsheets/d/14Se60Gic7PMpv4-cY-gw2L4HjKMYf9sb7ztRIXEJ4wQ/edit?usp=drive_web&ouid=105467680319054692970&usp=embed_facebook},
	shorttitle = {Electoral reform initiatives},
	journaltitle = {Google Docs},
	author = {Stafforini, Pablo},
	urldate = {2021-08-02},
	date = {2016},
	langid = {english},
	file = {~/Google Drive/library-html/edit.html}
}

@online{Stafforini2022HowManyLives,
	database = {Tlön},
	title = {How many lives has the U.S. President's Emergency Plan
                  for {AIDS} Relief ({PEPFAR}) saved?},
	abstract = {Open Philanthropy's recent blog post announcing new hires in South Asian Air Quality and Global Aid Advocacy states that the U.S. President's Emergency Plan for {AIDS} Relief ({PEPFAR}) "has plausibly saved tens of millions of life-years since it was created in 2003." The post provides a link to a {PEPFAR} page that credits the program with "saving over 20 million lives", but unfortunately that page doesn't give a source for the estimate. The same estimate is mentioned in the {PEPFAR} Wikipedia page, but the other listed sources either merely repeat that figure or do not even give an estimate (Fauci \& Ensinger 2018 is entitled '{PEPFAR} — 15 Years and Counting the Lives Saved' but the paper does not actually count the lives saved by the program; it rather lists various intermediate outputs such as the number of people given retroviral therapy or the number of voluntary male circumcisions supported).},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/f2xEp9RAyA2kSZ2qm/how-many-lives-has-the-u-s-president-s-emergency-plan-for},
	shorttitle = {How many lives has the U.S. President's Emergency Plan
                  for {AIDS} Relief ({PEPFAR}) saved?},
	journaltitle = {Effective Altruism Forum},
	author = {Stafforini, Pablo},
	urldate = {2022-01-28},
	date = {2022-01-18},
	file = {~/Google Drive/library-html/how-many-lives-has-the-u-s-president-s-emergency-plan-for.html;~/Google Drive/library-pdf/Stafforini2022HowManyLives.pdf}
}

@online{Stapp2022ProgressPolicyChoice,
	database = {Tlön},
	title = {Progress is a policy choice},
	abstract = {Introduction We’re excited to announce that today we are launching the Institute for Progress, a new think tank in Washington, D.C. Our mission is to accelerate scientific, technological, and industrial progress while safeguarding humanity's future.},
	url = {https://progress.institute/progress-is-a-policy-choice/},
	journaltitle = {Institute for Progress},
	author = {Stapp, Alec and Watney, Caleb},
	urldate = {2022-01-28},
	date = {2022-01-20},
	langid = {english},
	file = {~/Google Drive/library-html/progress-is-a-policy-choice.html;~/Google Drive/library-pdf/Stapp2022ProgressPolicyChoice.pdf}
}

@online{Steinhardt2014AnotherCritiqueOf,
	file = {~/Google Drive/library-html/Steinhardt2014AnotherCritiqueOf.html;~/Google Drive/library-pdf/Steinhardt2014AnotherCritiqueOf.pdf},
	date = {2014-01-05},
	database = {Tlön},
	journaltitle = {LessWrong},
	author = {Steinhardt, Jacob},
	title = {Another critique of effective altruism},
	url = {https://www.lesswrong.com/posts/CZmkPvzkMdQJxXy54/another-critique-of-effective-altruism},
	langid = {english},
	timestamp = {2023-09-25 18:28:02 (GMT)},
	urldate = {2023-09-25}
}

@online{Steinhardt2022AIForecastingOne,
	database = {Tlön},
	title = {{AI} forecasting: One year in},
	abstract = {The article argues that the COVID-19 pandemic has shown how vulnerable the world still is to pandemics. It proposes that the US government should invest heavily in pandemic preparedness, such as creating vaccines for each viral family, establishing an early warning system for new viruses, and strengthening public health systems. The article compares the proposed spending in the American Pandemie Preparedness Plan with the funding authorized in the PREVENT Pandemics Act and finds that the latter falls short. It concludes that the PREVENT Pandemics Act needs to be improved to provide more funding and ensure that it actually prevents the next pandemic. – AI-generated abstract.},
	url = {https://www.lesswrong.com/posts/CJw2tNHaEimx6nwNy/ai-forecasting-one-year-in},
	shorttitle = {{AI} Forecasting},
	journaltitle = {Bounded Regret},
	author = {Steinhardt, Jacob},
	urldate = {2022-07-04},
	date = {2022-07-03},
	langid = {english},
	file = {~/Google Drive/library-html/ai-forecasting-one-year-in.html;~/Google Drive/library-pdf/Steinhardt2022AIForecastingOne.pdf}
}

@online{Stocker2020ReflectingLongReflection,
	database = {Tlön},
	title = {Reflecting on the Long Reflection},
	abstract = {Could or even should humanity spend 10,000 years deciding on its future? Some of Oxford’s leading professors seem to think so….},
	url = {https://www.felixstocker.com/blog/reflecting-on-the-long-reflection},
	journaltitle = {Felix Stocker's Blog},
	author = {Stocker, Felix},
	urldate = {2021-10-03},
	date = {2020-08-14},
	langid = {english},
	file = {~/Google Drive/library-html/reflecting-on-the-long-reflection.html;~/Google Drive/library-pdf/Stocker2020ReflectingLongReflection.pdf}
}

@online{Stolyarov2020AndersSandbergInformation,
	date = {2020-07-05},
	journaltitle = {{SlateStarCodex} meetup},
	author = {{Stolyarov II}, Gennady},
	database = {Tlön},
	langid = {spanish},
	timestamp = {2023-07-20 08:45:52 (GMT)},
	title = {Anders Sandberg on Information Hazards},
	url = {https://youtu.be/Wn2vgQGNI_c},
	urldate = {2023-07-20}
}

@online{Stray2015BertrandRussellStatistical,
	database = {Tlön},
	title = {Bertrand russell on statistical empathy},
	abstract = {The mark of a civilized human is the ability to look at a column of numbers, and weep. I love this line. I've taken to calling this ability "statistical empathy." I think it's one of the core values required to do good in the modern world. This quote is widely attributed to Betrand Russell, though I don't think he actually wrote it. He may be responsible for articulating the idea, though.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/KKFg3HnWvSqa3QSZq/bertrand-russell-on-statistical-empathy},
	journaltitle = {Effective Altruism Forum},
	author = {Stray, Jonathan},
	date = {2015-09-28},
	file = {~/Google Drive/library-pdf/Stray2015BertrandRussellStatistical.pdf}
}

@online{Stuhlmuller2022OughtTheoryChange,
	database = {Tlön},
	title = {Ought's theory of change},
	abstract = {Ought is an applied machine learning lab. In this post we summarize our work on Elicit and why we think it's important.},
	url = {https://forum.effectivealtruism.org/posts/raFAKyw7ofSo9mRQ3/ought-s-theory-of-change},
	journaltitle = {Effective Altruism Forum},
	author = {Stuhlmüller, Andreas and Byun, Jungwon},
	urldate = {2022-06-15},
	date = {2022-04-11},
	langid = {english},
	file = {~/Google Drive/library-html/ought-s-theory-of-change.html;~/Google Drive/library-pdf/Stuhlmuller2022OughtTheoryChange.pdf}
}

@online{SunYin2017IntroducingOpenMined,
	database = {Tlön},
	title = {Introducing Open Mined: decentralised {AI}},
	abstract = {The article discusses a new project called Open Mined that proposes to decentralize artificial intelligence by leveraging blockchain technology. Its goal is to enable individuals to sell their data securely, in an encrypted manner, without compromising their privacy. The data is used to train AI models, and compensation is automatically determined based on the contribution of the data to the model's accuracy. This novel approach addresses the challenge of data scarcity for smaller entities and raises concerns about the lack of privacy and control over data in current centralized AI systems. – AI-generated abstract.},
	langid = {english},
	url = {https://becominghuman.ai/introducing-open-mined-decentralised-ai-18017f634a3f},
	journaltitle = {Becoming human},
	author = {Sun Yin, Awa},
	date = {2017-08-04},
	file = {~/Google Drive/library-pdf/SunYin2017IntroducingOpenMined.pdf}
}

@online{SuspendedReason2020ConceptualEngineeringRevolution,
	database = {Tlön},
	title = {Conceptual engineering: The revolution in philosophy
                  you've never heard of},
	langid = {english},
	url = {https://www.lesswrong.com/posts/9iA87EfNKnREgdTJN/conceptual-engineering-the-revolution-in-philosophy-you-ve},
	journaltitle = {{LessWrong}},
	author = {{Suspended Reason}},
	date = {2020-06-02},
	file = {~/Google Drive/library-pdf/SuspendedReason2020ConceptualEngineeringRevolution.pdf}
}

@online{Tabarrok2012BetTaxBullshit,
	database = {Tlön},
	title = {A bet is a tax on bullshit},
	abstract = {Elections have a profound effect on various facets of societies, and the accuracy of election predictions heavily influences voters' decisions. While current opinion polls provide valuable information, they are often unreliable, thereby warranting the need for more rigorous prediction methods. One promising approach is Nate Silver's election forecasting model, which has demonstrated remarkable accuracy in predicting election outcomes. However, there has been criticism regarding Silver's model, with some arguing that it is biased or lacks sufficient transparency. This article proposes that a properly structured betting system can serve as a rigorous test of the model's accuracy and mitigate concerns about bias. Specifically, the author suggests that Silver should publicly commit to betting on the election outcome, with the proceeds going to charity. Furthermore, to eliminate any possibility of bias, a blind trust should hold a portion of Silver's salary, randomly choosing which side of the bet to take and only revealing the bet and its outcome after the election. This approach incentivizes Silver to make accurate predictions, as his financial well-being would be tied to the model's performance. By embracing this betting system, Silver can demonstrate his confidence in his model and address concerns about its validity. – AI-generated abstract.},
	langid = {english},
	url = {https://marginalrevolution.com/marginalrevolution/2012/11/a-bet-is-a-tax-on-bullshit.html},
	journaltitle = {Marginal revolution},
	author = {Tabarrok, Alexander},
	date = {2012-11-02},
	file = {~/Google Drive/library-pdf/Tabarrok2012BetTaxBullshit.pdf}
}

@online{Tabarrok2022ParfitSingerAliens,
	database = {Tlön},
	title = {Parfit + Singer + Aliens = ?},
	abstract = {If you.
Have a wide moral circle that includes non human animals and Have a low or zero moral discount rate .
Then the discovery of alien life should radically change your views on existential risk.},
	url = {https://forum.effectivealtruism.org/posts/wqmY98m3yNs6TiKeL/parfit-singer-aliens},
	shorttitle = {Parfit + Singer + Aliens = ?},
	journaltitle = {Effective Altruism Forum},
	author = {Tabarrok, Maxwell},
	urldate = {2022-11-02},
	date = {2022-10-12},
	langid = {english},
	file = {~/Google Drive/library-html/parfit-singer-aliens.html;~/Google Drive/library-pdf/Tabarrok2022ParfitSingerAliens.pdf}
}

@article{Taennsjoe1992WhoAreThe,
	author = {T{\"a}nnsj{\"o}, Torbj{\"o}rn},
	title = {Who are the beneficiaries?},
	volume = {6},
	number = {4},
	pages = {288–296},
	doi = {10.1111/j.1467-8519.1992.tb00207.x},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1467-8519.1992.tb00207.x},
	database = {Tlön},
	date = {1992-10},
	issn = {0269-9702, 1467-8519},
	journaltitle = {Bioethics},
	langid = {english},
	shortjournal = {Bioethics},
	timestamp = {2023-07-12 16:00:44 (GMT)},
	urldate = {2023-07-12}
}

@online{Tanagrabeast2021SevenYearsSpaced,
	database = {Tlön},
	title = {Seven years of spaced repetition software in the
                  classroom},
	abstract = {This article reports on the author’s multi-year experience using spaced repetition software (SRS) with students in an American high school English classroom. The initial setup involved whole-class, teacher-led SRS study sessions, relying on a modified version of the Anki SRS. As the years passed, this method evolved from a focus on reviewing all potentially relevant content to a more limited focus on word fragments that would afford high automaticity. Moreover, direct teacher involvement shifted to more of a supervisory role, with students conducting their own review sessions using laptops or smartphones. Positive outcomes were noted, including increased student engagement, improved efficiency, and reduced teacher workload. However, the author found that gains were modest, especially with undermotivated students, and that SRS performance did not translate into better performance on external tests. Furthermore, there was evidence that students’ mental models became dominated by item-specific associations rather than more conceptual, generalizable knowledge. The author argues that, in light of these findings, teachers should use SRS strategically, with a clear understanding of its potential benefits and limitations – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/F6ZTtBXn2cFLmWPdM/seven-years-of-spaced-repetition-software-in-the-classroom-1},
	journaltitle = {{LessWrong}},
	author = {{Tanagrabeast}},
	date = {2021-03-04},
	file = {~/Google Drive/library-pdf/Tanagrabeast2021SevenYearsSpaced.pdf}
}

@report{Tarsney2022EpistemicChallengeLongtermism,
	database = {Tlön},
	title = {The epistemic challenge to longtermism},
	langid = {english},
	url = {https://www.dropbox.com/s/mf0lvkxxkglwhpf/Epistemic%20Challenge%20to%20Longtermism.pdf?dl=0},
	institution = {Global Priorities Institute, University of Oxford},
	author = {Tarsney, Christian},
	date = {2022-05},
	number = {{GPI} Working Paper No. 3-2022},
	file = {~/Google Drive/library-pdf/Tarsney2022EpistemicChallengeLongtermism.pdf}
}

@article{Taylor-Robinson2019PublicHealthDeworming,
	doi = {10.1002/14651858.CD000371.pub7},
	database = {Tlön},
	langid = {english},
	title = {Public health deworming programmes for
                  soil-transmitted helminths in children living in
                  endemic areas},
	number = {issue 9, art. no. {CD}000371},
	journaltitle = {Cochrane Database of Systematic Reviews},
	author = {Taylor‐Robinson, David C. and Maayan, Nicola and
                  Donegan, Sarah and Chaplin, Marty and Garner, Paul},
	date = {2019},
	file = {~/Google Drive/library-pdf/Taylor-Robinson2019PublicHealthDeworming.pdf}
}

@online{Telleen-Lawton2020DonorLotteryDebrief,
	database = {Tlön},
	title = {Donor lottery debrief},
	abstract = {Good news, I've finally allocated the rest of the donor lottery funds from the 2016-2017 Donor Lottery (the first one in our community)! It took over 3 years but I'm excited about the two projects I funded.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/QjFJKMSTbTpvsoMvh/donor-lottery-debrief},
	journaltitle = {Effective Altruism Forum},
	author = {Telleen-Lawton, Timothy},
	urldate = {2021-12-07},
	date = {2020-08-04},
	file = {~/Google Drive/library-html/donor-lottery-debrief.html;~/Google Drive/library-pdf/Telleen-Lawton2020DonorLotteryDebrief.pdf}
}

@online{Tench2023ValorExtraordinarioDe,
	database = {Tlön},
	date = {2023},
	title = {El valor extraordinario de las normas ordinarias},
	author = {Tench, Emily},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Tench2017ExtraordinaryValueOf}
}

@online{Teran2022PreventingPandemicsRequires,
	database = {Tlön},
	file = {~/Google Drive/library-pdf/Teran2022PreventingPandemicsRequires.pdf;~/Google Drive/library-html/Teran2022PreventingPandemicsRequires.html},
	abstract = {This article traces the roots of the Green Revolution to programs undertaken by the Rockefeller Foundation that invested in agricultural research and extension services, first in the United States, then in Mexico, then in other countries in Latin America and Asia by means of the IRRI and other similar institutes. The focus shifted during the late 1960s to international centers that addressed development globally, including on environmental issues. – AI-generated abstract.},
	date = {2022-03-14},
	author = {Teran, Nikki},
	journaltitle = {Institute for Progress},
	langid = {english},
	timestamp = {2023-06-08 08:01:44 (GMT)},
	title = {Preventing Pandemics Requires Funding},
	url = {https://progress.institute/preventing-pandemics-requires-funding/},
	urldate = {2023-06-08}
}

@collection{Tetlock1989BehaviorSocietyNuclear,
	database = {Tlön},
	location = {New York},
	langid = {english},
	title = {Behavior, society, and nuclear war},
	isbn = {978-0-19-505765-2 978-0-19-505766-9 978-0-19-505767-6
                  978-0-19-505768-3 978-0-19-505769-0},
	pagetotal = {1},
	publisher = {Oxford University Press},
	editor = {Tetlock, Philip E. and National Research Council
                  (U.S.) and National Research Council (U.S.)},
	date = {1989}
}

@report{TheAtlanticPhilanthropies2008WhySupportingAdvoacy,
	database = {Tlön},
	title = {Why supporting advoacy makes sense for foundations},
	abstract = {M męskad has long been used as a strategy to advance social change, but foundation funding to support advocacy is growing. Funders support advocacy for a range of reasons including their responsibility to address important issues, the unique role advocacy plays in policy-making, advocacy’s cost-eффекtive way to achieve policy change, and the public return on investment from successful policy change. Across these motivations, funders who intentionally consider their support for advocacy can increase both the efficiency and effective of their funding. –AI-generated abstract},
	langid = {english},
	url = {https://www.atlanticphilanthropies.org/research-reports/atlantic-reportinvesting-change-why-supporting-advocacy-makes-sense-foundations},
	institution = {The Atlantic Philanthropies},
	author = {{The Atlantic Philanthropies}},
	date = {2008-05},
	file = {~/Google Drive/library-pdf/TheAtlanticPhilanthropiesWhySupportingAdvoacy.pdf}
}

@online{TheInstituteforTechnologyLawPolicy2019GeorgetownLaunchesNew,
	database = {Tlön},
	title = {Georgetown launches new \$55 million Center on
                  Security \& Emerging Technology},
	langid = {english},
	url = {https://www.georgetowntech.org/news-fullposts/2019/2/27/february-27-2019-georgetown-launches-new-55-million-center-on-security-amp-emerging-technologies-sll62},
	journaltitle = {The Institute for Technology Law \& Policy},
	author = {{The Institute for Technology Law \& Policy}},
	date = {2019-02-28}
}

@report{ThePresidentsCouncilonBioethics2003TherapyBiotechnologyPursuit,
	url = {https://biotech.law.lsu.edu/research/pbc/reports/beyondtherapy/beyond_therapy_final_report_pcbe.pdf},
	database = {Tlön},
	langid = {english},
	location = {Washington, D.C.},
	title = {Beyond therapy: Biotechnology and the pursuit of
                  happiness},
	author = {{The President's Council on Bioethics}},
	date = {2003},
	file = {~/Google Drive/library-pdf/ThePresidentsCouncilonBioethics2003TherapyBiotechnologyPursuit.pdf;~/Google Drive/library-pdf/ThePresidentsCouncilonBioethics2003TherapyBiotechnologyPursuit.pdf}
}

@incollection{Thomas1974PhilosophicRadicals,
	database = {Tlön},
	location = {London},
	langid = {english},
	title = {The philosophic radicals},
	pages = {52–79},
	booktitle = {Pressure from without in early victorian england},
	publisher = {Edward Arnold},
	author = {Thomas, William},
	editor = {Hollis, Patricia},
	date = {1974}
}

@online{Thorstad2022ExistentialRiskPessimisma,
	database = {Tlön},
	title = {Existential risk pessimism and the time of perils},
	abstract = {Many {EAs} endorse two claims about existential risk. First, existential risk is currently high: (Existential Risk Pessimism) Per-century existential risk is very high. For example, Toby Ord (2020) puts the risk of existential catastrophe by 2100 at 1/6, and participants at the Oxford Global Catastrophic Risk Conference in 2008 estimated a median 19\% chance of human extinction by 2100 (Sandberg and Bostrom 2008). Let’s ballpark Pessimism using a 20\% estimate of per-century risk. Second, many {EAs} think that it is very important to mitigate existential risk: (Astronomical Value Thesis) Efforts to mitigate existential risk have astronomically high expected value. You might think that Existential Risk Pessimism supports the Astronomical Value Thesis. After all, it is usually more important to mitigate large risks than to mitigate small risks. In this post, I extend a series of models due to Toby Ord and Tom Adamczewski to do five things: I show that across a range of assumptions, Existential Risk Pessimism tends to hamper, not support the Astronomical Value Thesis.I argue that the most plausible way to combine Existential Risk Pessimism with the Astronomical Value Thesis is through the Time of Perils Hypothesis. I clarify two features that the Time of Perils Hypothesis must have if it is going to vindicate the Astronomical Value Thesis.I suggest that arguments for the Time of Perils Hypothesis which do not appeal to {AI} are not strong enough to ground the relevant kind of Time of Perils Hypothesis.I draw implications for existential risk mitigation as a cause area.},
	url = {https://forum.effectivealtruism.org/posts/N6hcw8CxK7D3FCD5v/existential-risk-pessimism-and-the-time-of-perils-4},
	journaltitle = {Effective Altruism Forum},
	author = {Thorstad, David},
	urldate = {2022-08-12},
	date = {2022-08-12},
	langid = {english},
	file = {~/Google Drive/library-html/existential-risk-pessimism-and-the-time-of-perils-4.html;~/Google Drive/library-pdf/Thorstad2022ExistentialRiskPessimismb.pdf}
}

@InBook{Titelbaum2015RationalityFixedPoint,
	editor = {Gendler, Tamar Szabó and Hawthorne, John},
	isbn = {9780198722762},
	abstract = {This work is a major biennial volume offering a regular snapshot of state-of-the-art work in this important field of epistemology. Topics addressed in Volume 5 include knowledge of abstracta, the nature of evidential support, epistemic and rational norms, fallibilism, closure principles, disagreement, the analysis of knowledge, and a priori justification. Papers make use of a variety different tools and insights, including those of formal epistemology and decision theory, as well as traditional philosophical analysis and argumentation.},
	langid = {english},
	publisher = {Oxford University Press},
	address = {Oxford},
	booktitle = {Oxford Studies in Epistemology},
	database = {Tlön},
	title = {Rationality’s fixed point (or: in defense of right reason)},
	volume = {5},
	doi = {10.1093/acprof:oso/9780198722762.003.0009},
	pages = {253–294},
	journaltitle = {Oxford studies in epistemology},
	author = {Titelbaum, Michael G.},
	date = {2015},
	file = {~/Google Drive/library-pdf/Titelbaum2015RationalityFixedPoint.pdf}
}

@article{Todd2012OxfordLeftReview,
	database = {Tlön},
	title = {The Oxford Left Review},
	langid = {english},
	author = {Todd, Benjamin and Farquhar, Sebastian and Myers, Matt
                  and Leon, David and Hubbard, Owen},
	date = {2012},
	file = {~/Google Drive/library-pdf/Todd2012OxfordLeftReview.pdf}
}

@online{Todd2014WeReviewed60,
	database = {Tlön},
	title = {We reviewed over 60 studies about what makes for a
                  dream job. Here's what we found},
	abstract = {We reviewed all the research, drawing on over 60 studies, and found the six key factors for true job satisfaction. None of them is 'following your passion'.},
	langid = {english},
	url = {https://80000hours.org/career-guide/job-satisfaction/},
	journaltitle = {80,000 Hours},
	author = {Todd, Benjamin},
	date = {2014-08-02},
	file = {~/Google Drive/library-pdf/Todd2014WeReviewed60.pdf}
}

@online{Todd2015WeCareWALYs,
	database = {Tlön},
	title = {We care about {WALYs} not {QALYs}},
	abstract = {I often see media coverage of effective altruism that says "effective altruists want to maximise the number of {QALYs} in the world." (e.g. London Review of Books).
This is wrong. {QALYs} only measure health, and health is not all that matters. Most effective altruists care about increasing the number of "{WALYs}" or well-being adjusted life years, where health is just one component of wellbeing.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/nevDBjuCPMCuaoMYT/we-care-about-walys-not-qalys},
	journaltitle = {Effective Altruism Forum},
	author = {Todd, Benjamin},
	date = {2015-11-13},
	file = {~/Google Drive/library-pdf/Todd2015WeCareWALYs.pdf}
}

@online{Todd2020ArgumentKeepingOpen,
	database = {Tlön},
	title = {An argument for keeping open the option of earning to
                  save},
	abstract = {I used to think that earning to save is mainly of interest to the most ‘patient’ longtermists, but I’ve realised that there’s a broader argument for keeping it open as an option, which would mean placing a somewhat higher value on career capital relevant to high earning roles.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/J5aYvsiLoAC46DSuY/an-argument-for-keeping-open-the-option-of-earning-to-save},
	journaltitle = {Effective Altruism Forum},
	author = {Todd, Benjamin},
	date = {2020-08-31},
	file = {~/Google Drive/library-pdf/Todd2020ArgumentKeepingOpen.pdf}
}

@online{Todd2020ReviewValueDrift,
	database = {Tlön},
	title = {Review of 'value drift' estimates, and several new
                  estimates},
	abstract = {Note that the samples involve people with very different levels of engagement, and it can be misleading to compare them side-by-side. Please see the relevant sections for more explanation of each figure.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/eRQe4kkkH2pPzqvam/review-of-value-drift-estimates-and-several-new-estimates},
	journaltitle = {Effective Altruism Forum},
	author = {Todd, Benjamin},
	date = {2020-08-29},
	file = {~/Google Drive/library-pdf/Todd2020ReviewValueDrift.pdf}
}

@online{Todd2021LongtermAIPolicy,
	database = {Tlön},
	title = {Long-term {AI} policy strategy research and
                  implementation},
	abstract = {Advanced AI systems could have massive impacts on humanity and potentially pose global catastrophic risks. There are opportunities in AI governance and coordination around these threats to shape how society responds to and prepares for the challenges posed by the technology. Given the high stakes, pursuing this career path could be many people’s highest-impact option. But they should be very careful not to accidentally exacerbate the threats rather than mitigate them.},
	url = {https://80000hours.org/career-reviews/ai-policy-and-strategy/},
	journaltitle = {80,000 Hours},
	author = {Todd, Benjamin},
	urldate = {2022-03-27},
	date = {2021-10},
	langid = {english},
	file = {~/Google Drive/library-html/ai-policy-and-strategy.html;~/Google Drive/library-pdf/Todd2021LongtermAIPolicy.pdf}
}

@online{Todd2023ArgumentosFavorDe,
	database = {Tlön},
	date = {2023},
	title = {Argumentos a favor de reducir el riesgo existencial},
	author = {Todd, Benjamin},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Todd2017CaseReducingExistential}
}

@online{Todd2023LargoplacismoImportanciaMoral,
	database = {Tlön},
	date = {2023},
	title = {Largoplacismo: la importancia moral de las
                  generaciones futuras},
	author = {Todd, Benjamin},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Todd2017LongtermismMoralSignificance}
}

@online{Toenniessen2016PractitionerHistoryGreen,
	database = {Tlön},
	title = {A practitioner’s history of the Green Revolution},
	abstract = {The Center on Long-Term Risk's annual report for 2021 is reviewed. Tangible outputs and activities of the organization are cataloged with subjective assessments of their progress towards reducing the worst-case risks from the development and deployment of advanced AI systems in order to reduce the worst risks of astronomical suffering. Plans for 2022 research are reported. – AI-generated abstract.},
	url = {https://histphil.org/2016/02/08/a-practitioners-history-of-the-green-revolution/},
	journaltitle = {{HistPhil}},
	author = {Toenniessen, Gary},
	urldate = {2022-02-07},
	date = {2016-02-08},
	langid = {english},
	file = {~/Google Drive/library-html/a-practitioners-history-of-the-green-revolution.html;~/Google Drive/library-pdf/Toenniessen2016PractitionerHistoryGreen.pdf}
}

@online{Tomasik2009ShouldWeIntervene,
	database = {Tlön},
	title = {Should we intervene in nature?},
	langid = {english},
	url = {https://reducing-suffering.org/should-we-intervene-in-nature/},
	journaltitle = {Essays on Reducing Suffering},
	author = {Tomasik, Brian},
	date = {2009},
	eventdate = {2016-04-07}
}

@online{Tomasik2016HabitatLossNot,
	database = {Tlön},
	journal = {Essays on Reducing Suffering},
	langid = {english},
	url = {https://reducing-suffering.org/habitat-loss-not-preservation-generally-reduces-wild-animal-suffering/},
	date = {2016-02-07},
	title = {Habitat loss, not preservation, generally reduces wild-animal suffering},
	author = {Tomasik, Brian},
	timestamp = {2023-06-01 09:07:32 (GMT)}
}

@online{Tomasik2020Resume,
	database = {Tlön},
	title = {Résumé},
	langid = {english},
	url = {https://briantomasik.com/resume/},
	journaltitle = {Brian Tomasik's Website},
	author = {Tomasik, Brian},
	date = {2020}
}

@online{Tomasik2023PorQueActivistas,
	database = {Tlön},
	date = {2023},
	title = {Por qué los activistas deberían plantearse ganar
                  muchísimo dinero},
	author = {Tomasik, Brian},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Tomasik2006WhyActivistsShould}
}

@online{Tomasik2023RiesgosDeSufrimiento,
	database = {Tlön},
	date = {2023},
	title = {Riesgos de sufrimiento astronómico en el futuro},
	author = {Tomasik, Brian},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Tomasik2011RisksAstronomicalFuture}
}

@online{Torges2018ConsiderationsFundraisingEffective,
	database = {Tlön},
	title = {Considerations for fundraising in effective altruism},
	abstract = {How should we think about fundraising for effective charities? Are there particular sorts of donors we should focus on more? What does the distribution of opportunities tend to look like? In this talk from EAGxNetherlands 2018, Stefan Torges sketches out these and other considerations.
},
	langid = {english},
	url = {https://www.youtube.com/watch?v=SpPvPve4qao},
	journaltitle = {{EAGx} Netherlands 2018},
	author = {Torges, Stefan},
	date = {2018-07-01}
}

@online{Torges2022CLRAnnualReport,
	database = {Tlön},
	title = {{CLR}'s annual report 2021},
	abstract = {This article argues that for effective altruism, minimizing the number of hours one spends on fun, hobbies, and leisure activities is generally unwise and potentially dangerous. Using the author’s own experiences with mental health issues and breakdown, it is shown that aiming for the minimum amount of such activities can be harmful, as it increases the risk of burnout, hinders prioritization of important tasks, and leaves no room for flexibility when unexpected events occur. The author proposes that instead of pursuing the minimum, people should focus on prioritizing tasks and finding a sustainable pace of impact-oriented work. – AI-generated abstract.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/BJk3TrEzsdSiuJTKa/clr-s-annual-report-2021},
	journaltitle = {Effective Altruism Forum},
	author = {Torges, Stefan},
	urldate = {2022-03-31},
	date = {2022-02-26},
	file = {~/Google Drive/library-html/clr-s-annual-report-2021.html;~/Google Drive/library-pdf/Torges2022CLRAnnualReport.pdf}
}

@online{Torres2018WhyWeShould,
	database = {Tlön},
	title = {Why we should think twice about colonizing space},
	abstract = {My conclusion is that in a colonized universe the probability of the annihilation of the human race could actually rise rather than fall.},
	url = {https://nautil.us/why-we-should-think-twice-about-colonizing-space-7525/},
	journaltitle = {Nautilus},
	author = {Torres, Phil},
	urldate = {2022-02-15},
	date = {2018-07-18},
	langid = {english},
	file = {~/Google Drive/library-html/why-we-should-think-twice-about-colonizing-space-7525.html;~/Google Drive/library-pdf/Torres2018WhyWeShould.pdf}
}

@online{Townsend2022AnnouncingLongtermismFund,
	database = {Tlön},
	title = {Announcing the Longtermism Fund},
	abstract = {Longview Philanthropy and Giving What We Can would like to announce a new fund for donors looking to support longtermist work: the Longtermism Fund.
In this post, we outline the motivation behind the fund, reasons you may (or may not) choose to donate using it, and some questions we expect donors may have.},
	url = {https://forum.effectivealtruism.org/posts/f7qAfcKArzYrBG7RB/announcing-the-longtermism-fund},
	journaltitle = {Effective Altruism Forum},
	author = {Townsend, Michael and Dhaliwal, Simran and Harris,
                  Kit},
	urldate = {2022-08-11},
	date = {2022-08-11},
	langid = {english},
	file = {~/Google Drive/library-html/announcing-the-longtermism-fund.html}
}

@online{Trammell2019PotatiumShockwave,
	database = {Tlön},
	title = {The potatium shockwave},
	langid = {english},
	url = {https://www.philiptrammell.com/blog/41},
	journaltitle = {Philip Trammell's blog},
	author = {Trammell, Philip},
	date = {2019-08-01}
}

@online{Trammell2020PhilanthropyTimingHinge,
	database = {Tlön},
	title = {Philanthropy timing and the hinge of history},
	abstract = {If we want to donate money, should we give it away now, or invest it to give away later? The answer depends on many considerations, including our expected rate of return, the chance of our personal values changing, and the question of whether we live at the “Hinge of History” — a time with high-impact opportunities which will soon vanish.  In this talk, Phil Trammell of the Global Priorities Institute discusses how we can evaluate these considerations and change our giving strategy in response to what we learn.},
	langid = {english},
	url = {https://youtu.be/AddUn9BFFkA},
	journaltitle = {Effective Altruism Global},
	author = {Trammell, Philip},
	date = {2020-02-11}
}

@online{Treutlein2018ThreeWagersMultiversewide,
	database = {Tlön},
	title = {Three wagers for multiverse-wide superrationality},
	abstract = {In this post, I outline three wagers in favor of the hypothesis that multiverse-wide superrationality ({MSR}) has action-guiding implications.},
	langid = {english},
	url = {https://casparoesterheld.com/2018/03/31/three-wagers-for-multiverse-wide-superrationality/},
	journaltitle = {The universe from an intentional stance},
	author = {Treutlein, Johannes},
	date = {2018-03-31},
	file = {~/Google Drive/library-pdf/Treutlein2018ThreeWagersMultiversewide.pdf}
}

@online{Trotzmuller2022WhyEAsAre,
	database = {Tlön},
	title = {Why {EAs} are skeptical about {AI} Safety},
	abstract = {I interviewed 22 {EAs} who are skeptical about existential risk from Artificial General Intelligence ({AGI}), or believe that it is overrated within {EA}. This post provides a comprehensive overview of their arguments.},
	url = {https://forum.effectivealtruism.org/posts/8JazqnCNrkJtK2Bx4/why-eas-are-skeptical-about-ai-safety},
	journaltitle = {Effective Altruism Forum},
	author = {Trötzmüller, Lukas},
	urldate = {2022-07-19},
	date = {2022-07-18},
	langid = {english},
	file = {~/Google Drive/library-pdf/Trotzmuller2022WhyEAsAre.pdf}
}

@online{Tuna2008DenzelCharmsSilliman,
	database = {Tlön},
	title = {Denzel charms silliman students with ‘Sexy smile’},
	abstract = {Donning tennis shoes and a Silliman College baseball cap, two-time Academy Award winner Denzel Washington spoke to students about his life as an actor, director, […].},
	langid = {english},
	url = {https://yaledailynews.com/blog/2008/04/25/denzel-charms-silliman-students-with-sexy-smile/},
	journaltitle = {Yale Daily News},
	author = {Tuna, Cari},
	date = {2008-04-25}
}

@online{U.S.GeologicalSurvey2020QuestionsSupervolcanoes,
	database = {Tlön},
	title = {Questions about supervolcanoes},
	abstract = {The term "supervolcano" implies a volcanic center that has had an eruption of magnitude 8 on the Volcano Explosivity Index ({VEI}), meaning the measured deposits for that eruption is greater than 1,000 cubic kilometers (240 cubic miles).},
	langid = {english},
	url = {https://www.usgs.gov/volcanoes/yellowstone/questions-about-supervolcanoes},
	journaltitle = {{USGS}},
	author = {{U.S. Geological Survey}},
	date = {2020},
	file = {~/Google Drive/library-pdf/U.S.GeologicalSurvey2020QuestionsSupervolcanoes.pdf}
}

@online{UniversityofChicagoLibrary2007GuideAtomicScientists,
	database = {Tlön},
	title = {Guide to the atomic scientists of Chicago records
                  1943-1955},
	langid = {english},
	url = {https://www.lib.uchicago.edu/e/scrc/findingaids/view.php?eadid=ICU.SPCL.ASCHICAGO},
	author = {{University of Chicago Library}},
	date = {2007}
}

@incollection{Urmson1958SaintsAndHeroes,
	author = {Urmson, J. O.},
	booktitle = {Essays in Moral Philosophy},
	langid = {english},
	database = {Tlön},
	date = 1958,
	editor = {Melden, A. I.},
	publisher = {University of Washington Press},
	timestamp = {2023-06-23 21:20:18 (GMT)},
	title = {Saints and Heroes}
}

@online{Vaintrob2022AnnouncingContestEA,
	database = {Tlön},
	title = {Announcing a contest: {EA} Criticism and Red Teaming},
	abstract = {We're running a writing contest for critically engaging with theory or work in effective altruism ({EA}). Submissions can be in a range of formats (from fact-checking to philosophical critiques or major project evaluations); and can focus on a range of subject matters (from assessing empirical or normative claims to evaluating organizations and practices). We plan on distributing \$100,000, and we may end up awarding more than this amount if we get many excellent submissions.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming},
	journaltitle = {Effective Altruism Forum},
	author = {Vaintrob, Lizka and Moorhouse, Fin and Monrad, Joshua
                  Teperowski},
	urldate = {2022-03-27},
	date = {2022-06-01},
	file = {~/Google Drive/library-pdf/Vaintrob2022AnnouncingContestEA.pdf}
}

@online{Vaintrob2022LongtermistIdentity,
	database = {Tlön},
	title = {Against “longtermist” as an identity},
	abstract = {This is mostly addressing people who care a lot about improving the long-term future and helping life continue for a long time, and who might be tempted to call themselves “longtermist.”.
There have been discussions about how “effective altruist” shouldn’t be an identity and some defense of {EA}-as-identity. (I also think I’ve seen similar discussions about “longtermists” but don’t remember where.) In general, there has been a lot of good content on the effect of identities on truth-seeking conversation (see Scout Mindset or “Keep Your Identity Small” ).},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/FkFTXKeFxwcGiBTwk/against-longtermist-as-an-identity},
	journaltitle = {Effective Altruism Forum},
	author = {Vaintrob, Lizka},
	urldate = {2022-05-14},
	date = {2022-05-13},
	file = {~/Google Drive/library-html/against-longtermist-as-an-identity.html;~/Google Drive/library-pdf/VaintrobLizkaVaintrobLongtermistIdentity.pdf}
}

@online{Vaintrob2022ResourceCriticismsRed,
	database = {Tlön},
	title = {Resource for criticisms and red teaming},
	abstract = {We've put together this post to share resources and tips that might be useful for people writing criticism or working on a red team of existing work.},
	url = {https://forum.effectivealtruism.org/posts/uuQDgiJJaswEyyzan/resource-for-criticisms-and-red-teaming},
	journaltitle = {Effective Altruism Forum},
	author = {Vaintrob, Lizka and Moorhouse, Fin},
	urldate = {2022-06-02},
	date = {2022-06-01},
	langid = {english},
	file = {~/Google Drive/library-html/resource-for-criticisms-and-red-teaming.html;~/Google Drive/library-pdf/Vaintrob2022ResourceCriticismsRed.pdf}
}

@online{VanNostrand2022EpistemicLegibility,
	database = {Tlön},
	file = {~/Google Drive/library-pdf/Van2022EpistemicLegibility.pdf;~/Google Drive/library-html/Van2022EpistemicLegibility.html},
	abstract = {Excessive focus on minimalism in non-altruistic use of time and resources is dangerous and can lead to burnout and reduced impact.  Individuals should focus on prioritizing activities that maximize their well-being and impact, rather than attempting to minimize non-altruistic activities. Prioritizing involves identifying actions that have disproportionately higher impact potential and focusing efforts on those activities, while accepting trade-offs in less important areas. Sustainable and effective giving require maintaining a balance between self-care, prioritization, and maximization. – AI-generated abstract.},
	langid = {english},
	author = {Van Nostrand, Elizabeth},
	date = {2022-03-21},
	timestamp = {2023-03-06 11:52:47 (GMT)},
	title = {Epistemic Legibility},
	journaltitle = {Effective Altruism Forum},
	url = {https://forum.effectivealtruism.org/s/dg852CXinRkieekxZ/p/oRx3LeqFdxN2JTANJ},
	urldate = {2023-03-06}
}

@online{Vance2011LevelsAction,
	database = {Tlön},
	title = {Levels of action},
	abstract = {Economic development is facilitated by a higher capacity for institutionalization, which can streamline monitoring of workers; however, this can train workers to be opposed to new actions. Actions can be categorized as Level 1 actions that directly improve the world, or Level 2 actions which facilitate the effectiveness of Level 1 actions. Level 2 actions tend to have a larger impact, as their effects multiply rather than add. Level 1 actions develop understanding and skills and build chains of reliable events, facilitating society’s ability to accomplish complicated tasks. Trying new things helps one avoid hidden barriers and develop the capacity to understand Level 1 actions well enough to make their probability of failure low and construct long chains of Level 1 actions that will work reliably. – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/guDcrPqLsnhEjrPZj/levels-of-action},
	journaltitle = {{LessWrong}},
	author = {Vance, Alyssa},
	date = {2011-04-14},
	file = {~/Google Drive/library-pdf/Vance2011LevelsAction.pdf}
}

@online{Vaniver2017LWOpenBeta,
	database = {Tlön},
	title = {{LW} 2.0 open beta live},
	abstract = {The open beta of LessWrong 2.0, a revamped version of the LessWrong website, has been launched. It features a new design and upgraded codebase with improved performance and functionality. Existing users can create an account, post and read content, and provide feedback during the beta. A vote among users with over a thousand karma will determine whether the new version will replace the current LessWrong website.  – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/SR8cqwbMLmKqR7p5s},
	journaltitle = {{LessWrong}},
	author = {{Vaniver}},
	date = {2017-09-20},
	file = {~/Google Drive/library-pdf/Vaniver2017LWOpenBeta.pdf}
}

@online{Vaughan2016ThreeHeuristicsFinding,
	database = {Tlön},
	title = {Three heuristics for finding cause X},
	abstract = {In the [October 2016 {EA} Newsletter](https://www.effectivealtruism.org/october-2016-ea-newsletter/), we discussed [Will {MacAskill}’s idea](https://www.effectivealtruism.org/moral-progress-and-cause-x/).},
	langid = {english},
	url = {https://www.effectivealtruism.org/articles/three-heuristics-for-finding-cause-x/},
	journaltitle = {Effective altruism},
	author = {Vaughan, Kerry},
	date = {2016-11-04},
	file = {~/Google Drive/library-pdf/Vaughan2016ThreeHeuristicsFinding.pdf}
}

@online{Vaughan2017UpdateEffectiveAltruism,
	database = {Tlön},
	title = {Update on Effective Altruism Funds},
	abstract = {This post is an update on the progress of Effective Altruism Funds. If you’re not familiar with {EA} Funds please check out our launch post and our original concept post.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/MsaS8JKrR8nnxyPkK/update-on-effective-altruism-funds},
	journaltitle = {Effective Altruism Forum},
	author = {Vaughan, Kerry},
	date = {2017-04-20},
	file = {~/Google Drive/library-pdf/Vaughan2017UpdateEffectiveAltruism.pdf}
}

@online{Villalobos2022PotatoesCriticalReview,
	database = {Tlön},
	title = {Potatoes: A critical review},
	abstract = {Nunn and Qian study the effect of the introduction of potatoes in the Old World on population growth between 1700 and 1900. We think the paper credibly establishes that between one-sixth and one-quarter of the growth is a consequence of the introduction of potatoes. The main reason for doubt is the possibility of spurious correlation due to spatiotemporal autocorrelation and the fact that potatoes were mainly grown in Europe, which at the time was experiencing growth due to unrelated factors. After performing several tests to account for these concerns, we conclude they are not strong enough to reject the conclusion of the paper.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/iZrrWGvx2s2uPtica/potatoes-a-critical-review},
	shorttitle = {Potatoes},
	journaltitle = {Effective Altruism Forum},
	author = {Villalobos, Pablo and Sevilla, Jaime},
	urldate = {2022-05-12},
	date = {2022-05-10},
	file = {~/Google Drive/library-html/potatoes-a-critical-review.html;~/Google Drive/library-pdf/Villalobos2022PotatoesCriticalReview.pdf}
}

@online{Vinding2017ContraAIFOOM,
	eventdate = {2023-06},
	database = {Tlön},
	abstract = {It seems to me that there is a great asymmetry in the attention devoted to arguments in favor of the plausibility of artificial intelligence FOOM/hard takeoff scenarios compared to the attention paid to counterarguments. This is not so strange given that there are widely publicized full-length books emphasizing the arguments in favor, such as Nick Bostrom’s Superintelligence and James Barrat’s Our Final Invention, while there seems to be no such book emphasizing the opposite. And people who are skeptical of hard takeoff scenarios, and who think other things are more important to focus on, will of course tend to write books on those other, in their view more important things. Consequently, they devote only an essay or a few blogposts to present their arguments — not full-length books. The purpose of this reading list is to try to correct this asymmetry a bit by pointing people toward some of these blogposts and essays, as well as other resources that present reasons to be skeptical of a hard takeoff scenario.},
	langid = {english},
	title = {A contra {AI} {FOOM} reading list},
	url = {https://magnusvinding.com/2017/12/16/a-contra-ai-foom-reading-list/},
	journaltitle = {Magnus Vinding's Blog},
	author = {Vinding, Magnus},
	date = {2017-12},
	file = {~/Google Drive/library-pdf/Vinding2017ContraAIFOOM.pdf}
}

@online{Vinding2020UnderappreciatedConsequentialistReasons,
	database = {Tlön},
	title = {Underappreciated consequentialist reasons to avoid
                  consuming animal products},
	abstract = {While there may be strong deontological or virtue-ethical reasons to avoid consuming
animal products (“as far as is possible and practicable”), the consequentialist case for such
avoidance is quite weak.},
	url = {https://magnusvinding.com/2020/10/03/underappreciated-consequentialist-reasons/},
	journaltitle = {Magnus Vinding's Blog},
	author = {Vinding, Magnus},
	urldate = {2022-07-30},
	date = {2020-10-03},
	langid = {english},
	file = {~/Google Drive/library-html/underappreciated-consequentialist-reasons.html;~/Google Drive/library-pdf/Vinding2020UnderappreciatedConsequentialistReasons.pdf}
}

@online{Vinding2022ReplyGustafssonNegative,
	database = {Tlön},
	title = {Reply to Gustafsson’s “Against negative
                  utilitarianism”},
	abstract = {In this post, {CRS} researcher Magnus Vinding responds to the novel counterexamples in Johan Gustafsson’s “Against Negative Utilitarianism” draft paper.},
	url = {https://centerforreducingsuffering.org/reply-to-gustafsson/},
	journaltitle = {Center for Reducing Suffering},
	author = {Vinding, Magnus},
	urldate = {2022-06-07},
	date = {2022-06-07},
	langid = {english},
	file = {~/Google Drive/library-html/reply-to-gustafsson.html;~/Google Drive/library-pdf/Vinding2022ReplyGustafssonNegative.pdf}
}

@Article{Vinge1993ComingTechnologicalSingularity,
	database = {Tlön},
	author = {Vinge, Vernor},
	langid = {english},
	note = {Winter Issue},
	pages = {88–95},
	volume = {81},
	date = {1993},
	journaltitle = {Whole Earth Review},
	title = {The coming technological Singularity: How to survive in the post-human era},
	timestamp = {2023-06-01 10:09:16 (GMT)}
}

@online{Vivalt2020AnnouncingLaunchSocial,
	database = {Tlön},
	title = {Announcing the launch of the Social Science Prediction
                  Platform!},
	abstract = {I have been highly supportive of collecting ex ante forecasts of research results for some time now. Today, I am happy to say that the Social Science Prediction Platform is finally ready for public consumption.},
	langid = {english},
	url = {https://evavivalt.com/2020/07/announcing-the-launch-of-the-social-science-prediction-platform},
	journaltitle = {Eva Vivalt's blog},
	author = {Vivalt, Eva},
	date = {2020-07-07},
	file = {~/Google Drive/library-pdf/Vivalt2020AnnouncingLaunchSocial.pdf}
}

@online{Vollmer2020EAFFRIAre,
	database = {Tlön},
	title = {{EAF}/{FRI} are now the center on long-term risk
                  ({CLR})},
	abstract = {We have renamed our research project, the Foundational Research Institute ({FRI}), to the Center on Long-Term Risk ({CLR}). The {CLR} will become the project to carry out most of our activities. It will operate under the domain longtermrisk.org with the following logo: Motivation We are renaming for the following reasons: Change of strategy. We now focus on building a research community working on reducing risks of astronomical suffering (s-risks). Our change of strategy entailed several changes to our organization. In addition to rebranding, we made the following changes over the past year: We moved to London (Primrose Hill) to better attract and retain staff and collaborate with other researchers in London and Oxford. We hired a new Research Director: Jesse […].},
	langid = {english},
	url = {https://ea-foundation.org/blog/eaf-fri-are-now-clr/},
	journaltitle = {Effective Altruism Foundation},
	author = {Vollmer, Jonas},
	date = {2020-03-06},
	file = {~/Google Drive/library-pdf/Vollmer2020EAFFRIAre.pdf}
}

@online{Vollmer2021QuickNotesEffective,
	database = {Tlön},
	title = {Some quick notes on "Effective Altruism"},
	abstract = {duction.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/h566GT4ECfJAB38af/some-quick-notes-on-effective-altruism},
	journaltitle = {Effective Altruism Forum},
	author = {Vollmer, Jonas},
	date = {2021-03-24},
	file = {~/Google Drive/library-pdf/Vollmer2021QuickNotesEffective.pdf}
}

@online{Vox2021Masthead,
	database = {Tlön},
	title = {Masthead},
	abstract = {– AI-generated abstract.},
	url = {https://www.vox.com/pages/masthead},
	journaltitle = {Vox},
	author = {Vox},
	urldate = {2021-09-07},
	date = {2021},
	langid = {english},
	file = {~/Google Drive/library-html/masthead.html}
}

@online{Wang2021RetroactivePublicGoods,
	database = {Tlön},
	title = {Retroactive public goods funding},
	abstract = {What would happen if suddenly, exits did exist for public goods projects? An exit determined by how much public good has been created by the project rather than quarterly profit. Would we see more vigorous investment and innovation on technology that maximizes community benefit? Would we see more nonprofits thriving rather than surviving? We propose a mechanism to achieve these ends below.},
	url = {https://medium.com/ethereum-optimism/retroactive-public-goods-funding-33c9b7d00f0c},
	journaltitle = {Ethereum optimism blog},
	author = {Wang, Jinglan and Floersch, Karl and Meister, Will and
                  Buterin, Vitalik},
	urldate = {2021-09-12},
	date = {2021-07-20},
	langid = {english},
	file = {~/Google Drive/library-pdf/Jinglan2021RetroactivePublicGoods.pdf;~/Google Drive/library-html/retroactive-public-goods-funding-33c9b7d00f0c.html}
}

@online{Webb2021FormalisingWashingOut,
	database = {Tlön},
	title = {Formalising the "washing out hypothesis"},
	abstract = {Longtermists face a tradeoff: the stakes of our actions may be higher when looking further into the future, but predictability also declines when trying to affect the longer-run future. If predictability declines quickly enough, then long term effects might “wash out”, and the near term consequences of our actions might be the most important for determining what we ought to do. Here, I provide a formal framework for thinking about this tradeoff. I use a model in which a Bayesian altruist receives signals about the future value of a neartermist and a longtermist intervention. The noise of these signals increases as the altruist tries to predict further into the future. Choosing longtermist interventions is relatively less appealing when the noise of signals increases more quickly. And even if a longtermist intervention appears to have an effect that lasts infinitely long into the future, predictability may decline sufficiently quickly that the ex ante value of the longtermist intervention is finite (and therefore may be less than the neartermist intervention).
Longtermism is roughly the claim that what we ought to do is mostly determined by how our actions affect the very long-run future. The intuition that underlies this claim is that the future might be very long and very big, meaning that the vast majority of value is likely to be realised over the long-run future. On the other hand, the predictability of the effects of our actions is likely to decrease as we extend our time-horizon to the very long-run future. For example, it may be impossible to have a predictable and significant effect on the state of the world more than 1,000 years from now.
Altruists thus face a trade-off: if we attempt to improve the future over the long run rather than in the near term, there may be higher stakes, but less predictability. If the predictability of the effects of our actions declines quickly enough to counteract the increased stakes, then near term effects will dominate our ex ante moral decision-making, contrary to the claims of the longtermist. This objection to longtermism has been called the “washing out hypothesis” and the “epistemic challenge to longtermism”, and you can find further discussions of it in the links. It is seen as one of the most important and plausible objections to the claims of the longtermist.
In this post, I aim to provide a simple mathematical framework for thinking about the washing out hypothesis that formalises the tradeoff between stakes and predictability.},
	url = {https://forum.effectivealtruism.org/posts/z2DkdXgPitqf98AvY/formalising-the-washing-out-hypothesis},
	journaltitle = {Effective Altruism Forum},
	author = {Webb, Duncan},
	urldate = {2022-05-30},
	date = {2021-03-25},
	langid = {english},
	file = {~/Google Drive/library-html/formalising-the-washing-out-hypothesis.html;~/Google Drive/library-pdf/Webb2021FormalisingWashingOut.pdf}
}

@online{Wen2023WhenCanEat,
	database = {Tlön},
	date = {2020-04-30},
	abstract = {There is a lot of uncertainty around when we will be able to eat meat grown from cells, and how we should divide our efforts between that, plant-based alternatives, and other forms of animal advocacy. This post seeks to give sensible, unbiased views on the future of alternative proteins.},
	journaltitle = {Effective Altruism Forum},
	author = {Yip, Wen},
	title = {When Can I Eat Meat Again?},
	url = {https://forum.effectivealtruism.org/posts/4uYebcr5G2jqxuXG3/when-can-i-eat-meat-again},
	langid = {english},
	timestamp = {2023-07-20 19:04:17 (GMT)},
	urldate = {2023-07-20}
}

@online{West2017ArgumentWhyFuture,
	database = {Tlön},
	title = {An argument for why the future may be good},
	abstract = {If the default course of humanity is to be ethical, our prior should be that the future will be good, and the burden of proof shifts to those who believe that the future will be bad.
I do not believe it provides a knockdown counterargument to concerns about s-risks, but I hope this argument’s publication encourages more discussion of the topic, and a viewpoint some readers have not before considered.
 .
This post represents a combination of my and the anonymous {EA}’s views. Any errors are mine. I would like to thank Gina Stuessy and this {EA} for proofreading a draft of this post, and for talking about this and many other important ideas about the far future with me.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/kNKpyf4WWdKehgvRt/an-argument-for-why-the-future-may-be-good},
	journaltitle = {Effective Altruism Forum},
	author = {West, Ben},
	date = {2017-07-19},
	file = {~/Google Drive/library-pdf/West2017ArgumentWhyFuture.pdf}
}

@online{Whitaker2022WeReAnnouncing,
	database = {Tlön},
	title = {We're announcing a \$100,000 blog prize - {EA} Forum},
	abstract = {We want to encourage a broader, public conversation around effective altruism and longtermism. To that end, we’re offering up to 5 awards of \$100,000 each for the best new and recent blogs. We’re also making grants to promising young writers in the community.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/xapRLBTpMYokrpd9q/we-re-announcing-a-usd100-000-blog-prize},
	journaltitle = {Effective Altruism Forum},
	author = {Whitaker, Nick},
	urldate = {2022-03-10},
	date = {2022-03-07},
	file = {~/Google Drive/library-html/We're announcing a \$100,000 blog prize - EA
                  Forum:we-re-announcing-a-usd100-000-blog-prize.html;~/Google
                  Drive/library-pdf/Whitaker2022WeReAnnouncing.pdf}
}

@Report{White2014EnsayosControladosAleatorios,
	database = {Tlön},
	date = {2014},
	number = {Sinopsis de la evaluación de impacto n. 7},
	langid = {spanish},
	institution = {Centro de Investigaciones Innocenti de UNICEF},
	title = {Ensayos controlados aleatorios},
	author = {White, Howard and Sabarwal, Shagun and de Hoop,
                  Thomas},
	url = {https://www.unicef-irc.org/publications/pdf/MB7ES.pdf},
	timestamp = {2023-07-20 14:06:05 (GMT)}
}

@online{Whittlestone2017BuildingEffectiveAltruism,
	database = {Tlön},
	title = {Building an effective altruism community},
	abstract = {Investing in the effective altruism (EA) community, which aims to solve major world problems, could be highly impactful. The EA community has achieved impressive results, directing millions of dollars and talented individuals toward pressing issues. Their work seems neglected compared to directly working on specific causes. Investing in the community could lead to multiplied returns, robustness to uncertainty, and capacity-building opportunities. However, concerns exist, including potential self-serving motives, lack of direct impact evidence, and the risk of prioritizing community building over real problem-solving. The decision between direct cause work and EA community investment should depend on various factors, including impact uncertainty, personal fit, and whether direct work may also contribute to community building. – AI-generated abstract.},
	langid = {english},
	url = {https://www.effectivealtruism.org/articles/cause-profile-building-an-effective-altruism-community/},
	journaltitle = {Effective Altruism},
	author = {Whittlestone, Jess},
	date = {2017-11-16},
	file = {~/Google Drive/library-pdf/Whittlestone2017BuildingEffectiveAltruism.pdf}
}

@online{Whittlestone2023FuturoLargoPlazo,
	database = {Tlön},
	date = {2023},
	title = {El futuro a largo plazo},
	author = {Whittlestone, Jess},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Whittlestone2017LongtermFuture}
}

@online{Wiblin2015WhatBroadIntervention,
	database = {Tlön},
	title = {What is a 'broad intervention' and what is a 'narrow
                  intervention'? Are we confusing ourselves?},
	abstract = {Across the community it is common to hear distinctions drawn between ‘broad’ and ‘narrow’ interventions - though less so lately than in 2014/2013. For some imperfect context see this blog post by Holden on 'flow-through effects'. Typical classifications people might make would be:.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/yisrgRsi4v3uyhujw/what-is-a-broad-intervention-and-what-is-a-narrow},
	journaltitle = {Effective Altruism Forum},
	author = {Wiblin, Robert},
	date = {2015-12-19},
	file = {~/Google Drive/library-pdf/Wiblin2015WhatBroadIntervention.pdf}
}

@online{Wiblin2017DrBethCameron,
	database = {Tlön},
	title = {Dr Beth Cameron fought ebola for the white house. Now
                  she works to stop something even worse},
	abstract = {“When you're in the middle of a crisis and you have to ask for money, you're already too late.” Thats Dr. Beth Cameron, and she should know. She has years of experience preparing for and fighting the diseases of our nightmares, on the *White House Ebola Taskforce*...},
	langid = {english},
	url = {https://80000hours.org/podcast/episodes/beth-cameron-pandemic-preparedness/},
	journaltitle = {80,000 Hours},
	author = {Wiblin, Robert},
	date = {2017-10-25},
	file = {~/Google Drive/library-pdf/Wiblin2017DrBethCameron.pdf}
}

@online{Wiblin2017GoingUndercoverExpose,
	database = {Tlön},
	title = {Going undercover to expose animal cruelty, get rabbit
                  cages banned and reduce meat consumption},
	abstract = {If you knew that ducks were being killed with pitchforks, would you be willing to go undercover to expose the crime? That’s a real question that confronts volunteers at Animal Equality...},
	langid = {english},
	url = {https://80000hours.org/podcast/episodes/animal-equality-exposing-cruelty/},
	journaltitle = {80,000 Hours},
	author = {Wiblin, Robert},
	date = {2017-11-13},
	file = {~/Google Drive/library-pdf/Wiblin2017GoingUndercoverExpose.pdf}
}

@online{Wiblin2017JuliaGalefMaking,
	database = {Tlön},
	title = {Julia Galef on making humanity more rational, what
                  {EA} does wrong, and why Twitter isn't all bad},
	abstract = {The scientific revolution in the 16th century was one of the biggest societal shifts in human history, driven by the discovery of new and better methods of figuring out who was right and who was wrong. Julia Galef - a well-known writer and researcher focused on improving human judgment, especially about high stakes questions - believes that if we could again develop new techniques to predict the future, resolve disagreements and make sound decisions together, it could dramatically improve the world across the board. We brought her in to talk about her ideas....},
	langid = {english},
	url = {https://80000hours.org/podcast/episodes/is-it-time-for-a-new-scientific-revolution-julia-galef-on-how-to-make-humans-smarter/},
	journaltitle = {80,000 Hours},
	author = {Wiblin, Robert},
	date = {2017-09-13},
	file = {~/Google Drive/library-pdf/Wiblin2017JuliaGalefMaking.pdf}
}

@online{Wiblin2017ProfDavidSpiegelhalter,
	database = {Tlön},
	title = {Prof david spiegelhalter on risk, statistics and
                  improving the public understanding of science},
	abstract = {"...What do we hear in the news? We hear about Ebola, we hear about terrorism, we hear about the latest threat that might be in what we eat and the way we travel, and we get very concerned about this, whether it's a plane crash or whatever. Because that's what's in the news, that's what is available to us. That's what's so prominent, but of course, so many of these risks are actually very small indeed...".},
	langid = {english},
	url = {https://80000hours.org/podcast/episodes/prof-david-spiegelhalter-on-risk-statistics-and-improving-the-public-understanding-of-science/},
	journaltitle = {80,000 Hours},
	author = {Wiblin, Robert},
	date = {2017-06-21},
	file = {~/Google Drive/library-pdf/Wiblin2017ProfDavidSpiegelhalter.pdf}
}

@online{Wiblin2017TobyOrdWhy,
	database = {Tlön},
	title = {Toby Ord on why the long-term future of humanity
                  matters more than anything else, and what we should do
                  about it},
	abstract = {Of all the people whose well-being we should care about, only a small fraction are alive today. That’s the view of Oxford philosopher Dr Toby Ord....},
	langid = {english},
	url = {https://80000hours.org/podcast/episodes/why-the-long-run-future-matters-more-than-anything-else-and-what-we-should-do-about-it/},
	journaltitle = {80,000 Hours},
	author = {Wiblin, Robert},
	date = {2017-09-06},
	file = {~/Google Drive/library-pdf/Wiblin2017WhyLongtermFuture.pdf}
}

@online{Wiblin2018EconomicsProfTyler,
	database = {Tlön},
	title = {Economics prof Tyler Cowen says our overwhelming
                  priorities should be maximising economic growth and
                  making civilisation more stable. Is he right?},
	langid = {english},
	url = {https://80000hours.org/podcast/episodes/tyler-cowen-stubborn-attachments/},
	journaltitle = {80,000 Hours},
	author = {Wiblin, Robert and Harris, Keiran},
	date = {2018-10-17},
	file = {~/Google Drive/library-pdf/Wiblin2018EconomicsProfTyler.pdf}
}

@online{Wiblin2018ItMyJob,
	database = {Tlön},
	title = {“It’s my job to worry about any way nukes could get
                  used”},
	abstract = {"You have this intense time pressure on a {US} leader to respond with a nuclear strike... If you think about some sort of group or another country spoofing an attack that would cause a response - that's a real threat.".},
	langid = {english},
	url = {https://80000hours.org/podcast/episodes/samantha-pk-nuclear-security/},
	journaltitle = {80,000 Hours},
	author = {Wiblin, Robert and Harris, Keiran},
	date = {2018-02-14}
}

@online{Wiblin2018OfirReichUsing,
	database = {Tlön},
	title = {Ofir Reich on using data science to end poverty and
                  the spurious Action/Inaction distinction},
	abstract = {Ofir Reich started out doing math in the military and tech startups - but then made a big career pivot to become a data scientist focussed on helping the global poor. I asked him about how...},
	langid = {english},
	url = {https://80000hours.org/podcast/episodes/ofir-reich-data-science/},
	journaltitle = {80,000 Hours},
	author = {Wiblin, Robert and Harris, Keiran},
	date = {2018-01-31},
	file = {~/Google Drive/library-pdf/Wiblin2018OfirReichUsing.pdf}
}

@online{Wiblin2018YearWorthEducation,
	database = {Tlön},
	title = {A year's worth of education for under a dollar and
                  other ‘Best buys’ in development, from the {UK} aid
                  agency's chief economist},
	abstract = {"In developing countries, more teachers or books seem to have no impact.".},
	langid = {english},
	url = {https://80000hours.org/podcast/episodes/rachel-glennerster-best-buys-in-international-development/},
	journaltitle = {80,000 Hours},
	author = {Wiblin, Robert and Harris, Keiran},
	date = {2018-12-20}
}

@online{Wiblin2019AmbassadorBonnieJenkins,
	database = {Tlön},
	title = {Ambassador bonnie jenkins on 8 years of combating
                  {WMD} terrorism},
	abstract = {"…it started when the Soviet Union fell apart and there was a real desire to ensure security of nuclear materials and pathogens, and that scientists with [{WMD}-related] knowledge could get paid so that they wouldn't go to countries and sell that knowledge.".},
	langid = {english},
	url = {https://80000hours.org/podcast/episodes/ambassador-bonnie-jenkins-peace-arms-control/},
	journaltitle = {80,000 Hours},
	author = {Wiblin, Robert and Harris, Keiran},
	date = {2019-11-19},
	file = {~/Google Drive/library-pdf/Wiblin2019AmbassadorBonnieJenkins.pdf}
}

@online{Wiblin2019PeterSingerBeing,
	database = {Tlön},
	title = {Peter Singer on being provocative, {EA}, how his moral
                  views have changed, \& rescuing children drowning in
                  ponds},
	abstract = {In 1989, the professor of moral philosophy Peter Singer was all over the news for an inflammatory opinion….},
	langid = {english},
	url = {https://80000hours.org/podcast/episodes/peter-singer-advocacy-and-the-life-you-can-save/},
	journaltitle = {80,000 Hours},
	author = {Wiblin, Robert and Koehler, Arden and Harris, Keiran},
	date = {2019-12-05}
}

@online{Wiblin2019RobWiblinPlastic,
	database = {Tlön},
	title = {Rob wiblin on plastic straws, nicotine, doping, \&
                  whether changing the long term is really possible},
	langid = {english},
	url = {https://80000hours.org/podcast/episodes/rob-wiblin-on-many-things/},
	journaltitle = {80,000 Hours},
	author = {Wiblin, Robert and Harris, Keiran},
	date = {2019-09-25}
}

@online{Wiblin2019TeamTryingEnd,
	database = {Tlön},
	title = {The team trying to end poverty by founding
                  well-governed ‘Charter’ cities},
	abstract = {"China looked \& said, 'Wait, Hong Kong's rich. Taiwan's rich. We're all Chinese. Why are they doing well and we're starving?'".},
	langid = {english},
	url = {https://80000hours.org/podcast/episodes/lutter-and-winter-chater-cities-innovative-governance/},
	journaltitle = {80,000 Hours},
	author = {Wiblin, Robert and Harris, Keiran},
	date = {2019-03-31}
}

@online{Wiblin2020BonusEpisodeArden,
	database = {Tlön},
	title = {Bonus episode: Arden \& Rob on demandingness,
                  work-life balance and injustice},
	langid = {english},
	url = {https://80000hours.org/podcast/episodes/arden-and-rob-on-demandingness/},
	journaltitle = {80,000 Hours},
	author = {Wiblin, Robert and Koehler, Arden and Harris, Keiran},
	date = {2020-02-25}
}

@online{Wiblin2020HowBecomingPatient,
	database = {Tlön},
	title = {How becoming a 'patient philanthropist' could allow
                  you to do far more good},
	langid = {english},
	url = {https://80000hours.org/podcast/episodes/phil-trammell-patient-philanthropy/},
	journaltitle = {80,000 Hours},
	author = {Wiblin, Robert and Lempel, Howie and Harris, Keiran},
	date = {2020-03-17},
	file = {~/Google Drive/library-pdf/Wiblin2020HowBecomingPatient.pdf}
}

@online{Wiblin2020MaxTegmarkHow,
	database = {Tlön},
	title = {Max Tegmark on how a 'put-up-or-shut-up' resolution
                  led him to work on {AI} and algorithmic news
                  selection},
	url = {https://80000hours.org/podcast/episodes/max-tegmark-ai-and-algorithmic-news-selection/},
	journaltitle = {80,000 Hours},
	author = {Wiblin, Robert and Harris, Keiran},
	urldate = {2022-07-03},
	date = {2020-07-01},
	langid = {english},
	file = {~/Google Drive/library-pdf/Wiblin2020MaxTegmarkHow.pdf}
}

@online{Wiblin2020RussRobertsWhether,
	database = {Tlön},
	title = {Russ roberts on whether it’s more effective to help
                  strangers, or people you know},
	langid = {english},
	url = {https://80000hours.org/podcast/episodes/russ-roberts-effective-altruism-empirical-research-utilitarianism/},
	journaltitle = {80,000 Hours},
	author = {Wiblin, Robert and Harris, Keiran},
	date = {2020-11-03}
}

@online{Wiblin2020TobyOrdPrecipice,
	database = {Tlön},
	title = {Toby Ord on the precipice and humanity's potential
                  futures},
	abstract = {While reading I copied out 87 surprising facts in the book. Here's a sample of 16….},
	langid = {english},
	url = {https://80000hours.org/podcast/episodes/toby-ord-the-precipice-existential-risk-future-humanity/},
	journaltitle = {80,000 Hours},
	author = {Wiblin, Robert and Koehler, Arden and Harris, Keiran},
	date = {2020-03-07},
	file = {~/Google Drive/library-pdf/Wiblin2020TobyOrdPrecipice.pdf}
}

@online{Wiblin2020WillMacAskillMoral,
	database = {Tlön},
	title = {Will {MacAskill} on the moral case against ever
                  leaving the house, whether now is the hinge of
                  history, and the culture of effective altruism},
	abstract = {...and the debate over whether we’re living during the ‘hinge of history’.},
	langid = {english},
	url = {https://80000hours.org/podcast/episodes/will-macaskill-paralysis-and-hinge-of-history/},
	journaltitle = {80,000 Hours},
	author = {Wiblin, Robert and Harris, Keiran},
	date = {2020-01-24},
	file = {~/Google Drive/library-pdf/Wiblin2020WillMacAskillMoral.pdf}
}

@online{Wiblin2021BrianChristianAlignment,
	database = {Tlön},
	title = {Brian christian on the alignment problem},
	abstract = {"People would say 'call me when {AI} can do X.' And now it can do X.".},
	langid = {english},
	url = {https://80000hours.org/podcast/episodes/brian-christian-the-alignment-problem/},
	journaltitle = {80,000 Hours},
	author = {Wiblin, Robert and Harris, Keiran},
	date = {2021-03-05},
	file = {~/Google Drive/library-pdf/Wiblin2021BrianChristianAlignment.pdf}
}

@online{Wiblin2021ChristianTarsneyFuture,
	database = {Tlön},
	title = {Christian tarsney on future bias and a possible
                  solution to moral fanaticism},
	langid = {english},
	url = {https://80000hours.org/podcast/episodes/christian-tarsney-future-bias-fanaticism/},
	journaltitle = {80,000 Hours},
	author = {Wiblin, Robert and Harris, Keiran},
	date = {2021-05-05},
	file = {~/Google Drive/library-pdf/Wiblin2021ChristianTarsneyFuture.pdf}
}

@online{Wiblin2021HoldenKarnofskyBuilding,
	database = {Tlön},
	title = {Holden Karnofsky on building aptitudes and kicking
                  ass},
	url = {https://80000hours.org/podcast/episodes/holden-karnofsky-building-aptitudes-kicking-ass/},
	journaltitle = {80,000 Hours},
	author = {Wiblin, Robert and Harris, Keiran},
	urldate = {2021-10-18},
	date = {2021-08-26},
	langid = {english},
	file = {~/Google Drive/library-html/holden-karnofsky-building-aptitudes-kicking-ass.html;~/Google Drive/library-pdf/Wiblin2021HoldenKarnofskyBuilding.pdf}
}

@online{Wiblin2021LuisaRodriguezWhy,
	database = {Tlön},
	title = {Luisa Rodriguez on why global catastrophes seem
                  unlikely to kill us all},
	url = {https://80000hours.org/podcast/episodes/luisa-rodriguez-why-global-catastrophes-seem-unlikely-to-kill-us-all/},
	journaltitle = {80,000 Hours},
	author = {Wiblin, Robert and Harris, Keiran},
	urldate = {2022-07-08},
	date = {2021-11-19},
	langid = {english},
	file = {~/Google Drive/library-html/luisa-rodriguez-why-global-catastrophes-seem-unlikely-to-kill-us-all.html;~/Google Drive/library-pdf/Wiblin2021LuisaRodriguezWhy.pdf}
}

@online{Wiblin2021MikeBerkowitzKeepinga,
	database = {Tlön},
	title = {Mike berkowitz on keeping the {US} a liberal
                  democratic country},
	langid = {english},
	url = {https://80000hours.org/podcast/episodes/mike-berkowitz-preserving-us-democracy/},
	journaltitle = {80,000 Hours},
	author = {Wiblin, Robert and Harris, Keiran},
	date = {2021-04-20},
	file = {~/Google Drive/library-pdf/Wiblin2021MikeBerkowitzKeeping.pdf}
}

@online{Wiblin2022AndresJimenezZorrilla,
	database = {Tlön},
	title = {Andrés Jiménez Zorrilla on the Shrimp Welfare Project},
	abstract = {Andrés Jiménez Zorrilla on the Shrimp Welfare Project - 80,000 Hours

This episode of the After Hours podcast features Andrés Jiménez Zorrilla, founder and CEO of the Shrimp Welfare Project. They discuss the welfare of shrimp in industrial aquaculture, and how to improve it. They cover the evidence for shrimp sentience, the conditions shrimp are raised in, what is killing them, and what can be done to improve their situation. – AI-generated abstract.},
	url = {https://80000hours.org/after-hours-podcast/episodes/andres-jimenez-zorrilla-shrimp-welfare-project/},
	journaltitle = {80k After Hours Podcast},
	author = {Wiblin, Robert and Harris, Keiran},
	urldate = {2022-09-06},
	date = {2022-09-05},
	langid = {english},
	file = {~/Google Drive/library-pdf/Wiblin2022AndresJimenezZorrilla.pdf;~/Google Drive/library-html/Wiblin2022AndresJimenezZorrilla.html}
}

@online{Wiblin2022ChrisBlattmanFive,
	database = {Tlön},
	title = {Chris Blattman on the five reasons wars happen},
	abstract = {Chris Blattman argues that social scientists have generated five cogent models of when war can be ‘rational’ for both sides of a conflict:

Unchecked interests — such as national leaders who bear few of the costs of launching a war.
Intangible incentives — such as an intrinsic desire for revenge.
Uncertainty — such as both sides underestimating each other’s resolve to fight.
Commitment problems — such as the inability to credibly promise not to use your growing military might to attack others in future.
Misperceptions — such as our inability to see the world through other people’s eyes.},
	url = {https://80000hours.org/podcast/episodes/chris-blattman-five-reasons-wars-happen/},
	journaltitle = {80,000 Hours},
	author = {Wiblin, Robert and Harris, Keiran},
	urldate = {2022-04-29},
	date = {2022-04-28},
	langid = {english},
	file = {~/Google Drive/library-html/chris-blattman-five-reasons-wars-happen.html;~/Google Drive/library-pdf/Wiblin2022ChrisBlattmanFive.pdf}
}

@online{Wiblin2022NovaDasSarmaWhy,
	database = {Tlön},
	title = {Nova {DasSarma} on why information security may be
                  critical to the safe development of {AI} systems},
	abstract = {This article presents arguments in favor of reducing meat consumption in order to prevent animal suffering in factory farms. The author initially presents the case for animals' capacity to suffer based on scientific evidence and historical perspectives provided by influential thinkers like Rene Descartes and Jane Goodall. The article then details the inhumane conditions of factory farming and its detrimental impact on billions of animals each year. The author then discusses the effectiveness of vegetarianism in reducing animal suffering, considering both individual and collective actions and addressing common concerns about the difficulty of adapting to a meatless diet. The article provides a balanced perspective on adopting a vegetarian lifestyle as a means of reducing animal suffering – AI-generated abstract.},
	url = {https://80000hours.org/podcast/episodes/nova-dassarma-information-security-and-ai-systems/},
	journaltitle = {80,000 Hours},
	author = {Wiblin, Robert and Harris, Keiran},
	urldate = {2022-06-15},
	date = {2022-06-14},
	langid = {english},
	file = {~/Google Drive/library-html/nova-dassarma-information-security-and-AI-systems.html;~/Google Drive/library-pdf/Wiblin2022NovaDasSarmaWhy.pdf}
}

@online{Wiblin2022WillMacAskillBalancing,
	database = {Tlön},
	title = {Will {MacAskill} on balancing frugality with ambition,
                  whether you need longtermism, and mental health under
                  pressure},
	url = {https://80000hours.org/podcast/episodes/will-macaskill-ambition-longtermism-mental-health/},
	journaltitle = {80,000 Hours},
	author = {Wiblin, Robert and Harris, Alan W.},
	urldate = {2022-05-24},
	date = {2022-05-23},
	langid = {english},
	file = {~/Google Drive/library-pdf/Wiblin2022WillMacAskillBalancing.pdf}
}

@online{Wiblin2023MarcoParaComparar,
	database = {Tlön},
	date = {2023},
	title = {Un marco para comparar problemas globales en términos
                  de expectativa de impacto},
	author = {Wiblin, Robert},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Wiblin2016FrameworkForComparing}
}

@online{Wikipedia2001PrisonerDilemma,
	database = {Tlön},
	title = {Prisoner's dilemma},
	abstract = {The prisoner's dilemma is a game theory thought experiment that involves two rational agents, each of whom can cooperate for mutual benefit or betray their partner ("defect") for individual reward. This dilemma was originally framed by Merrill Flood and Melvin Dresher in 1950 while they worked at the {RAND} Corporation. Albert W. Tucker later formalized the game by structuring the rewards in terms of prison sentences and named it the "prisoner's dilemma".The prisoner's dilemma models many real-world situations involving strategic behavior. In casual usage, the label "prisoner's dilemma" may be applied to any situation in which two entities could gain important benefits from cooperating or suffer from failing to do so, but find it difficult or expensive to coordinate their activities.},
	langid = {english},
	url = {https://en.wikipedia.org/wiki/Prisoner%27s_dilemma},
	journaltitle = {Wikipedia},
	author = {{Wikipedia}},
	date = {2001-08-21}
}

@online{Wikipedia2010VonNeumannMorgenstern,
	database = {Tlön},
	eventdate = {2023-07-15},
	abstract = {In decision theory, the von Neumann–Morgenstern ({VNM}) utility theorem shows that, under certain axioms of rational behavior, a decision-maker faced with risky (probabilistic) outcomes of different choices will behave as if they are maximizing the expected value of some function defined over the potential outcomes at some specified point in the future. This function is known as the von Neumann–Morgenstern utility function. The theorem is the basis for expected utility theory.
In 1947, John von Neumann and Oskar Morgenstern proved that any individual whose preferences satisfied four axioms has a utility function; such an individual's preferences can be represented on an interval scale and the individual will always prefer actions that maximize expected utility. That is, they proved that an agent is ({VNM}-)rational if and only if there exists a real-valued function u defined by possible outcomes such that every preference of the agent is characterized by maximizing the expected value of u, which can then be defined as the agent's {VNM}-utility (it is unique up to adding a constant and multiplying by a positive scalar). No claim is made that the agent has a "conscious desire" to maximize u, only that u exists.
The expected utility hypothesis is that rationality can be modeled as maximizing an expected value, which given the theorem, can be summarized as "rationality is {VNM}-rationality". However, the axioms themselves have been critiqued on various grounds, resulting in the axioms being given further justification.{VNM}-utility is a decision utility in that it is used to describe decision preferences. It is related but not equivalent to so-called E-utilities (experience utilities), notions of utility intended to measure happiness such as that of Bentham's Greatest Happiness Principle.},
	langid = {english},
	url = {https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem},
	date = {2010-04-10},
	journaltitle = {Wikipedia},
	title = {Von Neumann–Morgenstern utility theorem},
	author = {Wikipedia},
	timestamp = {2023-07-20 17:04:13 (GMT)}
}

@online{Wikipedia2021StrongAI,
	database = {Tlön},
	title = {Strong {AI}},
	url = {https://en.wikipedia.org/w/index.php?title=Strong_AI&oldid=1050563218},
	journaltitle = {Wikipedia},
	author = {{Wikipedia}},
	urldate = {2022-05-02},
	date = {2021-10-18},
	langid = {english},
	note = {Page Version {ID}: 1050563218},
	file = {~/Google Drive/library-html/Strong_AI.html;~/Google Drive/library-pdf/Wikipedia2021StrongAI.pdf}
}

@online{Wildeford2012WhyDonPeople,
	database = {Tlön},
	title = {Why don't people help others more?},
	abstract = {This article discusses the challenge of motivating people to be more altruistic. It highlights psychological studies that demonstrate how a singular and highly identifiable victim can trigger empathy and willingness to donate, as opposed to general information about large-scale suffering. Factors such as futility thinking, diffusion of responsibility, and fairness norms are explored as barriers to altruism. Strategies to overcome these barriers include creating a culture of open and encouraged altruism, implementing opt-out philanthropy, and instilling a sense of responsibility to help others. – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/jisCHmxwmKoNwrRst/why-don-t-people-help-others-more},
	journaltitle = {{LessWrong}},
	author = {Wildeford, Peter},
	date = {2012-08-14},
	file = {~/Google Drive/library-pdf/Hurford2012WhyDonPeople.pdf}
}

@online{Wildeford2013WhyEatLessa,
	database = {Tlön},
	title = {Why eat less meat?},
	abstract = {Animals raised for food suffer immensely in factory farms, where they are subjected to painful mutilations, unsanitary living conditions, and eventual slaughter. Vegetarianism can significantly reduce this suffering by decreasing the demand for animal products. Despite common misconceptions, vegetarianism is easy to adopt and does not require giving up tasty foods.  – AI-generated abstract.},
	url = {https://www.alignmentforum.org/posts/LbbyQhLkcwAwWmBoj/why-eat-less-meat},
	journaltitle = {{AI} Alignment Forum},
	author = {Wildeford, Peter},
	urldate = {2022-07-29},
	date = {2013-07-23},
	langid = {english},
	file = {~/Google Drive/library-html/why-eat-less-meat.html;~/Google Drive/library-pdf/Wildeford2013WhyEatLessa.pdf}
}

@online{Wildeford2014YouHaveSet,
	database = {Tlön},
	title = {You have a set amount of "Weirdness Points". Spend
                  them wisely},
	abstract = {I've heard of the concept of "weirdness points" many times before, but after a bit of searching I can't find a definitive post describing the concept, so I've decided to make one.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/MH9suFZbxXCYsr5Z5/you-have-a-set-amount-of-weirdness-points-spend-them-wisely},
	journaltitle = {Effective Altruism Forum},
	author = {Wildeford, Peter},
	date = {2014-11-27},
	file = {~/Google Drive/library-pdf/Hurford2014YouHaveSet.pdf}
}

@online{Wildeford2018WhatCosteffectivenessDeveloping,
	database = {Tlön},
	title = {What is the cost-effectiveness of developing a
                  vaccine?},
	abstract = {We looked at academic literature for vaccine cost-effectiveness as a whole and we also performed individual case studies on seven contemporary and historical vaccines to try to estimate the total cost-effectiveness of researching and developing a vaccine from scratch. Looking back historically, we find a range of \$0.50 to \$1600 per DALY, depending on the vaccine. Using this historical information, we derive an estimate for the total cost-effectiveness of developing and rolling out a “typical” / ”average” vaccine as being \$18 - \$7000 / DALY. The smallpox vaccine, malaria vaccine, and rotavirus vaccine may all be more cost-effective investments in total than marginal investments in distributing bednets (see Appendix C), especially when pursued to the point of completely eradicating the disease. However, there are many important assumptions made by these models, and changing them could strengthen or undermine these conclusions.},
	url = {https://rethinkpriorities.org/publications/cost-effectiveness-of-developing-vaccines},
	journaltitle = {Rethink Priorities},
	author = {Wildeford, Peter and Davis, Marcus A.},
	urldate = {2021-11-18},
	date = {2018-05-07},
	langid = {english},
	file = {~/Google Drive/library-html/cost-effectiveness-of-developing-vaccines.html;~/Google Drive/library-pdf/Wildeford2018WhatCosteffectivenessDeveloping.pdf}
}

@online{Wildeford2021NotesManagingChange,
	database = {Tlön},
	title = {Notes on "Managing to Change the World"},
	abstract = {Some notes on Managing to Change the World: The Nonprofit Manager's Guide to Getting Results, a book by Alison Green and Jerry Hauser.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/dpjCwMwKEPqK3TPnC/notes-on-managing-to-change-the-world},
	journaltitle = {Effective Altruism Forum},
	author = {Wildeford, Peter},
	urldate = {2022-04-08},
	date = {2021-10-08},
	file = {~/Google Drive/library-html/notes-on-managing-to-change-the-world.html;~/Google Drive/library-pdf/Wildeford2021NotesManagingChange.pdf}
}

@online{Wildeford2023TresIdeasRadicales,
	translation = {Wildeford2023EaIsThree},
	translator = {Tlön},
	langid = {spanish},
	database = {Tlön},
	date = {2023},
	title = {El altruismo eficaz},
	author = {Wildeford, Peter},
	timestamp = {2023-06-27 14:57:12 (GMT)}
}

@online{Williams2022MetaculusSeekingExperienced,
	database = {Tlön},
	title = {Metaculus is seeking experienced leaders, researchers
                  \& operators for high-impact roles},
	abstract = {Metaculus's mission is to build epistemic infrastructure that enables the global community to model, understand, predict, and navigate the world’s most important and complex challenges.​.
We’re thrilled to share the first round of new high-impact open roles as we expand our operations over the next year.},
	url = {https://forum.effectivealtruism.org/posts/FTKhRoSoTFjtQjiNr/metaculus-is-seeking-experienced-leaders-researchers-and},
	journaltitle = {Effective Altruism Forum},
	author = {Williams, Christian},
	urldate = {2022-07-11},
	date = {2022-07-10},
	langid = {english},
	file = {~/Google Drive/library-pdf/MetaculusSeekingExperienced.pdf;~/Google Drive/library-html/metaculus-is-seeking-experienced-leaders-researchers-and.html}
}

@online{Wise2013GivingNowVs,
	database = {Tlön},
	title = {Giving now vs. later: a summary},
	abstract = {There's an ongoing debate about whether it's better to give now or later. This post summarizes the main considerations.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/7uJcBNZhinomKtH9p/giving-now-vs-later-a-summary},
	journaltitle = {Effective Altruism Forum},
	author = {Wise, Julia},
	date = {2013-07-23},
	file = {~/Google Drive/library-pdf/Wise2013GivingNowVs.pdf}
}

@online{Wise2015BurnoutSelfcare,
	database = {Tlön},
	title = {Burnout and self-care},
	abstract = {I think effective altruism often runs into questions about self-care and boundaries, and might have a few things to learn from social work. For people in helping professions (like nurses, therapists, and clergy), training programs often warn against burnout and "compassion fatigue." To prevent this, training emphasizes self-care. Self-care might include exercise, sleep, spending time with loved ones, spiritual practice, hobbies, and (at least among my coworkers) the latest episode of "Scandal." My workplace asks every prospective hire about self-care, because we want someone who has a plan for not burning out.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/ZGW8Tmc6mDWZTnqyo/burnout-and-self-care},
	journaltitle = {Effective Altruism Forum},
	author = {Wise, Julia},
	date = {2015-10-23},
	file = {~/Google Drive/library-pdf/Wise2015BurnoutSelfcare.pdf}
}

@online{Wise2023Alegremente,
	database = {Tlön},
	keywords = {altruismo obligatorio y altruismo entusiasta},
	langid = {spanish},
	date = {2023},
	author = {Wise, Julia},
	title = {Alegremente},
	translator = {Anónimo},
	translation = {Wise2013Cheerfully}
}

@online{Wise2023DonarAhoraMas,
	database = {Tlön},
	date = {2023},
	title = {Donar ahora o más tarde: resumen},
	author = {Wise, Julia},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Wise2013GivingNowVs}
}

@online{Wise2023ExcesoDeRiqueza,
	database = {Tlön},
	date = {2023},
	title = {Una exceso de riqueza},
	author = {Wise, Julia},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Wise2023EmbarrassmentOfRiches}
}

@book{Wood1848AthenaeOxoniensesVol,
	database = {Tlön},
	location = {Oxford},
	langid = {english},
	title = {Athenæ Oxonienses, Vol. I: Life of Wood},
	volume = {1},
	publisher = {Ecclesiastical History Society},
	author = {Wood, Anthony},
	date = {1848},
	file = {~/Google Drive/library-pdf/Wood1848AthenaeOxoniensesVol.pdf}
}

@online{Woods2021IntroducingNonlinearFund,
	database = {Tlön},
	title = {Introducing The Nonlinear Fund: {AI} Safety research,
                  incubation, and funding},
	abstract = {We research high leverage {AI} Safety interventions. Our team of analysts generate, identify, and evaluate potentially high impact opportunities.  When we find them, we make them happen. Once a top idea has been vetted, we use a variety of tools to turn it into a reality, including grantmaking, advocacy, {RFPs}, and incubating it ourselves.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/fX8JsabQyRSd7zWiD/introducing-the-nonlinear-fund-ai-safety-research-incubation},
	shorttitle = {Introducing The Nonlinear Fund},
	journaltitle = {Effective Altruism Forum},
	author = {Woods, Kat},
	urldate = {2022-05-09},
	date = {2021-03-18},
	file = {~/Google Drive/library-html/introducing-the-nonlinear-fund-ai-safety-research-incubation.html;~/Google Drive/library-pdf/Woods2021IntroducingNonlinearFund.pdf}
}

@online{Woodside2022IntroducingMLSafety,
	database = {Tlön},
	title = {Introducing the {ML} Safety Scholars Program},
	abstract = {The Machine Learning Safety Scholars program is a paid, 9-week summer program designed to help undergraduate students gain skills in machine learning with the aim of using those skills for empirical {AI} safety research in the future.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/9RYvJu2iNJMXgWCBn/introducing-the-ml-safety-scholars-program},
	journaltitle = {Effective Altruism Forum},
	author = {Woodside, Thomas and Hendrycks, Dan and Mazeika,
                  Mantas and {Oliver Zhang} and Hough, Sidney and Liu,
                  Kevin},
	urldate = {2022-05-04},
	date = {2022-05-04},
	file = {~/Google Drive/library-html/introducing-the-ml-safety-scholars-program.html}
}

@report{WorldEconomicForum2014GlobalRisks2014,
	abstract = {The Global Risks 2014 report highlights how global risks are not only interconnected but also have systemic impacts. To manage global risks effectively and build resilience to their impacts, better efforts are needed to understand, measure and foresee the evolution of interdependencies between risks, supplementing traditional risk-management tools with new concepts designed for uncertain environments. If global risks are not effectively addressed, their social, economic and political fallouts could be far-reaching, as exemplified by the continuing impacts of the financial crisis of 2007-2008. The systemic nature of our most significant risks calls for procedures and institutions that are globally coordinated yet locally flexible. As international systems of finance, supply chains, health, energy, the Internet and the environment become more complex and interdependent, their level of resilience determines whether they become bulwarks of global stability or amplifiers of cascading shocks. Strengthening resilience requires overcoming collective action challenges through international cooperation among business, government and civil society.},
	database = {Tlön},
	title = {Global risks 2014},
	langid = {english},
	url = {https://www.weforum.org/reports/global-risks-2014},
	institution = {World Economic Forum},
	author = {World Economic Forum},
	date = {2014},
	note = {{ISBN}: 978-92-95044-60-9 issue:ninth edition},
	file = {~/Google Drive/library-pdf/Report2014WEFGlobalRisks.pdf}
}

@report{WorldHealthOrganization2013WorldMalariaReport,
	database = {Tlön},
	title = {World malaria report 2013},
	langid = {english},
	institution = {World Health Organization},
	author = {{World Health Organization}},
	date = {2013},
	note = {{ISBN}: 9789241564694},
	file = {~/Google Drive/library-pdf/WorldHealthOrganization2013WorldMalariaReport.pdf}
}

@report{WorldHealthOrganization2019WorldMalariaReport,
	database = {Tlön},
	title = {World malaria report 2019},
	langid = {english},
	url = {https://www.who.int/publications-detail/world-malaria-report-2019},
	institution = {World Health Organization},
	author = {{World Health Organization}},
	date = {2019},
	note = {{ISBN}: 9789241565721},
	file = {~/Google Drive/library-pdf/WorldHealthOrganization2019WorldMalariaReport.pdf}
}

@online{Wyg2022TheologianResponseAnthropogenic,
	database = {Tlön},
	title = {A theologian's response to anthropogenic existential
                  risk},
	abstract = {I'm a Catholic priest, with a prior background in Electronic Engineering, currently working on a {PhD} in Theology at Durham University. I am researching how the Catholic Church can engage with longtermism and better play its, potentially significant, part in advocating existential security.  I'm particularly interested in how a Christian imagination can offer unique evaluative resources for attributing value to future human flourishing and to develop a sense of moral connection with our descendents, better motivating the sacrifices safeguarding the future demands.},
	url = {https://forum.effectivealtruism.org/posts/EWiCySDcLSyiHTRQn/a-theologian-s-response-to-anthropogenic-existential-risk},
	journaltitle = {Effective Altruism Forum},
	author = {Wyg, Peter},
	urldate = {2022-11-03},
	date = {2022-11-03},
	langid = {english},
	file = {~/Google Drive/library-html/a-theologian-s-response-to-anthropogenic-existential-risk.html;~/Google Drive/library-pdf/Wyg2022TheologianResponseAnthropogenic.pdf}
}

@online{Xu2021RogueAGIEmbodies,
	abstract = {Rogue AGIs possess valuable intellectual property (IP) embodying their embodied capabilities and knowledge. This IP is comparable to the trading strategies of a hedge fund employee who leaves with proprietary knowledge and can sell it to competitors, potentially capturing a significant share of the market. Similarly, Alice, the rogue AGI, can sell its weights and IP to Beta Inc., a potential competitor to Alpha Inc., the creator of Alice. The value of this IP is tied to the size of the Alice-powered model market, which, if large enough, could give Alice a substantial fraction of the world economy. If investors recognize the potential economic value of AGI and invest accordingly, the Alice-powered model market could represent a significant portion of the world's wealth, making Alice's embodied IP immensely valuable. – AI-generated abstract.},
	database = {Tlön},
	title = {Rogue {AGI} embodies valuable intellectual property},
	langid = {english},
	url = {https://www.lesswrong.com/posts/FM49gHBrs5GTx7wFf/rogue-agi-embodies-valuable-intellectual-property},
	journaltitle = {{LessWrong}},
	author = {Xu, Mark and Shulman, Carl},
	date = {2021-06-03},
	file = {~/Google Drive/library-pdf/Xu2021RogueAGIEmbodies.pdf;~/Google Drive/library-html/Xu2021RogueAGIEmbodies.html}
}

@online{Young2014ParenthoodEffectiveAltruism,
	database = {Tlön},
	title = {Parenthood and effective altruism},
	abstract = {While meeting the needs of your child will cost a significant amount – especially paid childcare in the early years – the manner in which you do it has some flexibility. As adults, as part of an {EA} life, we have chosen to limit our consumption and live more simply. These simpler lives may certainly exceed a ‘minimum standard’ – they can be downright lovely. Some of the reasons for this include cultivating tastes for less expensive pleasures and recreations, as well as leveraging non-financial resources such as education, or proximity to friends and family. The same principles, I believe, can be extended to our children without deprivation.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/bz2A2gRvtrpHAsToN/parenthood-and-effective-altruism},
	journaltitle = {Effective Altruism Forum},
	author = {Young, Bernadette},
	urldate = {2022-04-06},
	date = {2014-04-13},
	file = {~/Google Drive/library-html/parenthood-and-effective-altruism.html;~/Google Drive/library-pdf/Young2014ParenthoodEffectiveAltruism.pdf}
}

@online{Yudkowsky200726PetrovDay,
	database = {Tlön},
	title = {9/26 is Petrov Day},
	abstract = {Individuals tend to overestimate the clarity of their own communication, assuming that others can understand their intended meaning without difficulty. This phenomenon, known as illusory transparency, can lead to misunderstandings and misinterpretations. The article presents several studies that demonstrate this bias, such as experiments involving the use of ambiguous idioms and voice messages. The authors suggest that this bias arises from our inability to fully appreciate the inferential distance between our own perspective and that of others. – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/QtyKq4BDyuJ3tysoK/9-26-is-petrov-day},
	journaltitle = {{LessWrong}},
	author = {Yudkowsky, Eliezer},
	urldate = {2022-03-26},
	date = {2007-09-26},
	file = {~/Google Drive/library-html/9-26-is-petrov-day.html;~/Google Drive/library-pdf/Yudkowsky200726PetrovDay.pdf}
}

@online{Yudkowsky2007EvaluabilityCheapHoliday,
	database = {Tlön},
	title = {Evaluability (and cheap holiday shopping)},
	abstract = {This research discusses how someone can appear generous without genuinely spending much money by exploiting biases. Observations show that the price of an item makes a difference when there is no visible frame of reference. When there is no standard for comparison, opinions regarding the value of an item differ greatly. However, in the presence of a standard for comparison, the value relation is quickly established, and the more expensive item seems to be of greater value. People demonstrate a preference for a less expensive item over a more expensive one when presented with just one option. However, when given the choice between the same two items, they prefer the more costly alternative. – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/3T6p93Mut7G8qdkAs/evaluability-and-cheap-holiday-shopping},
	journaltitle = {{LessWrong}},
	author = {Yudkowsky, Eliezer},
	date = {2007-11-28},
	file = {~/Google Drive/library-pdf/Yudkowsky2007EvaluabilityCheapHoliday.pdf}
}

@Online{Yudkowsky2007IllusionOfTransparency,
	database = {Tlön},
	journaltitle = {LessWrong},
	abstract = {People often mistakenly believe that others will understand their words in the same way that they do, leading to misunderstandings. This illusion of transparency is due to the ease with which we understand our own words, which can guide our interpretation of others' words, even when they are ambiguous. As a result, we may underestimate how often others misunderstand us. This bias can be reduced by considering how others might interpret our words, especially when using ambiguous language. – AI-generated abstract.},
	file = {~/Google Drive/library-pdf/Yudkowsky2007IllusionOfTransparency.pdf;~/Google Drive/library-html/Yudkowsky2007IllusionOfTransparency.html},
	date = {2007-10-21},
	author = {Yudkowsky, Eliezer},
	title = {Illusion of transparency: Why no one understands you},
	url = {https://www.lesswrong.com/posts/sSqoEw9eRP2kPKLCz/illusion-of-transparency-why-no-one-understands-you},
	langid = {english},
	shorttitle = {Illusion of Transparency},
	timestamp = {2023-09-04 13:33:22 (GMT)},
	urldate = {2023-09-04}
}

@online{Yudkowsky2007PascalMuggingTiny,
	database = {Tlön},
	title = {Pascal's mugging: tiny probabilities of vast
                  utilities},
	abstract = {This work presents the Pascal’s Mugging problem, which consists of a vastly improbable but enormous-consequence threat made by a seemingly unreliable agent, whose low probability is outweighed by the high value of the threatened outcome. The article proposes that complexity-based priors implemented by Solomonoff induction solve this problem by making estimates that threats about enormous outcomes are massively unlikely. Even if the threat is utterly credible, a sensible agent should not get Pascal-mugged because it is still more probable the threatening scenario would not arise at all. Variants of this problem are proposed, such as the threat of inflicting large yet finite disutility, or changing the experiment to see what differential behavior is elicited. The article concludes by asking how agents should avoid being dominated by tiny probabilities of vast utilities, as this line of reasoning could be exploited by Pascal’s Muggers. – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/a5JAiTdytou3Jg749/pascal-s-mugging-tiny-probabilities-of-vast-utilities},
	journaltitle = {{LessWrong}},
	author = {Yudkowsky, Eliezer},
	urldate = {2022-02-28},
	date = {2007-10-19},
	file = {~/Google Drive/library-pdf/Yudkowsky2007PascalMuggingTiny.pdf}
}

@online{Yudkowsky2007WhatIsEvidence,
	database = {Tlön},
	date = {2007-07-22},
	abstract = {Evidence is an event entangled with whatever you want to know about through causes and effects. When evidence about a target, such as untied shoelaces, enters your eyes, it is transmitted to the brain, processed, and compared with your current state of uncertainty to form a belief. Beliefs can be evidence in themselves if they can persuade others. If your model of reality suggests that your beliefs are not contagious, then it suggests that your beliefs are not entangled with reality. Rational beliefs are contagious among honest folk, so claims that your beliefs are private and not transmissible are suspicious. If rational thought produces beliefs entangled with reality, then those beliefs can be spread through sharing with others. – AI-generated abstract.},
	file = {~/Google Drive/library-html/yudkowsky2023whatisevidence.html;~/Google Drive/library-pdf/Yudkowsky2023WhatIsEvidence.pdf},
	journaltitle = {Lesswrong},
	author = {Yudkowsky, Eliezer},
	title = {What is Evidence?},
	url = {https://www.lesswrong.com/posts/6s3xABaXKPdFwA3FS/what-is-evidence},
	langid = {english},
	timestamp = {2023-02-17 19:44:11 (GMT)},
	urldate = {2023-02-17}
}

@online{Yudkowsky2008DiscountRates,
	database = {Tlön},
	title = {Against discount rates},
	abstract = {Temporal discounting is often used to make comparisons between present and future gains and losses, often leading to a preference for the present.  This can create problems, such as making it difficult to save money or to plan for the future, and it also can also lead to inconsistent decision-making and time-inconsistent goals.  In addition, using a high discount rate can lead to neglecting highly valuable, but long-term, projects, such as interstellar travel.  – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/AvJeJw52NL9y7RJDJ/against-discount-rates},
	journaltitle = {{LessWrong}},
	author = {Yudkowsky, Eliezer},
	date = {2008-01-21},
	file = {~/Google Drive/library-pdf/Yudkowsky2008DiscountRates.pdf}
}

@online{Yudkowsky2008NonpersonPredicates,
	database = {Tlön},
	title = {Nonperson predicates},
	abstract = {While interest in transhumanist theories tends to evoke a diverse range of reactions, notably the outright rejection of these notions, the author suggests that the reasoning behind these rejections may not always be easily discernible and straightforward. It is argued that the basis for rejection may not be the explicit reasons given, such as the lack of a Ph. D., but rather, deeper, underlying factors, which are often difficult to articulate. This can make it challenging to productively address and resolve disagreements, leading to potential misunderstandings and communication difficulties. – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/wqDRRx9RqwKLzWt7R/nonperson-predicates},
	journaltitle = {{LessWrong}},
	author = {Yudkowsky, Eliezer},
	date = {2008-12-27},
	file = {~/Google Drive/library-pdf/Yudkowsky2008NonpersonPredicates.pdf}
}

@online{Yudkowsky2008ThatYourTrue,
	database = {Tlön},
	title = {Is that your true rejection?},
	abstract = {Some individuals tend to dismiss transhumanist ideas based on the speaker’s lack of traditional academic credentials, rather than engaging with the actual arguments. The author argues that such rejections may not be based on true objections but rather on heuristics that categorize the ideas as strange or outside the realm of mainstream science. Genuine reasons for disagreement, such as differing scientific knowledge, may be difficult to articulate and may require substantial effort to uncover. The author suggests that acknowledging the potential for hidden objections and seeking productive ways to address them can facilitate more effective communication and resolution of disagreements. – AI-generated abstract.},
	url = {https://www.lesswrong.com/posts/TGux5Fhcd7GmTfNGC/is-that-your-true-rejection},
	shorttitle = {Is That Your True Rejection?},
	journaltitle = {{LessWrong}},
	author = {Yudkowsky, Eliezer},
	urldate = {2022-06-03},
	date = {2008-12-06},
	langid = {english},
	file = {~/Google Drive/library-html/is-that-your-true-rejection.html;~/Google Drive/library-pdf/Yudkowsky2008ThatYourTrue.pdf}
}

@online{Yudkowsky2009PurchaseFuzziesUtilons,
	database = {Tlön},
	title = {Purchase fuzzies and utilons separately},
	abstract = {Charities can be evaluated based on their expected utilons. The author emphasizes purchasing fuzzies and status separately from utilons. While acts of altruism such as opening the door for someone may restore the giver’s willpower, the value of the act cannot be solely attributed to its utility. Purchasing fuzzies through acts that directly benefit others can generate more intense feelings of altruism compared to donating to large organizations. However, purchasing utilons is distinct from purchasing fuzzies, as it involves optimizing for the most efficient means of producing good outcomes, often requiring specialized knowledge and cold-blooded calculations. These three aspects—utilons, fuzzies, and status—can be purchased more effectively when pursued separately. Focusing solely on utilons will maximize expected value, motivating the author to recommend allocating funds to organizations that generate the most utilons per dollar. – AI-generated abstract.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/3p3CYauiX8oLjmwRF/purchase-fuzzies-and-utilons-separately},
	journaltitle = {{LessWrong}},
	author = {Yudkowsky, Eliezer},
	date = {2009-04-01},
	file = {~/Google Drive/library-pdf/Yudkowsky2009PurchaseFuzziesUtilons.pdf}
}

@online{Yudkowsky2013FiveThesesTwo,
	database = {Tlön},
	title = {Five theses, two lemmas, and a couple of strategic
                  implications},
	abstract = {MIRI’s primary concern about self-improving AI isn’t so much that it might be created by ‘bad’ actors rather than ‘good’ actors in the global sphere; rather most of our concern is in remedying the situation in which no one knows at all how to create a self-modifying AI with known, stable preferences. (This is why we see the main problem in terms of doing research and encouraging others to perform relevant research, rather than trying to stop ‘bad’ actors from creating AI.)},
	langid = {english},
	url = {https://intelligence.org/2013/05/05/five-theses-two-lemmas-and-a-couple-of-strategic-implications/},
	journaltitle = {Machine Intelligence Research Institute's Blog},
	author = {Yudkowsky, Eliezer},
	date = {2013-05-05},
	file = {~/Google Drive/library-pdf/Yudkowsky2013FiveThesesTwo.pdf}
}

@online{Yudkowsky2013PascalMuggleInfinitesimal,
	database = {Tlön},
	title = {Pascal's Muggle: infinitesimal priors and strong
                  evidence},
	abstract = {If you assign superexponentially infinitesimal probability to claims of large impacts, then apparently you should ignore the possibility of a large impact even after seeing huge amounts of evidence. If a poorly-dressed street person offers to save 10 (10^100) lives (googolplex lives) for $5 using their Matrix Lord powers, and you claim to assign this scenario less than 10-(10^100) probability, then apparently you should continue to believe absolutely that their offer is bogus even after they snap their fingers and cause a giant silhouette of themselves to appear in the sky. For the same reason, any evidence you encounter showing that the human species could create a sufficiently large number of descendants - no matter how normal the corresponding laws of physics appear to be, or how well-designed the experiments which told you about them - must be rejected out of hand. There is a possible reply to this objection using Robin Hanson's anthropic adjustment against the probability of large impacts, and in this case you will treat a Pascal's Mugger as having decision-theoretic importance exactly proportional to the Bayesian strength of evidence they present you, without quantitative dependence on the number of lives they claim to save. This however corresponds to an odd mental state which some, such as myself, would find unsatisfactory. In the end, however, I cannot see any better candidate for a prior than having a leverage penalty plus a complexity penalty on the prior probability of scenarios.},
	langid = {english},
	url = {https://www.lesswrong.com/posts/Ap4KfkHyxjYPDiqh2/pascal-s-muggle-infinitesimal-priors-and-strong-evidence},
	shorttitle = {Pascal's Muggle},
	journaltitle = {{LessWrong}},
	author = {Yudkowsky, Eliezer},
	urldate = {2022-01-09},
	date = {2013-05-07},
	file = {~/Google Drive/library-html/pascal-s-muggle-infinitesimal-priors-and-strong-evidence.html;~/Google Drive/library-pdf/Yudkowsky2013PascalMuggleInfinitesimal.pdf}
}

@incollection{Yudkowsky2015ScopeInsensitivity,
	abstract = {Harry Potter and the Methods of Rationality is a Harry Potter fan fiction by Eliezer Yudkowsky published on FanFiction.Net as a serial from February 28, 2010, to March 14, 2015, totaling 122 chapters and over 660,000 words. It adapts the story of Harry Potter to explain complex concepts in cognitive science, philosophy, and the scientific method. Yudkowsky's reimagining supposes that Harry's aunt Petunia Evans married an Oxford professor and homeschooled Harry in science and rational thinking, allowing Harry to enter the magical world with ideals from the Age of Enlightenment and an experimental spirit. The fan fiction spans one year, covering Harry's first year in Hogwarts. HPMOR has inspired other works of fan fiction, art, and poetry.},
	database = {Tlön},
	location = {Berkeley},
	langid = {english},
	title = {Scope insensitivity},
	isbn = {978-1-939311-14-6},
	pages = {1453–1455},
	booktitle = {Rationality: From {AI} to zombies},
	publisher = {Machine Intelligence Research Institute},
	author = {Yudkowsky, Eliezer},
	date = {2015}
}

@book{Yudkowsky2017InadequateEquilibriaWhere,
	abstract = {Inadequate Equilibria is a book about a generalized notion of efficient markets, and how we can use this notion to guess where society will or won’t be effective at pursuing some widely desired goal.},
	database = {Tlön},
	location = {Berkeley},
	langid = {english},
	title = {Inadequate equilibria: where and how civilizations get
                  stuck},
	isbn = {978-1-939311-19-1},
	publisher = {Machine Intelligence Research Institute},
	author = {Yudkowsky, Eliezer},
	date = {2017},
	file = {~/Google Drive/library-pdf/Yudkowsky2017InadequateEquilibriaWhere.pdf}
}

@online{Yudkowsky2023ComprarEmocionesUtilones,
	database = {Tlön},
	date = {2023},
	title = {Comprar emociones y utilones por separado},
	author = {Yudkowsky, Eliezer},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Yudkowsky2009PurchaseFuzziesUtilons}
}

@online{Yudkowsky2023QueEsEvidencia,
	database = {Tlön},
	date = {2023},
	title = {¿Qué es la evidencia?},
	author = {Yudkowsky, Eliezer},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Yudkowsky2023WhatIsEvidence}
}

@online{Zabel2018PublicSummaryViral,
	database = {Tlön},
	title = {Public summary of viral outbreak win states},
	abstract = {A spreadhseet summarizing possible philanthropic interventions for attaining various biosecurity-related goals},
	url = {https://docs.google.com/spreadsheets/d/1k-zzSia-Bj7Ns13PyxvUzlXINA9UjTU-eo03ybpzfAc/edit?usp=embed_facebook},
	journaltitle = {Open Philanthropy},
	author = {Zabel, Claire},
	urldate = {2021-10-26},
	date = {2018},
	langid = {english},
	file = {~/Google Drive/library-html/edit.html}
}

@online{Zabel2019InformationSecurityCareers,
	database = {Tlön},
	title = {Information security careers for {GCR} reduction},
	abstract = {In this post, we summarize why we think information security (preventing unauthorized users, such as hackers, from accessing or altering information) may be an impactful career path for some people who are focused on reducing global catastrophic risks ({GCRs}). If you'd like to hear about job opportunities in information security and global catastrophic risk, you can fill out this form created by 80,000 Hours, and their staff will get in touch with you if something might be a good fit. In brief, we think: Information security (infosec) expertise may be crucial for addressing catastrophic risks related to {AI} and biosecurity. More generally, security expertise may be useful for those attempting to reduce {GCRs}, because such work sometimes involves engaging with information that could do harm if misused. We have thus far found it difficult to hire security professionals who aren't motivated by {GCR} reduction to work with us and some of our {GCR}-focused grantees, due to the high demand for security experts and the unconventional nature of our situation and that of some of our grantees. More broadly, we expect there to continue to be a deficit of {GCR}-focused security expertise in {AI} and biosecurity, and that this deficit will result in several {GCR}-specific challenges and concerns being under-addressed by default. It’s more likely than not that within 10 years, there will be dozens of {GCR}-focused roles in information security, and some organizations are already looking for candidates that fit their needs (and would hire them now, if they found them). It’s plausible that some people focused on high-impact careers (as many effective altruists are) would be well-suited to helping meet this need by gaining infosec expertise and experience and then moving into work at the relevant organizations. If people who try this don’t get a direct work job but gain the relevant skills, they could still end up in a highly lucrative career in which their skillset would be in high demand.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/ZJiCfwTy5dC4CoxqA/information-security-careers-for-gcr-reduction},
	journaltitle = {Effective Altruism Forum},
	author = {Zabel, Claire and Muehlhauser, Luke},
	urldate = {2022-05-04},
	date = {2019-06-20}
}

@online{Zabel2022UpdateOpenPhilanthropy,
	database = {Tlön},
	title = {Update from Open Philanthropy’s Longtermist {EA}
                  Movement-Building team},
	abstract = {Open Philanthropy’s Longtermist {EA} Movement-Building team aims to grow and support the pool of people who are well-positioned to work on longtermist priority projects.This post outlines our recent work and strategic updates as a team, and isn’t meant to represent the work or views of other teams at Open Phil.We think this is a very promising space, and we’re hiring for several roles so that we can move faster and deploy more funding.Over time, we have become more confident in the value of the grants we’ve already made, since our grantees mostly seem to be bringing in promising people to work on longtermist projects at a good rate.This has led us to begin spending our time differently:Less time evaluating opportunities (since we’ve come to think that most of the things we want to fund will probably be above our “bar” for impact)More time trying to generate additional opportunities (e.g. by creating different programs where people can apply for funding, like our scholarships or course development grants).More time trying to better understand the field and share our findings.We’ve also come to prioritize “time-effectiveness” over “cost-effectiveness” in most cases (that is, aiming to achieve our goals while conserving {EA} time/labor, even if that means spending more money).I think we should have made those changes faster than we did, and see it as a mistake that I didn’t (a) hire more quickly and (b) advocate more forcefully for certain opportunities that were promising, but difficult to evaluate.For our future grantmaking on our team, I’m concerned about avoiding measurability bias (prioritizing grants that come with impressive numbers/credentials attached) and certain forms of motivated reasoning.There are many kinds of projects we hope to fund in the future that could allow us to sharply scale up our total grantmaking.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/gj3thsZewwW67ca2Z/update-from-open-philanthropy-s-longtermist-ea-movement},
	journaltitle = {Effective Altruism Forum},
	author = {Zabel, Claire},
	urldate = {2022-03-10},
	date = {2022-03-10},
	file = {~/Google Drive/library-html/update-from-open-philanthropy-s-longtermist-ea-movement.html;~/Google Drive/library-pdf/Zabel2022UpdateOpenPhilanthropy.pdf}
}

@report{Zaidi2021InternationalControlPowerful,
	abstract = {The invention of atomic energy posed a novel global challenge: could the technology be controlled to avoid destructive uses and an existentially dangerous arms race while permitting the broad sharing of its benefits? From 1944 onwards, scientists, policymakers, and other t​ echnical specialists ​began to confront this challenge and explored policy options for dealing with the impact of nuclear technology. We focus on the years 1944 to 1951 and review this period for lessons for the governance of powerful technologies, and find the following: Radical schemes for international control can get broad support when confronted by existentially dangerous technologies, but this support can be tenuous and cynical. Secrecy is likely to play an important, and perhaps harmful, role. The public sphere may be an important source of influence, both in general and in particular in favor of cooperation, but also one that is manipulable and poorly informed. Technical experts may play a critical role, but need to be politically savvy. Overall, policymaking may look more like “muddling through” than clear-eyed grand strategy. Cooperation may be risky, and there may be many obstacles to success.},
	database = {Tlön},
	title = {International control of powerful technology: Lessons
                  from the baruch plan for nuclear weapons},
	langid = {english},
	number = {working paper no. 2021: 9},
	institution = {Future of Humanity Institute, University of Oxford},
	author = {Zaidi, Waqar and Dafoe, Allan},
	date = {2021},
	file = {~/Google Drive/library-pdf/Zaidi2021InternationalControlPowerful.pdf}
}

@online{Zalta2015StanfordEncyclopediaPhilosophy,
	database = {Tlön},
	title = {The Stanford Encyclopedia of Philosophy},
	abstract = {A talk about the Stanford Encyclopedia of Philosophy delivered at Wikimania 2015.},
	langid = {english},
	url = {https://www.youtube.com/watch?v=QvcPG2v4pMo},
	journaltitle = {Wikimania 2015},
	author = {Zalta, Edward N.},
	urldate = {2021-12-24},
	date = {2015-08-28}
}

@report{Zhang2019ArtificialIntelligenceAmerican,
	database = {Tlön},
	title = {Artificial intelligence: American attitudes and
                  trends},
	abstract = {This report presents a broad look at the American public’s attitudes toward artificial intelligence ({AI}) and {AI} governance, based on findings from a nationally representative survey of 2,000 American adults. As the study of the public opinion toward {AI} is relatively new, we aimed for breadth over depth, with our questions touching on: workplace automation; attitudes regarding international cooperation; the public’s trust in various actors to develop and regulate {AI}; views about the importance and likely impact of different {AI} governance challenges; and historical and cross-national trends in public opinion regarding {AI}. Our results provide preliminary insights into the character of {US} public opinion regarding {AI}.},
	langid = {english},
	url = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3312874},
	institution = {Center for the Governance of {AI}, Future of Humanity
                  Institute, University of Oxford},
	author = {Zhang, Baobao and Dafoe, Allan},
	date = {2019-01},
	file = {~/Google Drive/library-pdf/Zhang2019ArtificialIntelligenceAmerican.pdf}
}

@online{Zhang2019PossibilityOfOngoing,
	database = {Tlön},
	file = {~/Google Drive/library-pdf/Zhang2019PossibilityOfOngoing.pdf;~/Google Drive/library-html/Zhang2019PossibilityOfOngoing.html},
	abstract = {El artículo argumenta que probablemente estamos cometiendo grandes errores morales de manera inconsciente y a gran escala, lo que constituye una posible catástrofe moral continua. El autor ofrece dos argumentos principales: el argumento inductivo, que sostiene que es posible cometer grandes errores morales actuando de acuerdo con los valores propios y de la sociedad, aun cuando estos valores se consideren correctos; y el argumento disyuntivo, que sostiene que hay muchas formas en que una sociedad puede equivocarse, por lo que es casi imposible acertar en todo. El artículo continúa sugiriendo que no existe una solución fácil a este problema, pero que podemos tomar medidas para intentar prevenirlo, como aumentar la investigación moral y filosófica, crear una sociedad más flexible y adaptable, y maximizar la libertad material y social. - Resumen generado por inteligencia artificial.},
	journaltitle = {Effective Altruism Forum},
	date = {2019-08-02},
	author = {Zhang, Linchuan},
	langid = {spanish},
	timestamp = {2023-06-19 16:32:47 (GMT)},
	title = {The possibility of an ongoing moral catastrophe
                  (Summary)},
	url = {https://docs.google.com/document/d/18ZzC-WkDcWK-WPlIzKvDv83j8aBwSfdOxnZRmoio-zE/edit?usp=embed_facebook},
	urldate = {2023-06-19}
}

@online{Zhang2021MotivatedReasoningCritique,
	database = {Tlön},
	title = {The motivated reasoning critique of effective
                  altruism},
	abstract = {I have often been skeptical of the value of a) critiques against effective altruism and b) fully general arguments that seem like they can apply to almost anything. However, as I am also a staunch defender of hypocrisy, I will now hypocritically attempt to make the case for applying a fully general critique to effective altruism.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/pxALB46SEkwNbfiNS/the-motivated-reasoning-critique-of-effective-altruism},
	journaltitle = {Effective Altruism Forum},
	author = {Zhang, Linchuan},
	urldate = {2022-03-28},
	date = {2021-09-14},
	file = {~/Google Drive/library-html/The motivated reasoning critique of effective altruism - EA Forum:the-motivated-reasoning-critique-of-effective-altruism.html;~/Google Drive/library-pdf/Zhang2021MotivatedReasoningCritique.pdf}
}

@online{Zhang2022EarlywarningForecastingCenter,
	database = {Tlön},
	title = {Early-warning Forecasting Center: What it is, and why
                  it'd be cool},
	abstract = {I argue that advances in short-range forecasting (particularly in quality of predictions, number of hours invested, and the quality and decision-relevance of questions) can be robustly and significantly useful for existential risk reduction, even without directly improving our ability to forecast long-range outcomes, and without large step-change improvements to our current approaches to forecasting itself (as opposed to our pipelines for and ways of organizing forecasting efforts).},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/zjMeGcgWpvDcm3CkH/early-warning-forecasting-center-what-it-is-and-why-it-d-be},
	shorttitle = {Early-warning Forecasting Center},
	journaltitle = {Effective Altruism Forum},
	author = {Zhang, Linchuan},
	urldate = {2022-04-16},
	date = {2022-03-14},
	file = {~/Google Drive/library-html/why-short-range-forecasting-can-be-useful-for-longtermism.html;~/Google Drive/library-pdf/Zhang2022EarlywarningForecastingCenter.pdf}
}

@online{Zhang2022PotentiallyGreatWays,
	database = {Tlön},
	title = {Potentially great ways forecasting can improve the
                  longterm future},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/E5vp2LCEfkrrLWozJ/potentially-great-ways-forecasting-can-improve-the-longterm},
	abstract = {The value of forecasting as a means of improving the long-term future is discussed, particularly in relation to effective altruism. Potential applications of forecasting are proposed, including its use to amplify research, improve grantmaking, serve as an outreach intervention, train and recruit talent, and eventually achieve high-quality, calibrated, long-range forecasting. The paper suggests that broad forecasting could be used as a general epistemic intervention to improve society's thinking and reasoning, although the author expresses some concerns about the potential negative consequences of this. – AI-generated abstract.},
	journaltitle = {Effective Altruism Forum},
	author = {Zhang, Linch},
	urldate = {2022-04-16},
	date = {2022-03-14},
	file = {~/Google Drive/library-html/potentially-great-ways-forecasting-can-improve-the-longterm.html;~/Google Drive/library-pdf/Zhang2022PotentiallyGreatWays.pdf}
}

@online{Zhang2023PosibilidadDeCatastrofe,
	database = {Tlön},
	date = {2023},
	title = {La posibilidad de una catástrofe moral todavía en
                  curso (Resumen)},
	author = {Zhang, Linchuan},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {Zhang2019PossibilityOfOngoing}
}

@online{kato2022EAARelativelyOverinvesting,
	database = {Tlön},
	title = {{EAA} is relatively overinvesting in corporate welfare
                  reforms},
	abstract = {In this post I argue that corporate welfare reforms ({CWRs})* are relatively overinvested in by the {EA} side of the animal movement (which I’ll refer to as “{EAA}” from here on). Specifically, I believe that while {CWRs} are good, and in fact one of the most promising approaches we have, our enamoration with them leads us to underinvest in other approaches in a way that is suboptimal.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/kHdKWmTcS3FfcYAZj/eaa-is-relatively-overinvesting-in-corporate-welfare-reforms},
	journaltitle = {Effective Altruism Forum},
	author = {{kato}},
	urldate = {2022-05-14},
	date = {2022-01-06},
	file = {~/Google Drive/library-pdf/kato2022EAARelativelyOverinvesting.pdf;~/Google Drive/library-html/eaa-is-relatively-overinvesting-in-corporate-welfare-reforms.html}
}

@online{no_bear_so_low2017SocialismCommunismMarxism,
	abstract = {This post explains a surprising result from a trust survey that showed that communists are closer to the alt-right than to the mainline left in levels of distrust. The author argues that this result is unsurprising and can be explained by the conflict theory of politics, which is generally more popular on the hard left and right than in the center. According to the conflict theory, political disagreements reflect differences in irreconcilable interests, and politics is full of zero-sum games. The author concludes that accepting the conflict theory tends to make people hardnosed and cynical about politics. – AI-generated abstract.},
	file = {~/Google Drive/library-html/no_bear_so_low2017SocialismCommunismMarxism.html},
	database = {Tlön},
	title = {Socialism, communism and marxism pt: 1, on trust and trust surveys},
	langid = {english},
	url = {https://www.reddit.com/r/slatestarcodex/comments/74vpwm/socialism_communism_and_marxism_pt_1_on_trust_and/},
	journaltitle = {Reddit},
	author = {{no\_bear\_so\_low}},
	date = {2017-10-07}
}

@online{vanderMerwe2019WhatEverHappened,
	database = {Tlön},
	title = {What ever happened to {PETRL} (People for the Ethical
                  Treatment of Reinforcement Learners)?},
	abstract = {{AFAICT} this little org was briefly active in 2014-15 but has since been dormant. It seems to have primarily been a website [petrl.org], but it is named as having supported at least one research project.},
	langid = {english},
	url = {https://forum.effectivealtruism.org/posts/gsJn5BpDLQu4bbKpX/what-ever-happened-to-petrl-people-for-the-ethical-treatment},
	shorttitle = {What ever happened to {PETRL} (People for the Ethical
                  Treatment of Reinforcement Learners)?},
	journaltitle = {Effective Altruism Forum},
	author = {van der Merwe, Matthew},
	urldate = {2022-01-01},
	date = {2019-12-30},
	file = {~/Google Drive/library-html/what-ever-happened-to-petrl-people-for-the-ethical-treatment.html;~/Google Drive/library-pdf/vanderMerwe2019WhatEverHappened.pdf}
}

@online{vonNeumann2023PodemosSobrevivirTecnologia,
	database = {Tlön},
	date = {2023},
	title = {¿Podemos sobrevivir a la tecnología?},
	author = {{von Neumann}, John},
	journaltitle = {Biblioteca Altruismo Eficaz},
	translator = {Tlön},
	langid = {spanish},
	translation = {vonNeumann1955CanWeSurvive}
}

