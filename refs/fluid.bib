
@online{Shulman2020NuclearWarIs,
	crossref = {Ladish2023NuclearWarIs},
	database = {Tlön},
	database = {Tlön},
	file = {~/Google Drive/library-pdf/Shulman2020NuclearWarIs.pdf;~/Google Drive/library-html/Shulman2020NuclearWarIs.html},
	date = {2020-11-07},
	author = {Shulman, Carl},
	abstract = {Comment by {CarlShulman} - I agree it's very unlikely
                  that a nuclear war discharging current arsenals could
                  directly cause human extinction. But the conditional
                  probability of extinction given all-out nuclear war
                  can go much higher if the problem gets worse. Some
                  aspects of this: -at the peak of the Cold War arsenals
                  there were over 70,000 nuclear weapons, not 14,000
                  -this Brookings estimate puts spending building the
                  {US} nuclear arsenal at several trillion current
                  dollars, with lower marginal costs per weapon, e.g.
                  \$20M per weapon and \$50-100M all-in for for {ICBMs}
                  -economic growth since then means the world could
                  already afford far larger arsenals in a renewed arms
                  race -current {US} military expenditure is over \$700B
                  annually, about 1/30th of {GDP}; at the peak of the
                  Cold War in the 50s and 60s it was about 1/10th;
                  Soviet expenditure was proportionally higher -so with
                  1950s proportional military expenditures, half going
                  to nukes, the {US} and China could each produce
                  20,000+ {ICBMs}, each of which could be fitted with
                  {MIRVs} and several warheads, building up to millions
                  of warheads over a decade or so; the numbers could be
                  higher for cheaper delivery systems -economies of
                  scale and improvements in technology would likely
                  bring down the per warhead cost -if {AI} and robotics
                  greatly increase economic growth the above numbers
                  could be increased by orders of magnitude -radiation
                  effects could be intentionally greatly increased with
                  alternative warhead composition -all-out discharge of
                  strategic nuclear arsenals is also much more likely to
                  be accompanied by simultaneous deployment of other
                  {WMD}, including pandemic bioweapons (which the
                  Soviets pursued as a strategic weapon for such
                  circumstances)and drone swarms (which might kill
                  survivors in bunkers); the combined effects of future
                  versions of all of these {WMD} at once may
                  synergistically cause extinction},
	langid = {english},
	timestamp = {2023-08-05 12:19:19 (GMT)},
	title = {Comment on "Nuclear war is unlikely to cause human extinction"},
	url = {https://forum.effectivealtruism.org/posts/mxKwP2PFtg8ABwzug/nuclear-war-is-unlikely-to-cause-human-extinction},
	urldate = {2023-08-05}
}

@InBook{Carruthers2012AgainstMoralStanding,
	file = {~/Google Drive/library-pdf/Carruthers2012AgainstMoralStanding.pdf},
	date = {2012},
	url = {https://faculty.philosophy.umd.edu/pcarruthers/The%20Animals%20Issue.pdf},
	author = {Carruthers, Peter},
	title = {Against the moral standing of animals},
	booktitle = {Questions of life and death: readings in practical ethics},
	crossref = {Morris2012QuestionsOfLife},
	timestamp = {2023-09-27 08:53:23 (GMT)}
}

@InBook{Brynjolfsson2019ArtificialIntelligenceAnd,
	title = {Artificial intelligence and the modern productivity paradox: a clash of expectations
and statistics},
	author = {Brynjolfsson, Erik and Rock, Daniel and Syverson, Chad and De},
	date = {2019},
	booktitle = {The economics of artificial intelligence: an agenda},
	crossref = {Agrawal2019EconomicsOfArtificial},
	timestamp = {2023-09-28 11:02:36 (GMT)}
}

@online{Aaronson2008BulletSwallowers,
	file = {~/Google Drive/library-html/Aaronson2008BulletSwallowers.html;~/Google Drive/library-pdf/Aaronson2008BulletSwallowers.pdf},
	author = {Aaronson, Scott},
	database = {Tlön},
	abstract = {Question for the day: what do libertarianism and the
                  Many-Worlds Interpretation of quantum mechanics have
                  in common? Interest in the two worldviews seems to be
                  positively correlated: think of quant...},
	date = {2008-05-13},
	journaltitle = {Shtetl-Optimized},
	langid = {american},
	timestamp = {2023-09-25 20:04:51 (GMT)},
	title = {The bullet-swallowers},
	url = {https://scottaaronson.blog/?p=326},
	urldate = {2023-09-25}
}

@Book{Agrawal2019EconomicsOfArtificial,
	date = {2019},
	editor = {Agrawal, Ajay and Gans, Joshua and Goldfarb, Avi},
	langid = {english},
	database = {Tlön},
	isbn = {9780226613338},
	keywords = {Artificial intelligence, Economic aspects},
	location = {Chicago},
	pagetotal = {630},
	publisher = {The University of Chicago Press},
	shorttitle = {The economics of artificial intelligence},
	timestamp = {2023-09-28 11:01:48 (GMT)},
	title = {The economics of artificial intelligence: an agenda}
}

@article{Aird2020MakingDecisionsUnder,
	file = {~/Google Drive/library-html/Aird2020MakingDecisionsUnder.html;~/Google Drive/library-pdf/Aird2020MakingDecisionsUnder.pdf},
	date = {2020-01-01},
	journaltitle = {Effective Altruism Forum},
	author = {Aird, Michael},
	title = {Making decisions under moral uncertainty},
	url = {https://forum.effectivealtruism.org/posts/ex834aaANLhamLkvf/making-decisions-under-moral-uncertainty},
	abstract = {Overview/purpose of this sequence While working on an
                  (upcoming) post about a new way to think about moral
                  uncertainty, I unexpectedly discovered th...},
	langid = {english},
	timestamp = {2023-09-25 18:32:37 (GMT)},
	urldate = {2023-09-25}
}

@article{Ali2019DiscriminationThroughOptimization,
	file = {~/Google Drive/library-pdf/Ali2019DiscriminationThroughOptimization.pdf},
	author = {Ali, Muhammad and Sapiezynski, Piotr and Bogen, Miranda and
                  Korolova, Aleksandra and Mislove, Alan and Rieke, Aaron},
	database = {Tlön},
	title = {Discrimination Through Optimization: How Facebook's Ad
                  Delivery Can Lead To Biased Outcomes},
	volume = {3},
	pages = {1--30},
	doi = {10.1145/3359301},
	url = {https://dl.acm.org/doi/10.1145/3359301},
	abstract = {The enormous financial success of online advertising platforms
                  is partially due to the precise targeting features they offer.
                  Although researchers and journalists have found many ways that
                  advertisers can target---or exclude---particular groups of
                  users seeing their ads, comparatively little attention has
                  been paid to the implications of the platform's ad delivery
                  process, comprised of the platform's choices about which users
                  see which ads. It has been hypothesized that this process can
                  "skew" ad delivery in ways that the advertisers do not intend,
                  making some users less likely than others to see particular
                  ads based on their demographic characteristics. In this paper,
                  we demonstrate that such skewed delivery occurs on Facebook,
                  due to market and financial optimization effects as well as
                  the platform's own predictions about the "relevance" of ads to
                  different groups of users. We find that both the advertiser's
                  budget and the content of the ad each significantly contribute
                  to the skew of Facebook's ad delivery. Critically, we observe
                  significant skew in delivery along gender and racial lines for
                  "real" ads for employment and housing opportunities despite
                  neutral targeting parameters. Our results demonstrate
                  previously unknown mechanisms that can lead to potentially
                  discriminatory ad delivery, even when advertisers set their
                  targeting parameters to be highly inclusive. This underscores
                  the need for policymakers and platforms to carefully consider
                  the role of the ad delivery optimization run by ad platforms
                  themselves---and not just the targeting choices of
                  advertisers---in preventing discrimination in digital
                  advertising.},
	date = {2019-11-07},
	issn = {2573-0142},
	issue = {{CSCW}},
	journaltitle = {Proceedings of the {ACM} on Human-Computer Interaction},
	langid = {english},
	shortjournal = {Proc. {ACM} Hum.-Comput. Interact.},
	shorttitle = {Discrimination through Optimization},
	timestamp = {2023-09-28 10:51:22 (GMT)},
	urldate = {2023-09-28}
}

@online{AnimalCharitiesEvaluators2017SubjectiveConfidenceIntervals,
	file = {~/Google Drive/library-pdf/Evaluators2017SubjectiveConfidenceIntervals.pdf;~/Google Drive/library-html/Evaluators2017SubjectiveConfidenceIntervals.html},
	eventdate = {2018-04},
	database = {Tlön},
	date = {2017-11-23},
	author = {{Animal Charity Evaluators}},
	abstract = {We use subjective confidence intervals ({SCIs}) in our work to
                  identify the best ways to help animals. Here we explain what
                  our {SCIs} are and how we use them, and provide some simple
                  and detailed examples to help clarify possible points of
                  confusion. ... Read more},
	journaltitle = {Animal Charity Evaluators},
	langid = {american},
	timestamp = {2023-09-27 18:57:29 (GMT)},
	title = {Subjective Confidence Intervals},
	url = {https://animalcharityevaluators.org/research/methodology/subjective-confidence-interval/},
	urldate = {2023-09-27}
}

@online{AnimalEthics2019WildAnimalSuffering,
	file = {~/Google Drive/library-html/AnimalEthics2019WildAnimalSuffering.html;~/Google Drive/library-pdf/AnimalEthics2019WildAnimalSuffering.pdf},
	url = {https://www.animal-ethics.org/introduction-to-wild-animal-suffering/},
	date = {2019-12-16},
	langid = {english},
	database = {Tlön},
	journaltitle = {Animal Ethics},
	title = {Wild animal suffering: An introduction},
	author = {{Animal Ethics}},
	timestamp = {2023-09-27 07:33:02 (GMT)}
}

@online{Anonymous2020AnonymousContributorsAnswer,
	file = {~/Google Drive/library-html/Anonymous2020AnonymousContributorsAnswer.html;~/Google Drive/library-pdf/Anonymous2020AnonymousContributorsAnswer.pdf},
	abstract = {Unattributed answers, from people whose work we
                  respect.},
	author = {{Anonymous}},
	date = {2020-03-02},
	journaltitle = {80,000 Hours},
	langid = {american},
	shorttitle = {Anonymous contributors answer},
	timestamp = {2023-09-25 18:46:02 (GMT)},
	title = {Anonymous contributors answer: What are the biggest
                  flaws of the effective altruism community?},
	url = {https://80000hours.org/2020/03/anonymous-answers-flaws-effective-altruism-community/},
	urldate = {2023-09-25}
}

@article{Anthis2019GlobalFarmedFactory,
	file = {~/Google Drive/library-html/Anthis2019GlobalFarmedFactory.html},
	journaltitle = {Sentience Institute},
	langid = {english},
	database = {Tlön},
	author = {Anthis, Kelly and Anthis, Jacy Reese},
	title = {Global farmed \& factory farmed animals estimates},
	url = {https://sentienceinstitute.org/global-animal-farming-estimates},
	abstract = {We estimate that over 90\% of farmed animals globally
                  are living in factory farms at present. This includes
                  an estimated 74\% of farmed land animals and virtually
                  all farmed fish. However, there is substantial
                  uncertainty in these figures given the land animal
                  estimates' heavy reliance on information from
                  Worldwatch Institute with unclear methodology and
                  limited data on fish farming. In total, we estimate
                  that around 31.0 billion land animals and 38.8 to
                  215.9 billion fish are being farmed globally at any
                  given time.},
	date = {2019-02-19},
	series = {Estimates},
	timestamp = {2023-09-21 18:58:34 (GMT)},
	urldate = {2023-09-21}
}

@online{Baumann2022HowAnimalMovement,
	file = {~/Google Drive/library-html/Baumann2022HowAnimalMovement.html;~/Google Drive/library-pdf/Baumann2022HowAnimalMovement.pdf},
	date = {2022-03-01},
	database = {Tlön},
	journal = {Effective Altruism Forum},
	journaltitle = {Effective Altruism Forum},
	author = {Baumann, Tobias},
	title = {How the animal movement could do even more good},
	url = {https://forum.effectivealtruism.org/posts/uz3NjAJjkjpb3mj6w/how-the-animal-movement-could-do-even-more-good},
	abstract = {Introduction The animal advocacy movement is doing a lot to
                  help animals, and has enjoyed some level of success. Yet there
                  is still substantial room...},
	langid = {english},
	timestamp = {2023-09-27 08:43:32 (GMT)},
	urldate = {2023-09-27}
}

@article{Bearne2019GiveAwayHalf,
	file = {~/Google Drive/library-pdf/Bearne2019GiveAwayHalf.pdf;~/Google Drive/library-html/Bearne2019GiveAwayHalf.html},
	author = {Bearne, Suzanne},
	title = {‘I give away half to three-quarters of my income every year’},
	url = {https://www.theguardian.com/money/2019/nov/09/i-give-away-half-to-three-quarters-of-my-income-every-year},
	abstract = {Allan Saldanha, 41, on how he achieved financial security and
                  now hopes to help save lives},
	date = {2019-11-09},
	issn = {0261-3077},
	journaltitle = {The Guardian},
	keywords = {Charitable giving, Savings, Money, Charities, Society, {UK}
                  news},
	langid = {british},
	timestamp = {2023-09-27 10:19:03 (GMT)},
	urldate = {2023-09-27}
}

@article{Bearne2022NewYearsResolutions,
	file = {~/Google Drive/library-pdf/Bearne2022NewYearsResolutions.pdf;~/Google Drive/library-html/Bearne2022NewYearsResolutions.html},
	author = {Bearne, Suzanne},
	title = {New year's resolutions: ‘I'm going to give away 10\% of my
                  income’},
	url = {https://www.theguardian.com/money/2022/jan/01/new-year-resolutions-income-money-saving-charity},
	abstract = {We talk to five people about their aims for 2022, whether it
                  is saving more or donating to charity},
	date = {2022-01-01},
	issn = {0261-3077},
	journaltitle = {The Guardian},
	keywords = {Consumer affairs, Money, Life and style, {UK} news, New year,
                  Family finances, Ethical business, Investing, Charitable
                  giving, Pensions, Online shopping, Business, Ethical money},
	langid = {british},
	shorttitle = {New year's resolutions},
	timestamp = {2023-09-27 10:22:04 (GMT)},
	urldate = {2023-09-27}
}

@book{Bentham2002DeontologyTogetherTable,
	location = {Oxford},
	editor = {Goldworth, Amnon},
	title = {Deontology: Together with a table of the springs of
                  action and the article on utilitarianism},
	isbn = {978-0-19-822609-3},
	series = {The collected works of Jeremy Bentham},
	shorttitle = {Deontology},
	number = {general eds. F. Rosen and P. Schofield},
	publisher = {Clarendon Press},
	author = {Bentham, Jeremy},
	date = 1983,
	file = {~/Google Drive/library-pdf/Bentham2002DeontologyTogetherTable.pdf;~/Google Drive/library-pdf/OxfordTogetherTABLEARTICLE.pdf}
}

@online{BibliotecaAltruismoEficaz2023SeguridadNuclear,
	translation = {EffectiveAltruismForum2023NuclearSecurityEa},
	url = {https://altruismoeficaz.net/temas/seguridad-nuclear},
	date = {2023},
	langid = {spanish},
	database = {Tlön},
	journaltitle = {Biblioteca Altruismo Eficaz},
	title = {Seguridad nuclear},
	author = {Biblioteca Altruismo Eficaz},
	timestamp = {2023-09-27 15:30:05 (GMT)}
}

@online{Briggs2022AiMessiah,
	file = {~/Google Drive/library-html/Briggs2022AiMessiah.html;~/Google Drive/library-pdf/Briggs2022AiMessiah.pdf},
	date = {2022-05-05},
	journaltitle = {Effective Altruism Forum},
	author = {Briggs, Ryan C.},
	title = {The {AI} Messiah},
	url = {https://forum.effectivealtruism.org/posts/r72wjMns9wyaAhWhc/the-ai-messiah},
	abstract = {This post will be direct because I think directness on
                  important topics is valuable. I sincerely hope that my
                  directness is not read as mockery or di...},
	langid = {english},
	timestamp = {2023-09-26 14:45:12 (GMT)},
	urldate = {2023-09-26}
}

@online{Buck2020HowGoodIs,
	file = {~/Google Drive/library-html/Buck2020HowGoodIs.html;~/Google Drive/library-pdf/Buck2020HowGoodIs.pdf},
	date = {2020-07-21},
	journaltitle = {LessWrong},
	author = {{Buck}},
	title = {How good is humanity at coordination?},
	url = {https://www.lesswrong.com/posts/y3jDSoTTdBD9Nj3Gx/how-good-is-humanity-at-coordination},
	abstract = {When {EAs} look at the history of nuclear weapons,
                  their reactions tend to fall into two camps. ...},
	langid = {english},
	timestamp = {2023-09-26 14:35:43 (GMT)},
	urldate = {2023-09-26}
}

@online{Careers2021IsSkilledVolunteering,
	file = {~/Google Drive/library-html/Careers2021IsSkilledVolunteering.html;~/Google Drive/library-pdf/Careers2021IsSkilledVolunteering.pdf},
	journaltitle = {Animal Advocacy Careers},
	author = {Animal Advocacy Careers},
	date = {2021},
	timestamp = {2023-09-27 10:55:38 (GMT)},
	title = {Is skilled volunteering for you?},
	url = {https://web.archive.org/web/20210616134616/https://www.animaladvocacycareers.org/sv-for-you},
	urldate = {2023-09-27}
}

@online{Chappell2023ImperfectlyRight,
	file = {~/Google Drive/library-html/Chappell2023ImperfectlyRight.html;~/Google Drive/library-pdf/Chappell2023ImperfectlyRight.pdf},
	date = {2007-09-12},
	journaltitle = {Philosophy, et cetera},
	abstract = {Maximizing consequentialists claim that the right action is
                  that which maximizes the good (e.g. aggregate human welfare).
                  So it's impermiss...},
	author = {Chappell, Richard Yetter},
	timestamp = {2023-09-27 11:01:21 (GMT)},
	title = {Imperfectly right},
	url = {https://www.philosophyetc.net/2007/09/imperfectly-right.html},
	urldate = {2023-09-27}
}

@online{Clarke2021DistinguishingAiTakeover,
	file = {~/Google Drive/library-pdf/Clarke2021DistinguishingAiTakeover.pdf;~/Google Drive/library-html/Clarke2021DistinguishingAiTakeover.html},
	date = {2021-09-08},
	journaltitle = {{AI} Alignment Forum},
	author = {Clarke, Sam and Martin, Sammy},
	title = {Distinguishing {AI} takeover scenarios},
	url = {https://www.alignmentforum.org/posts/qYzqDtoQaZ3eDDyxa/distinguishing-ai-takeover-scenarios},
	abstract = {{TLDR}: see the summary table. \textbullet In the last few
                  years, people have proposed various {AI} takeover scenarios.
                  We think this type of scenario building is...},
	langid = {english},
	timestamp = {2023-09-28 10:27:20 (GMT)},
	urldate = {2023-09-28}
}

@online{Cooper2022TaleOf2,
	file = {~/Google Drive/library-html/Cooper2022TaleOf2.html;~/Google Drive/library-pdf/Cooper2022TaleOf2.pdf},
	date = {2022-05-01},
	database = {Tlön},
	journaltitle = {Effective Altruism Forum},
	author = {Cooper, Sasha},
	title = {A tale of 2.5 orthogonality theses},
	url = {https://forum.effectivealtruism.org/posts/kCAcrjvXDt2evMpBz/a-tale-of-2-5-orthogonality-theses},
	abstract = {tl;dr-tl;dr You can summarise this whole post as 'we
                  shouldn't confuse theoretical possibility with
                  likelihood, let alone with theoretical certainty'...},
	langid = {english},
	timestamp = {2023-09-26 14:39:09 (GMT)},
	urldate = {2023-09-26}
}

@online{Cotra2016IntroductionToEa,
	date = {2016-05-24},
	journaltitle = {{EAGx} Berkeley},
	author = {Cotra, Ajeya},
	abstract = {Ajeya Cotra introduces the core principles of
                  effective altruism.See a transcript of this talk:
                  https://forum.effectivealtruism.org/s/{YCa}8BRQoxKbmf5CJb/p/5Eq...},
	langid = {spanish},
	timestamp = {2023-09-26 06:59:13 (GMT)},
	title = {Introduction to {EA}},
	url = {https://www.youtube.com/watch?v=48VAQtGmfWY},
	urldate = {2023-09-26}
}

@online{Cotra2020ForecastingTaiWitha,
	file = {~/Google Drive/library-pdf/Cotra2020ForecastingTaiWitha.pdf},
	url = {https://docs.google.com/document/d/1IJ6Sr-gPeXdSJugFulwIpvavc0atjHGM82QjIfUSBGQ/edit#heading=h.r808lkmw6i0l},
	date = {2020-07},
	langid = {english},
	database = {Tlön},
	title = {Forecasting TAI with biological anchors. Part 1: Overview, conceptual foundations, and runtime computation},
	author = {Cotra, Ajeya},
	timestamp = {2023-09-28 11:45:09 (GMT)}
}

@online{Cotton-Barratt2014StrategicConsiderationsAbout,
	abstract = {{FHI} is a multidisciplinary research institute at Oxford
                  University studying big picture questions for human
                  civilization.},
	author = {Cotton-Barratt, Owen and Ord, Toby},
	date = {2014-08-12},
	journaltitle = {Future of Humanity Institute},
	langid = {british},
	timestamp = {2023-10-06 06:03:56 (GMT)},
	title = {Strategic considerations about different speeds of AI takeoff},
	url = {http://www.fhi.ox.ac.uk/},
	urldate = {2023-10-06}
}

@online{Cotton-Barratt2020BlueprintsLensesFor,
	file = {~/Google Drive/library-html/Cotton-Barratt2020BlueprintsLensesFor.html;~/Google Drive/library-pdf/Cotton-Barratt2020BlueprintsLensesFor.pdf},
	date = {2020-12-21},
	database = {Tlön},
	journaltitle = {Effective Altruism Forum},
	author = {Cotton-Barratt, Owen},
	title = {Blueprints (\& lenses) for longtermist
                  decision-making},
	url = {https://forum.effectivealtruism.org/posts/NdSoipXQhdzozLqW4/blueprints-and-lenses-for-longtermist-decision-making},
	abstract = {tl;dr: This is a post about ontologies. I think that
                  working out what to do is always confusing, but that
                  longtermism poses large extra challenges. I...},
	langid = {english},
	timestamp = {2023-09-26 09:12:20 (GMT)},
	urldate = {2023-09-26}
}

@Article{Cremer2021DemocratisingRiskIn,
	file = {~/Google Drive/library-pdf/Cremer2021DemocratisingRiskIn.pdf},
	journaltitle = {{SSRN} Electronic Journal},
	database = {Tlön},
	abstract = {Studying potential global catastrophes is vital. The high
                  stakes of existential risk studies ({ERS}) necessitate serious
                  scrutiny and self-reflection. We argue that existing
                  approaches to studying existential risk are not yet fit for
                  purpose, and perhaps even run the risk of increasing harm. We
                  highlight general challenges in {ERS}: accommodating value
                  pluralism, crafting precise definitions, developing
                  comprehensive tools for risk assessment, dealing with
                  uncertainty, and accounting for the dangers associated with
                  taking exceptional actions to mitigate or prevent
                  catastrophes. The most influential framework for {ERS}, the
                  "techno-utopian approach" ({TUA}), struggles with these issues
                  and has a unique set of additional problems: it unnecessarily
                  combines the study of longtermism and longtermist ethics with
                  the study of extinction, relies on a non-representative moral
                  worldview, uses ambiguous and inadequate definitions, fails to
                  incorporate insights from risk assessment in relevant fields,
                  chooses arbitrary categorisations of risk, and advocates for
                  dangerous mitigation strategies. Its moral and empirical
                  assumptions might be particularly vulnerable to securitisation
                  and misuse. We suggest several key improvements: separating
                  the study of extinction ethics (ethical implications of
                  extinction) and existential ethics (the ethical implications
                  of different societal forms), from the analysis of human
                  extinction and global catastrophe; drawing on the latest
                  developments in risk assessment literature; diversifying the
                  field, and; democratising its policy recommendations.},
	author = {Cremer, Carla Zoe and Kemp, Luke},
	date = {2021-12-28},
	keywords = {existential risk, extinction, democracy, global catastrophic
                  risk, longtermism, transhumanism, utilitarianism},
	langid = {english},
	location = {Rochester, {NY}},
	shorttitle = {Democratising Risk},
	timestamp = {2023-09-27 14:47:59 (GMT)},
	title = {Democratising Risk: In Search of a Methodology to Study
                  Existential Risk},
	url = {https://papers.ssrn.com/abstract=3995225},
	urldate = {2023-09-27}
}

@online{Critch2020SomeAiResearch,
	file = {~/Google Drive/library-pdf/Critch2020SomeAiResearch.pdf},
	date = {2020-11-19},
	journaltitle = {LessWrong},
	author = {Critch, Andrew },
	title = {Some {AI} research areas and their relevance to existential
                  safety},
	url = {https://www.lesswrong.com/posts/hvGoYXi2kgnS3vxqb/some-ai-research-areas-and-their-relevance-to-existential-1},
	abstract = {Followed by: What Multipolar Failure Looks Like, and Robust
                  Agent-Agnostic Processes ({RAAPs}), which provides examples of
                  multi-stakeholder/multi-agen...},
	langid = {english},
	timestamp = {2023-09-28 12:20:09 (GMT)},
	urldate = {2023-09-28}
}

@online{Dalton2022CommentToEa,
	file = {~/Google Drive/library-html/Dalton2022CommentToEa.html;~/Google Drive/library-pdf/Dalton2022CommentToEa.pdf},
	date = {2022-05-04},
	journaltitle = {Effective Altruism Forum},
	author = {Dalton, Max},
	abstract = {Comment by {MaxDalton} - (Not a response to your whole
                  comment, hope that's {OK}.) I agree that there should
                  be some critiques of longtermism or working on X risk
                  in the curriculum. We're working on an update at the
                  moment. Does anyone have thoughts on what the best
                  critiques are? Some of my current thoughts: - Why I am
                  probably not a longtermist - This post arguing that
                  it's not clear if X risk reduction is positive - On
                  infinite ethics (and Ajeya's crazy train metaphor)},
	langid = {english},
	timestamp = {2023-09-26 09:44:54 (GMT)},
	title = {Comment to '{EA} is more than longtermism'},
	url = {https://forum.effectivealtruism.org/posts/LRmEezoeeqGhkWm2p/ea-is-more-than-longtermism},
	urldate = {2023-09-26}
}

@online{Danzig2018TechnologyRouletteManaging,
	file = {~/Google Drive/library-pdf/Danzig2018TechnologyRouletteManaging.pdf;~/Google Drive/library-html/Danzig2018TechnologyRouletteManaging.html},
	journaltitle = {Center for a New American Security},
	date = {2018-05-30},
	author = {Danzig, Richard},
	abstract = {Developing strong, pragmatic and principled national
                  security and defense policies.},
	langid = {english},
	timestamp = {2023-09-26 14:20:43 (GMT)},
	title = {Technology roulette: Managing loss of control as many militaries pursue technological superiority},
	url = {https://www.cnas.org/publications/reports/technology-roulette},
	urldate = {2023-09-26}
}

@online{Davidson2021ReportSemiInformative,
	file = {~/Google Drive/library-pdf/Davidson2021ReportSemiInformative.pdf;~/Google Drive/library-html/Davidson2021ReportSemiInformative.html},
	date = {2021-03-25},
	author = {Davidson, Tom},
	abstract = {One of Open Phil's major focus areas is technical research and
                  policy work aimed at reducing potential risks from advanced
                  {AI}. To inform this work, I have written a report developing
                  one approach to forecasting when artificial general
                  intelligence ({AGI}) will be developed. By {AGI}, I mean
                  computer program(s) that can perform virtually any cognitive
                  task as well [...]},
	journaltitle = {Open Philanthropy},
	langid = {english},
	timestamp = {2023-09-28 11:53:35 (GMT)},
	title = {Report on semi-informative priors},
	url = {https://www.openphilanthropy.org/research/report-on-semi-informative-priors/},
	urldate = {2023-09-28}
}

@Article{Deeks2018MachineLearningArtificial,
	file = {~/Google Drive/library-pdf/Deeks2018MachineLearningArtificial.pdf},
	number = {no. 1},
	volume = {10},
	journal = {Journal of National Security Law & Policy},
	journaltitle = {{SSRN} Electronic Journal},
	abstract = {Big data technology and machine learning techniques play a
                  growing role across all areas of modern society. Machine
                  learning provides the ability to predict likely future
                  outcomes, to calculate risks between competing choices, to
                  make sense of vast amounts of data at speed, and to draw
                  insights from data that would be otherwise invisible to human
                  analysts. Despite the significant attention given to machine
                  learning generally in academic writing and public discourse,
                  however, there has been little analysis of how it may affect
                  war-making decisions, and even less analysis from an
                  international law perspective. The advantages that flow from
                  machine learning algorithms mean that it is inevitable that
                  governments will begin to employ them to help officials decide
                  whether, when, and how to resort to force internationally. In
                  some cases, these algorithms may lead to more accurate and
                  defensible uses of force than we see today; in other cases,
                  states may intentionally abuse these algorithms to engage in
                  acts of aggression, or unintentionally misuse algorithms in
                  ways that lead them to make inferior decisions relating to
                  force.This essay's goal is to draw attention to current and
                  near future developments that may have profound implications
                  for international law, and to present a blueprint for the
                  necessary analysis. More specifically, this article seeks to
                  identify the most likely ways in which states will begin to
                  employ machine learning algorithms to guide their decisions
                  about when and how to use force, to identify legal challenges
                  raised by use of force-related algorithms, and to recommend
                  prophylactic measures for states as they begin to employ these
                  tools.},
	author = {Deeks, Ashley and Lubell, Noam and Murray, Daragh},
	date = {2018-11-16},
	keywords = {machine learning, artificial intelligence, united nations
                  charter, self-defense, use of force, cyber, international law,
                  military, proportionality},
	langid = {english},
	timestamp = {2023-09-28 12:34:25 (GMT)},
	title = {Machine learning, artificial intelligence, and the use of
                  force by states},
	url = {https://jnslp.com/2018/11/16/machine-learning-artificial-intelligence-and-the-use-of-force-by-states/},
	urldate = {2023-09-28}
}

@online{Directedevolution2019ReviewOfItn,
	file = {~/Google Drive/library-html/Directedevolution2019ReviewOfItn.html;~/Google Drive/library-pdf/Directedevolution2019ReviewOfItn.pdf},
	date = {2019-10-09},
	journaltitle = {Effective Altruism Forum},
	author = {{DirectedEvolution}},
	title = {Review of {ITN} Critiques},
	url = {https://forum.effectivealtruism.org/posts/MtCAsPMftvJqRBYzr/review-of-itn-critiques},
	abstract = {After 5000 years of delay, I have finally taken 10
                  seconds to dump this post in a community editable
                  Google Doc. Thanks to Vaidehi for her important,...},
	langid = {english},
	timestamp = {2023-09-25 19:51:49 (GMT)},
	urldate = {2023-09-25}
}

@online{Directedevolution2023DontBeBycatch,
	file = {~/Google Drive/library-pdf/Directedevolution2023DontBeBycatch.pdf;~/Google Drive/library-html/Directedevolution2023DontBeBycatch.html},
	date = {2021-03-10},
	journaltitle = {Effective Altruism Forum},
	author = {{DirectedEvolution}},
	title = {Don't be bycatch},
	url = {https://forum.effectivealtruism.org/posts/2BEecjksNZNHQmdyM/don-t-be-bycatch},
	abstract = {It's a common story. Someone who's passionate about {EA}
                  principles, but has little in the way of resources, tries and
                  fails to do {EA} things. They writ...},
	langid = {english},
	timestamp = {2023-09-27 11:41:31 (GMT)},
	urldate = {2023-09-27}
}

@online{Duda2016FactoryFarming,
	file = {~/Google Drive/library-html/Duda2016FactoryFarming.html;~/Google Drive/library-pdf/Duda2016FactoryFarming.pdf},
	eventdate = {2022-02},
	date = {2016-06},
	author = {Duda, Roman},
	abstract = {50 billion animals are raised and slaughtered in factory farms
                  around the world each year, around 10 billion of these in the
                  {USA}. Most experience extreme levels of suffering over the
                  course of their lives due to their poor treatment. Relatively
                  small improvements to their treatment could substantially
                  improve their welfare.},
	journaltitle = {80,000 Hours},
	langid = {american},
	timestamp = {2023-09-27 07:05:57 (GMT)},
	title = {Factory farming},
	url = {https://80000hours.org/problem-profiles/factory-farming/},
	urldate = {2023-09-27}
}

@online{EAApplicant2023AfterOneYear,
	file = {~/Google Drive/library-html/EAApplicant2023AfterOneYear.html;~/Google Drive/library-pdf/EAApplicant2023AfterOneYear.pdf},
	date = {2019-02-26},
	journal = {Effective Altruism Forum},
	database = {Tlön},
	journaltitle = {Effective Altruism Forum},
	author = {{EA Applicant}},
	title = {After one year of applying for {ea} jobs: it is really, really
                  hard to get hired by an {ea} organisation},
	url = {https://forum.effectivealtruism.org/posts/jmbP9rwXncfa32seH/after-one-year-of-applying-for-ea-jobs-it-is-really-really},
	abstract = {(I am writing this post under a pseudonym because I don't want
                  potential future non-{EA} employers to find this with a quick
                  google search. Initially m...},
	langid = {english},
	shorttitle = {After one year of applying for {EA} jobs},
	timestamp = {2023-09-27 11:30:25 (GMT)},
	urldate = {2023-09-27}
}

@online{EffectiveAltruismForum2023NuclearSecurityEa,
	file = {~/Google Drive/library-pdf/EffectiveAltruismForum2023NuclearSecurityEa.pdf;~/Google Drive/library-html/EffectiveAltruismForum2023NuclearSecurityEa.html},
	eventdate = {2022-07-14},
	date = {2022-03-30},
	database = {Tlön},
	journaltitle = {Effective Altruism Wiki},
	author = {Effective Altruism Forum},
	abstract = {Nuclear security is the set of procedures, practices or other
                  measures used to manage risks from nuclear weapons and other
                  nuclear materials. Evaluation 80,000 Hours rates nuclear
                  security a "second-highest priority area": an unusually
                  pressing global problem ranked slightly below their four
                  highest priority areas.[1] Recommendations In The Precipice:
                  Existential Risk and the Future of Humanity, Toby Ord offers
                  several policy and research recommendations for handling risks
                  from nuclear weapons:[2] * Restart the Intermediate-Range
                  Nuclear Forces Treaty ({INF}). * Take {US} {ICBMs} off
                  hair-trigger alert (officially called Launch on Warning). *
                  Increase the capacity of the International Atomic Energy
                  Agency ({IAEA}) to verify nations are complying with
                  safeguards agreements. * Work on resolving the key
                  uncertainties in nuclear winter modeling. * Characterize the
                  remaining uncertainties then use Monte Carlo techniques to
                  show the distribution of outcome possibilities, with a special
                  focus on the worst-case possibilities compatible with our
                  current understanding. * Investigate which parts of the world
                  appear most robust to the effects of nuclear winter and how
                  likely civilization is to continue there. Further reading
                  {McIntyre}, Peter (2016) Nuclear security, 80,000 Hours,
                  April. Open Philanthropy (2013) Nuclear security, Open
                  Philanthropy, July 18. Open Philanthropy (2015) Nuclear
                  weapons policy, Open Philanthropy, September. Related entries
                  nuclear disarmament movement {\textbar} nuclear warfare
                  {\textbar} nuclear winter {\textbar} weapons of mass
                  destruction 1. {\textasciicircum} 80,000 Hours (2021) Our
                  current list of the most important world problems, 80,000
                  Hours. 2. {\textasciicircum} Ord, Toby (2020) The Precipice:
                  Existential Risk and the Future of Humanity, London:
                  Bloomsbury Publishing, p. 278},
	langid = {english},
	timestamp = {2023-09-27 15:26:03 (GMT)},
	title = {Nuclear security},
	url = {https://forum.effectivealtruism.org/topics/nuclear-security},
	urldate = {2023-09-27}
}

@online{EffectiveAltruismForum2023OpportunitiesToTake,
	file = {~/Google Drive/library-html/EffectiveAltruismForum2023OpportunitiesToTake.html;~/Google Drive/library-pdf/EffectiveAltruismForum2023OpportunitiesToTake.pdf},
	date = {2023},
	journaltitle = {Effective Altruism Wiki},
	database = {Tlön},
	author = {Effective Altruism Forum},
	abstract = {The {EA} Forum is mostly about discussing ideas. But
                  ultimately, the reason for discussing those ideas is to take
                  actions that improve the world. Here are some ways to get
                  involved with effective altruism via the {EA} Forum or
                  elsewhere: * Jobs: Find job listings on the Forum, apply for
                  high-impact career advising, or visit the job boards on 80,000
                  Hours, {EA} Work Club, and Animal Advocacy Careers. *
                  Volunteering: Find volunteering opportunities using the
                  volunteering tag, by visiting {EA} Work Club, by looking at
                  the {EA} Opportunities Board, or by directly reaching out to
                  high-impact non-profit organisations. * Donating: Support
                  high-impact work by donating to high impact causes and taking
                  an effective giving pledge. * Internships and fellowships:
                  Find internships and fellowships on the Forum or visit the
                  {EA} Opportunities Board, which lists everything short of
                  full-time positions. * Startup ideas: Start a high impact
                  non-profit by applying to Charity Entrepreneurship, finding a
                  startup idea from this list of ideas for valuable new
                  non-profit startups (of which there might already be funding
                  available if you want to try one of these ideas). * Research
                  ideas: Find a research topic by looking at the central
                  directory for research questions, or use Effective Thesis to
                  help find a high-impact thesis topic. * Funding opportunities:
                  Seek funding for a high-impact project by posting using the
                  funding opportunity tag, apply to {EA} Funds, or find other
                  funding opportunities. (This page is an experiment: please
                  give us feedback on it!) Use the take action tag for posts
                  that offer readers a way to get involved with useful {EA}
                  work. It is a call to action, an option to volunteer, or
                  anything else. Ideally, this tag will only be applied to posts
                  that discuss an active opportunity. If you see a post that has
                  this tag, but the time to get involved has already passed,
                  please downvote this tag's karma so that the post will no
                  longer appear on the "get involved" tag page and upvot},
	langid = {english},
	timestamp = {2023-09-27 12:04:09 (GMT)},
	title = {Opportunities to take action},
	url = {https://forum.effectivealtruism.org/topics/opportunities-to-take-action},
	urldate = {2023-09-27}
}

@online{Englander2020NewArticleFrom,
	file = {~/Google Drive/library-html/Englander2020NewArticleFrom.html;~/Google Drive/library-pdf/Englander2020NewArticleFrom.pdf},
	date = {2020-02-25},
	database = {Tlön},
	journaltitle = {Effective Altruism Forum},
	author = {Englander, Aryeh},
	title = {New article from Oren Etzioni},
	url = {https://forum.effectivealtruism.org/posts/7x2BokkkemjnXD9B6/new-article-from-oren-etzioni},
	abstract = {This just appeared in this week's {MIT} Technology
                  Review: Oren Etzioni, "How to know if {AI} is about to
                  destroy civilization." Etzioni is a noted skept...},
	langid = {english},
	timestamp = {2023-09-26 14:43:38 (GMT)},
	urldate = {2023-09-26}
}

@online{Esvelt2017OpenUntilDangerous,
	url = {https://www.youtube.com/watch?v=N1EyMRGrmho},
	date = {2017-06-03},
	journaltitle = {{EAG} Boston},
	title = {Open until dangerous: gene drive and the case for reforming research},
	author = {Esvelt, Kevin},
	timestamp = {2023-09-27 15:06:15 (GMT)}
}

@online{Fabienne2023CanHaveImpact,
	file = {~/Google Drive/library-html/Fabienne2023CanHaveImpact.html;~/Google Drive/library-pdf/Fabienne2023CanHaveImpact.pdf},
	date = {2021-01-03},
	journaltitle = {Effective Altruism Forum},
	author = {Sandkühler, Julia Fabienne},
	title = {Can I Have Impact If I'm Average?},
	url = {https://forum.effectivealtruism.org/posts/ptFkbqksdPRNyzNBB/can-i-have-impact-if-i-m-average},
	abstract = {A friend of mine who is into {EA} said a few days ago that he
                  thinks most people cannot have an impact, because to have an
                  impact you need to be among...},
	langid = {english},
	timestamp = {2023-09-27 11:43:15 (GMT)},
	urldate = {2023-09-27}
}

@book{Fink2013FiveDaysAt,
	langid = {english},
	author = {Fink, Sheri},
	database = {Tlön},
	title = {Five days at Memorial: life and death in a
                  storm-ravaged hospital},
	publisher = {Crown Publishers},
	date = {2013},
	edition = {First edition},
	isbn = {9780307718969},
	keywords = {Memorial Medical Center (New Orleans, La.), Disaster
                  hospitals, Louisiana New Orleans, Case studies,
                  Disaster medicine, Louisiana New Orleans, Case
                  studies, Health facilities, Administration, Louisiana,
                  Case studies, Forensic pathology, Louisiana New
                  Orleans, Case studies, Hurricane Katrina, 2005},
	location = {New York},
	pagetotal = {558},
	shorttitle = {Five days at Memorial},
	timestamp = {2023-09-22 15:33:54 (GMT)}
}

@article{Fodor2019EffectiveAltruismIs,
	file = {~/Google Drive/library-html/Fodor2019EffectiveAltruismIs.html;~/Google Drive/library-pdf/Fodor2019EffectiveAltruismIs.pdf},
	date = {2019-06-28},
	journaltitle = {Effective Altruism Forum},
	author = {Fodor, James},
	title = {Effective altruism is an ideology, not (just) a
                  question},
	url = {https://forum.effectivealtruism.org/posts/uxFvTnzSgw8uakNBp/effective-altruism-is-an-ideology-not-just-a-question},
	abstract = {Introduction In a widely-cited article on the {EA}
                  forum, Helen Toner argues that effective altruism is a
                  question, not an ideology. Here is her core a...},
	langid = {english},
	timestamp = {2023-09-25 17:36:09 (GMT)},
	urldate = {2023-09-25}
}

@online{Fodor2020CriticalReviewOf,
	file = {~/Google Drive/library-html/Fodor2020CriticalReviewOf.html;~/Google Drive/library-pdf/Fodor2020CriticalReviewOf.pdf},
	date = {2020-05-11},
	journaltitle = {Centre for Effective Altruism},
	author = {Fodor, James},
	title = {Critical review of 'The Precipice': a reassessment of the
                  risks of {AI} and pandemics},
	url = {https://forum.effectivealtruism.org/posts/2sMR7n32FSvLCoJLQ/critical-review-of-the-precipice-a-reassessment-of-the-risks},
	abstract = {Introduction In this essay I will present a critical response
                  to Toby Ord's recent book The Precipice (page numbers refer to
                  the soft cover version o...},
	langid = {english},
	shorttitle = {Critical Review of 'The Precipice'},
	timestamp = {2023-09-27 14:53:49 (GMT)},
	urldate = {2023-09-27}
}

@online{Friedrich2019CreatingNewAgricultural,
	url = {https://www.youtube.com/watch?v=Ahm5d6OIfv0},
	date = {2019},
	journaltitle = {EAGlobal},
	title = {Creating a new agricultural revolution},
	author = {Friedrich, Bruce},
	timestamp = {2023-09-27 07:29:15 (GMT)}
}

@online{Gabriel2015ResponseToEffective,
	file = {~/Google Drive/library-html/Gabriel2015ResponseToEffective.html;~/Google Drive/library-pdf/Gabriel2015ResponseToEffective.pdf},
	date = {2015-07-01},
	database = {Tlön},
	author = {Gabriel, Iason},
	abstract = {Effective altruism encourages people to do "the most
                  good they can," typically by contributing a portion of
                  their income to the},
	journaltitle = {Boston Review},
	langid = {american},
	timestamp = {2023-09-25 17:31:15 (GMT)},
	title = {Response to effective altruism},
	url = {https://www.bostonreview.net/forum_response/response-iason-gabriel/},
	urldate = {2023-09-25}
}

@online{GiveWell2023MostCharitiesEvidence,
	date = {2023},
	journaltitle = {GiveWell},
	author = {GiveWell},
	abstract = {Soon after starting {GiveWell} in 2007, we guessed
                  that many charities didn't have the type of evidence
                  we were interested in: High-quality, rigorous evidence
                  demonstrating that the intervention they are
                  implementing works. But we assumed that the most
                  effective charities would.},
	langid = {english},
	database = {Tlön},
	timestamp = {2023-09-23 16:02:39 (GMT)},
	title = {Most Charities' Evidence},
	url = {https://www.givewell.org/giving101/Most-Charities-Evidence},
	urldate = {2023-09-23}
}

@online{Givewell2019YourDollarGoesa,
	date = {2019},
	journaltitle = {GiveWell},
	database = {Tlön},
	author = {GiveWell},
	abstract = {Charities vary in effectiveness, so your choice of
                  charity has a huge effect on the impact you ultimately
                  achieve with your donation.},
	langid = {english},
	timestamp = {2023-09-21 19:13:48 (GMT)},
	title = {Your dollar goes further when you fund the right
                  program},
	url = {https://www.givewell.org/giving101/Funding-the-Right-Program},
	urldate = {2023-09-21}
}

@online{Givewell2021WhyIsIt,
	file = {~/Google Drive/library-pdf/Givewell2021WhyIsIt.pdf;~/Google Drive/library-html/Givewell2021WhyIsIt.html},
	date = {2021-12},
	journaltitle = {GiveWell},
	author = {GiveWell},
	database = {Tlön},
	abstract = {This page describes why estimated cost to save a life
                  is higher than we might expect, even for inexpensive
                  interventions.},
	langid = {english},
	shorttitle = {Why Is It So Expensive to Save Lives?},
	timestamp = {2023-09-23 15:58:24 (GMT)},
	title = {Why is it so expensive to save lives?},
	url = {https://www.givewell.org/cost-to-save-a-life},
	urldate = {2023-09-23}
}

@online{Givewell2022OurCriteria,
	file = {~/Google Drive/library-html/Givewell2022OurCriteria.html;~/Google Drive/library-pdf/Givewell2022OurCriteria.pdf},
	date = {2022-09},
	journaltitle = {GiveWell},
	author = {GiveWell},
	abstract = {We aim to help donors do as much good as possible. The
                  criteria for our top charities: evidence, cost-effectiveness,
                  room for more funding, and transparency.},
	langid = {english},
	timestamp = {2023-09-27 18:46:24 (GMT)},
	title = {Our criteria},
	url = {https://www.givewell.org/how-we-work/criteria},
	urldate = {2023-09-27}
}

@online{Givewell2022ProcessForIdentifying,
	file = {~/Google Drive/library-pdf/Givewell2022ProcessForIdentifying.pdf;~/Google Drive/library-html/Givewell2022ProcessForIdentifying.html},
	date = {2022-12},
	journaltitle = {GiveWell},
	author = {GiveWell},
	abstract = {This page describes the process we use to identify our top
                  charities, following our aim of finding the most outstanding
                  charities possible.},
	langid = {english},
	timestamp = {2023-09-27 18:48:42 (GMT)},
	title = {Process for Identifying Top Charities {\textbar} {GiveWell}},
	url = {https://www.givewell.org/how-we-work/process},
	urldate = {2023-09-27}
}

@online{GivingWhatWeCan2019HowRichAm,
	date = {2019},
	journaltitle = {Giving What We Can},
	database = {Tlön},
	author = {{Giving What We Can}},
	abstract = {Calculate how rich you are compared to the rest of the
                  world. Are you in the top global income percentile?
                  Does your household income make you wealthy?},
	langid = {english},
	shorttitle = {How Rich Am I?},
	timestamp = {2023-09-20 16:38:00 (GMT)},
	title = {How Rich Am I? World Income Percentile Calculator:
                  Global Rich List},
	url = {https://howrichami.givingwhatwecan.org/how-rich-am-i?income=29000&countryCode=USA&household%5Badults%5D=1&household%5Bchildren%5D=0},
	urldate = {2023-09-20}
}

@online{GivingWhatWeCan2022VascoGrilo,
	file = {~/Google Drive/library-pdf/GivingWhatWeCan2022VascoGrilo.pdf;~/Google Drive/library-html/GivingWhatWeCan2022VascoGrilo.html},
	date = {2022-06-13},
	database = {Tlön},
	journaltitle = {The Giving What We Can blog},
	author = {{Giving What We Can}},
	langid = {english},
	timestamp = {2023-09-23 16:51:22 (GMT)},
	title = {Vasco Grilo},
	url = {https://www.givingwhatwecan.org/people/vasco-grilo},
	urldate = {2023-09-23}
}

@online{GivingWhatWeCan2023CaseStudiesPeople,
	file = {~/Google Drive/library-pdf/GivingWhatWeCan2023CaseStudiesPeople.pdf;~/Google Drive/library-html/GivingWhatWeCan2023CaseStudiesPeople.html},
	date = {2023},
	journaltitle = {Giving What We Can},
	database = {Tlön},
	author = {Giving What We Can},
	abstract = {Have you shared your story? Let us know!},
	langid = {english},
	shorttitle = {Case Studies},
	timestamp = {2023-09-27 10:47:50 (GMT)},
	title = {Case studies: people who pledge to give},
	url = {https://www.givingwhatwecan.org/case-studies-people-who-pledge-to-give},
	urldate = {2023-09-27}
}

@online{GivingWhatWeCan2023EffectiveGivingGuide,
	file = {~/Google Drive/library-pdf/Can2023EffectiveGivingGuide.pdf},
	url = {https://www.givingwhatwecan.org/giving-guide},
	langid = {english},
	database = {Tlön},
	date = {2023},
	journaltitle = {Giving What We Can},
	title = {Effective Giving Guide 2023},
	author = {{Giving What We Can}},
	timestamp = {2023-09-27 10:08:38 (GMT)}
}

@online{GivingWhatWeCan2023WhyMakeGiving,
	date = {2023},
	journaltitle = {Giving What We Can},
	database = {Tlön},
	author = {{Giving What We Can}},
	abstract = {If you're already on board with donating to effective
                  charities, you might ask: why take a public giving pledge?
                  Especially when commitments can be daunting and there's
                  something a little uncomfortable about donating in public - we
                  might think it's easier to remain uncommitted and anonymous.},
	langid = {english},
	timestamp = {2023-09-27 10:12:28 (GMT)},
	title = {Why make a giving pledge when you could just donate?},
	url = {https://www.givingwhatwecan.org/pledge/why-pledge},
	urldate = {2023-09-27}
}

@article{Grace2018ViewpointWhenWill,
	file = {~/Google Drive/library-pdf/Grace2018ViewpointWhenWill.pdf},
	author = {Grace, Katja and Salvatier, John and Dafoe, Allan and Zhang,
                  Baobao and Evans, Owain},
	title = {Viewpoint: When will {AI} exceed human performance? evidence
                  from {AI} experts},
	volume = {62},
	pages = {729--754},
	doi = {10.1613/jair.1.11222},
	url = {http://jair.org/index.php/jair/article/view/11222},
	abstract = {Advances in artificial intelligence ({AI}) will transform
                  modern life by reshaping transportation, health, science,
                  finance, and the military. To adapt public policy, we need to
                  better anticipate these advances. Here we report the results
                  from a large survey of machine learning researchers on their
                  beliefs about progress in {AI}. Researchers predict {AI} will
                  outperform humans in many activities in the next ten years,
                  such as translating languages (by 2024), writing high-school
                  essays (by 2026), driving a truck (by 2027), working in retail
                  (by 2031), writing a bestselling book (by 2049), and working
                  as a surgeon (by 2053). Researchers believe there is a 50\%
                  chance of {AI} outperforming humans in all tasks in 45 years
                  and of automating all human jobs in 120 years, with Asian
                  respondents expecting these dates much sooner than North
                  Americans. These results will inform discussion amongst
                  researchers and policymakers about anticipating and managing
                  trends in {AI}.   This article is part of the
                  special track on {AI} and Society.},
	date = {2018-07-31},
	issn = {1076-9757},
	journaltitle = {Journal of Artificial Intelligence Research},
	shortjournal = {jair},
	shorttitle = {Viewpoint},
	timestamp = {2023-09-28 11:58:03 (GMT)},
	urldate = {2023-09-28}
}

@online{Graham2012BlackSwanFarming,
	file = {~/Google Drive/library-html/Graham2012BlackSwanFarming.html;~/Google Drive/library-pdf/Graham2012BlackSwanFarming.pdf},
	date = {2012-09},
	journaltitle = {Paul Graham's Blog},
	author = {Graham, Paul},
	timestamp = {2023-09-29 09:45:24 (GMT)},
	title = {Black Swan Farming},
	url = {http://paulgraham.com/swan.html},
	urldate = {2023-09-29}
}

@online{Greaves2020EvidenceCluelessnessAnd,
	date = {2020-10-25},
	journaltitle = {{EA} Student Summit 2020},
	author = {Greaves, Hilary},
	abstract = {Principles of effective altruism counsel paying close
                  attention to the evidence, to find the most
                  cost-effective opportunities to do good. But evidence
                  cover...},
	langid = {spanish},
	timestamp = {2023-09-25 19:39:34 (GMT)},
	title = {Evidence, cluelessness and the long term},
	url = {https://www.youtube.com/watch?v=fySZIYi2goY},
	urldate = {2023-09-25}
}

@book{Griffin1976QuestionOfAnimal,
	author = {Griffin, Donald R.},
	title = {The question of animal awareness: evolutionary continuity of
                  mental experience},
	publisher = {Rockefeller University Press},
	date = {1976},
	isbn = {9780874700206},
	keywords = {Animal intelligence, Animal communication, Animal behavior},
	location = {New York},
	pagetotal = {135},
	shorttitle = {The question of animal awareness},
	timestamp = {2023-10-05 18:58:25 (GMT)}
}

@online{Hall2021UsingExportControls,
	file = {~/Google Drive/library-html/Hall2021UsingExportControls.html;~/Google Drive/library-pdf/Hall2021UsingExportControls.pdf},
	date = {2021-12-23},
	journaltitle = {Effective Altruism Forum},
	author = {Hall, Cate},
	title = {Using export controls to reduce biorisk},
	url = {https://forum.effectivealtruism.org/posts/4KaEXujJam8PawqEp/using-export-controls-to-reduce-biorisk},
	abstract = {I'm an {EA} lawyer with an interest in legal routes to reduce
                  biorisk. In the course of some research, I've come to think
                  the Bureau of Industry and Se...},
	langid = {english},
	timestamp = {2023-09-27 15:13:06 (GMT)},
	urldate = {2023-09-27}
}

@online{Harris202280000Hours,
	date = {2022},
	author = {Harris, Sam},
	abstract = {Open the door to a deeper understanding of
                  yourself-with guided meditations and insights for
                  living a more examined life.},
	journaltitle = {Waking Up},
	langid = {english},
	timestamp = {2023-09-23 19:11:51 (GMT)},
	title = {80,000 Hours},
	url = {https://dynamic.wakingup.com/socialshare/C1A610},
	urldate = {2023-09-23}
}

@online{Hawking2022BadOmensIn,
	file = {~/Google Drive/library-html/Hawking2022BadOmensIn.html;~/Google Drive/library-pdf/Hawking2022BadOmensIn.pdf},
	date = {2022-05-12},
	journaltitle = {Effective Altruism Forum},
	author = {Hawking, Theo},
	title = {Bad Omens in Current Community Building},
	url = {https://forum.effectivealtruism.org/posts/xomFCNXwNBeXtLq53/bad-omens-in-current-community-building},
	abstract = {Community building has recently had a surge in energy
                  and resources. It scales well, it's high leverage, and
                  you can get involved in it even if six m...},
	langid = {english},
	timestamp = {2023-09-25 19:30:02 (GMT)},
	urldate = {2023-09-25}
}

@misc{Hendrycks2022UnsolvedProblemsIn,
	abstract = {Machine learning ({ML}) systems are rapidly increasing in
                  size, are acquiring new capabilities, and are increasingly
                  deployed in high-stakes settings. As with other powerful
                  technologies, safety for {ML} should be a leading research
                  priority. In response to emerging safety challenges in {ML},
                  such as those introduced by recent large-scale models, we
                  provide a new roadmap for {ML} Safety and refine the technical
                  problems that the field needs to address. We present four
                  problems ready for research, namely withstanding hazards
                  ("Robustness"), identifying hazards ("Monitoring"), reducing
                  inherent model hazards ("Alignment"), and reducing systemic
                  hazards ("Systemic Safety"). Throughout, we clarify each
                  problem's motivation and provide concrete research
                  directions.},
	author = {Hendrycks, Dan and Carlini, Nicholas and Schulman, John and
                  Steinhardt, Jacob},
	langid = {english},
	database = {Tlön},
	date = {2022-06-16},
	doi = {10.48550/arXiv.2109.13916},
	eprint = {2109.13916 [cs]},
	eprinttype = {arxiv},
	keywords = {Computer Science - Machine Learning, Computer Science -
                  Artificial Intelligence, Computer Science - Computation and
                  Language, Computer Science - Computer Vision and Pattern
                  Recognition},
	number = {{arXiv}:2109.13916},
	publisher = {{arXiv}},
	timestamp = {2023-10-06 16:00:09 (GMT)},
	title = {Unsolved Problems in {ML} Safety},
	url = {http://arxiv.org/abs/2109.13916},
	urldate = {2023-10-06}
}

@online{Hobbhahn2022AiSafetyStarter,
	file = {~/Google Drive/library-html/Hobbhahn2022AiSafetyStarter.html;~/Google Drive/library-pdf/Hobbhahn2022AiSafetyStarter.pdf},
	date = {2022-03-28},
	journaltitle = {Effective Altruism Forum},
	author = {Hobbhahn, Marius},
	title = {{AI} safety starter pack},
	url = {https://forum.effectivealtruism.org/posts/pbiGHk6AjRxdBPoD8/ai-safety-starter-pack},
	abstract = {There are a ton of good resources for {AI} safety out
                  there. However, conversations with people trying to
                  get into the field revealed that these mater...},
	langid = {english},
	timestamp = {2023-09-26 14:28:07 (GMT)},
	urldate = {2023-09-26}
}

@Online{Hooker2003RuleConsequentialism,
	file = {~/Google Drive/library-pdf/Hooker2003RuleConsequentialism.pdf;~/Google Drive/library-html/Hooker2003RuleConsequentialism.html},
	eventdate = {2015-09-18},
	journaltitle = {Stanford Encylopedia of Philosophy},
	author = {Hooker, Brad},
	title = {Rule consequentialism},
	url = {https://plato.stanford.edu/archives/win2016/entries/consequentialism-rule/},
	date = {2003-12-31},
	timestamp = {2023-10-07 15:52:16 (GMT)},
	urldate = {2023-10-07}
}

@online{Horowitz2021AiAndInternational,
	file = {~/Google Drive/library-pdf/Horowitz2021AiAndInternational.pdf;~/Google Drive/library-html/Horowitz2021AiAndInternational.html},
	date = {2021-01-12},
	journaltitle = {Center for a New American Security},
	author = {Horowitz, Michael and Scharre, Paul},
	abstract = {Exploring the potential use of confidence-building measures
                  built around the shared interests that all countries have in
                  preventing inadvertent war.},
	langid = {english},
	shorttitle = {{AI} and International Stability},
	timestamp = {2023-09-28 12:42:34 (GMT)},
	title = {{AI} and international stability: risks and
                  confidence-building measures},
	url = {https://www.cnas.org/publications/reports/ai-and-international-stability-risks-and-confidence-building-measures},
	urldate = {2023-09-28}
}

@online{Hubinger2020ClarifyingInnerAlignment,
	file = {~/Google Drive/library-pdf/Hubinger2020ClarifyingInnerAlignment.pdf;~/Google Drive/library-html/Hubinger2020ClarifyingInnerAlignment.html},
	date = {2020-11-09},
	journaltitle = {{AI} Alignment Forum},
	author = {Hubinger, Evan},
	title = {Clarifying Inner Alignment Terminology},
	url = {https://www.alignmentforum.org/posts/SzecSPYxqRa5GCaSF/clarifying-inner-alignment-terminology},
	abstract = {I have seen a lot of confusion recently surrounding exactly
                  how outer and inner alignment should be defined and I want to
                  try and provide my attempt...},
	langid = {english},
	timestamp = {2023-09-28 12:18:08 (GMT)},
	urldate = {2023-09-28}
}

@Article{Hubinger2020OverviewOf11,
	file = {~/Google Drive/library-pdf/Hubinger2020OverviewOf11.pdf},
	number = {arXiv:2012.07532v1 [cs.LG]},
	abstract = {This paper analyzes and compares 11 different proposals for
                  building safe advanced {AI} under the current machine learning
                  paradigm, including major contenders such as iterated
                  amplification, {AI} safety via debate, and recursive reward
                  modeling. Each proposal is evaluated on the four components of
                  outer alignment, inner alignment, training competitiveness,
                  and performance competitiveness, of which the distinction
                  between the latter two is introduced in this paper. While
                  prior literature has primarily focused on analyzing individual
                  proposals, or primarily focused on outer alignment at the
                  expense of inner alignment, this analysis seeks to take a
                  comparative look at a wide range of proposals including a
                  comparative analysis across all four previously mentioned
                  components.},
	author = {Hubinger, Evan},
	date = {2020-12-04},
	doi = {10.48550/arXiv.2012.07532},
	eprint = {2012.07532 [cs]},
	eprinttype = {arxiv},
	keywords = {Computer Science - Machine Learning, Computer Science -
                  Artificial Intelligence},
	publisher = {{arXiv}},
	timestamp = {2023-09-28 10:33:45 (GMT)},
	title = {An overview of 11 proposals for building safe advanced {AI}},
	url = {http://arxiv.org/abs/2012.07532},
	urldate = {2023-09-28}
}

@Online{Hubinger2022TransparencyAndInterpretability,
	file = {~/Google Drive/library-html/Hubinger2022TransparencyAndInterpretability.html;~/Google Drive/library-pdf/Hubinger2022TransparencyAndInterpretability.pdf},
	date = {2022-06-17},
	journaltitle = {{AI} Alignment Forum},
	author = {Hubinger, Evan},
	title = {A transparency and interpretability tech tree},
	url = {https://www.alignmentforum.org/posts/nbq2bWLcYmSGup9aF/a-transparency-and-interpretability-tech-tree},
	abstract = {Thanks to Chris Olah, Neel Nanda, Kate Woolverton, Richard
                  Ngo, Buck Shlegeris, Daniel Kokotajlo, Kyle {McDonell}, Laria
                  Reynolds, Eliezer Yudkowksy, M...},
	langid = {english},
	timestamp = {2023-10-06 15:51:10 (GMT)},
	urldate = {2023-10-06}
}

@online{Idinsight2019SaveLifeOr,
	file = {~/Google Drive/library-html/Idinsight2019SaveLifeOr.html;~/Google Drive/library-pdf/Idinsight2019SaveLifeOr.pdf},
	abstract = {New research explores the preferences and values of
                  individuals and communities in Ghana and Kenya to inform
                  funding allocations.},
	database = {Tlön},
	author = {{IDinsight}},
	date = {2019-12-03},
	journaltitle = {{IDinsight}},
	langid = {english},
	shorttitle = {Save a life or receive cash?},
	timestamp = {2023-09-27 19:13:04 (GMT)},
	title = {Save a life or receive cash? Which do recipients want?},
	url = {https://www.idinsight.org/article/save-a-life-or-receive-cash-which-do-recipients-want/},
	urldate = {2023-09-27}
}

@online{Jaderberg2019CaptureFlagEmergence,
	file = {~/Google Drive/library-pdf/Jaderberg2019CaptureFlagEmergence.pdf;~/Google Drive/library-html/Jaderberg2019CaptureFlagEmergence.html},
	date = {2019-05-30},
	journaltitle = {DeepMind},
	author = {Jaderberg, Max and Czarnecki, Wojciech Marian and Dunning, Iain and Graepel, Thore and Marris, Luke},
	abstract = {Mastering the strategy, tactical understanding, and team play
                  involved in multiplayer video games represents a critical
                  challenge for {AI} research. In our latest paper, now
                  published in the journal Science, we present new developments
                  in reinforcement learning, resulting in human-level
                  performance in Quake {III} Arena Capture the Flag. This is a
                  complex, multi-agent environment and one of the canonical 3D
                  first-person multiplayer games. The agents successfully
                  cooperate with both artificial and human teammates, and
                  demonstrate high performance even when trained with reaction
                  times comparable to human players. Furthermore, we show how
                  these methods have managed to scale beyond research Capture
                  the Flag environments to the full game of Quake {III} Arena.},
	langid = {english},
	shorttitle = {Capture the Flag},
	timestamp = {2023-10-09 08:49:49 (GMT)},
	title = {Capture the Flag: the emergence of complex cooperative agents},
	url = {https://www.deepmind.com/blog/capture-the-flag-the-emergence-of-complex-cooperative-agents},
	urldate = {2023-10-09}
}

@online{John2020RepresentingFutureGenerations,
	date = {2020-03-21},
	journaltitle = {EAGlobal Virtual},
	author = {John, Tyler},
	note = {No translation found on 2023-09-26.},
	abstract = {Politics is a notoriously short-termist enterprise.
                  Political institutions generally operate on
                  2-to-4-year timescales (as the issue of climate change
                  has sh...},
	langid = {spanish},
	shorttitle = {Representing future generations {\textbar} Tyler John
                  {\textbar} {EA} Global},
	timestamp = {2023-09-26 09:09:22 (GMT)},
	title = {Representing future generations},
	url = {https://www.youtube.com/watch?v=095kFEA-jpE&list=PLwp9xeoX5p8Nje_8jJsmkz5ork-dVk9wK&index=11},
	urldate = {2023-09-26}
}

@article{Kagan2016WhatsWrongWith,
	file = {~/Google Drive/library-pdf/Kagan2016WhatsWrongWith.pdf},
	author = {Kagan, Shelly},
	title = {What's wrong with speciesism?},
	volume = {33},
	number = {1},
	pages = {1--21},
	doi = {10.1111/japp.12164},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/japp.12164},
	date = {2016-02},
	issn = {02643758},
	journaltitle = {Journal of Applied Philosophy},
	langid = {english},
	shortjournal = {J Appl Philos},
	shorttitle = {What's Wrong with Speciesism?},
	timestamp = {2023-09-27 08:57:34 (GMT)},
	urldate = {2023-09-27}
}

@online{Karnofsky2021Duplicator,
	file = {~/Google Drive/library-pdf/Karnofsky2021Duplicator.pdf;~/Google Drive/library-html/Karnofsky2021Duplicator.html},
	author = {Karnofsky, Holden},
	abstract = {If people could be insta-copied, economic growth would
                  explode.},
	date = {2021-07-20},
	journaltitle = {Cold Takes},
	langid = {english},
	timestamp = {2023-10-09 12:48:34 (GMT)},
	title = {The Duplicator},
	url = {https://www.cold-takes.com/the-duplicator/},
	urldate = {2023-10-09}
}

@online{Karnofsky2021RowingSteeringAnchoring,
	file = {~/Google Drive/library-pdf/Karnofsky2021RowingSteeringAnchoring.pdf;~/Google Drive/library-html/Karnofsky2021RowingSteeringAnchoring.html},
	author = {Karnofsky, Holden},
	database = {Tlön},
	abstract = {Five clashing pictures of how to help the world, and the
                  questions about history they raise.},
	date = {2021-11-09},
	journaltitle = {Cold Takes},
	langid = {english},
	timestamp = {2023-09-27 12:11:32 (GMT)},
	title = {Rowing, steering, anchoring, equity, mutiny},
	url = {https://www.cold-takes.com/rowing-steering-anchoring-equity-mutiny/},
	urldate = {2023-09-27}
}

@online{Karnofsky2023OtherCenteredEthics,
	file = {~/Google Drive/library-pdf/Karnofsky2023OtherCenteredEthics.pdf;~/Google Drive/library-html/Karnofsky2023OtherCenteredEthics.html},
	date = {2022-02-02},
	journaltitle = {Effective Altruism Forum},
	author = {Karnofsky, Holden},
	title = {Other-Centered Ethics and Harsanyi's Aggregation
                  Theorem},
	url = {https://forum.effectivealtruism.org/posts/iupkbiubpzDDGRpka/other-centered-ethics-and-harsanyi-s-aggregation-theorem},
	abstract = {I'm interested in doing as much good as possible. This
                  naturally raises the question of what counts as
                  "good," and how (if at all) we can compare som...},
	langid = {english},
	timestamp = {2023-09-26 07:12:11 (GMT)},
	urldate = {2023-09-26}
}

@online{Kbog2023AiWeapons,
	file = {~/Google Drive/library-pdf/Kbog2023AiWeapons.pdf;~/Google Drive/library-html/Kbog2023AiWeapons.html},
	date = {2019-11-13},
	journaltitle = {Effective Altruism Forum},
	author = {{kbog}},
	title = {On {AI} weapons},
	url = {https://forum.effectivealtruism.org/posts/vdqBn65Qaw77MpqXz/on-ai-weapons},
	abstract = {In this post I comprehensively review the risks and upsides of
                  lethal autonomous weapons ({LAWs}). I incorporate and expand
                  upon the ideas in this prev...},
	langid = {english},
	timestamp = {2023-09-28 12:30:00 (GMT)},
	urldate = {2023-09-28}
}

@online{Klein2023AIGetWeirder,
	author = {Klein, Ezra},
	title = {{A.I.} is about to get much weirder. Here s what to watch for},
	database = {Tlön},
	url = {https://www.nytimes.com/2023/03/21/opinion/ezra-klein-podcast-kelsey-piper.html},
	abstract = {The Vox writer Kelsey Piper talks about the increasing
                  pace of A.I. development, how it's changing the world
                  and what to do about it.},
	date = {2023-03-21},
	issn = {0362-4331},
	journaltitle = {The New York Times},
	langid = {american},
	timestamp = {2023-09-23 17:04:33 (GMT)},
	urldate = {2023-09-23}
}

@online{Klotz2019BiologicalWeaponsConvention,
	file = {~/Google Drive/library-html/Klotz2019BiologicalWeaponsConvention.html;~/Google Drive/library-pdf/Klotz2019BiologicalWeaponsConvention.pdf},
	abstract = {When the Biological Weapons Convention was enacted, it had no
                  provisions for ensuring that countries were complying with it.
                  As representatives of the countries that have signed the
                  treaty prepare to meet in Geneva this December, they should
                  reconsider an idea to provide for site visits and
                  investigations for alleged cases of weapons stockpiling,
                  development, and use.},
	author = {Klotz, Lynn},
	date = {2019-11-15},
	journaltitle = {Bulletin of the Atomic Scientists},
	langid = {american},
	timestamp = {2023-09-27 15:15:19 (GMT)},
	title = {The Biological Weapons Convention protocol should be
                  revisited},
	url = {https://thebulletin.org/2019/11/the-biological-weapons-convention-protocol-should-be-revisited/},
	urldate = {2023-09-27}
}

@online{Koehler2020IdeasForHigh,
	file = {~/Google Drive/library-html/Koehler2020IdeasForHigh.html;~/Google Drive/library-pdf/Koehler2020IdeasForHigh.pdf},
	abstract = {Below we list some more career options beyond our priority
                  paths that seem promising to us for positively influencing the
                  long-term future. Some of these are likely to be written up as
                  priority paths in the future, or wrapped into existing ones,
                  but we haven't written full profiles for them yet-for example
                  policy careers outside {AI} and biosecurity policy that seem
                  promising from a longtermist perspective. Others, like
                  information security, we think might be as promising for many
                  people as our priority paths, but because we haven't
                  investigated them much we're still unsure. Still others seem
                  like they'll typically be less impactful than our priority
                  paths for people who can succeed equally in either, but still
                  seem high-impact to us and like they could be top options for
                  a substantial number of people, depending on personal fit-for
                  example research management.},
	author = {Koehler, Arden},
	date = {2020-08-03},
	journaltitle = {80,000 Hours},
	langid = {american},
	timestamp = {2023-09-27 09:52:31 (GMT)},
	title = {Ideas for high-impact careers beyond our priority paths},
	url = {https://80000hours.org/2020/08/ideas-for-high-impact-careers-beyond-our-priority-paths/},
	urldate = {2023-09-27}
}

@report{Korinek2017ArtificialIntelligenceAnd,
	file = {~/Google Drive/library-pdf/Korinek2017ArtificialIntelligenceAnd.pdf},
	author = {Korinek, Anton and Stiglitz, Joseph},
	date = {2017-12},
	doi = {10.3386/w24174},
	institution = {National Bureau of Economic Research},
	langid = {english},
	location = {Cambridge, {MA}},
	number = {working paper 24174},
	timestamp = {2023-09-28 10:42:38 (GMT)},
	title = {Artificial intelligence and its implications for income
                  distribution and unemployment},
	url = {http://www.nber.org/papers/w24174.pdf},
	urldate = {2023-09-28}
}

@book{Kymlicka1990ContemporaryPoliticalPhilosophy,
	edition = {1},
	author = {Kymlicka, Will},
	langid = {english},
	database = {Tlön},
	title = {Contemporary Political Philosophy: An Introduction},
	publisher = {Clarendon Press},
	date = {1990},
	isbn = {9780198277231},
	keywords = {Political science, History, 20th century, Political
                  science, Philosophy},
	location = {Oxford},
	pagetotal = {321},
	shorttitle = {Contemporary political philosophy},
	timestamp = {2023-09-23 07:55:06 (GMT)}
}

@Book{Kymlicka1995FilosofiaPoliticaContemporanea,
	langid = {spanish},
	translation = {Kymlicka1990ContemporaryPoliticalPhilosophy},
	publisher = {Ariel},
	address = {Barcelona},
	translator = {Gargarella, Roberto},
	file = {~/Google Drive/library-pdf/Kymlicka1995FilosofiaPoliticaContemporanea.pdf},
	date = {1995},
	title = {Filosofía política contemporánea. Una introducción},
	author = {Kymlicka, Will},
	timestamp = {2023-09-23 07:53:16 (GMT)}
}

@online{MacAskill2018WhatAreMost,
	database = {Tlön},
	title = {What are the most important moral problems of our
                  time?},
	url = {https://www.ted.com/talks/will_macaskill_what_are_the_most_important_moral_problems_of_our_time},
	abstract = {Of all the problems facing humanity, which should we
                  focus on solving first? In a compelling talk about how
                  to make the world better, moral philosopher Will
                  {MacAskill} provides a framework for answering this
                  question based on the philosophy of "effective
                  altruism" — and shares ideas for taking on three
                  pressing global issues.},
	journaltitle = {{TED}},
	author = {{MacAskill}, William},
	urldate = {2021-11-27},
	date = {2018-04},
	langid = {english},
	file = {~/Google Drive/library-pdf/MacAskill1536763373WhatAreMost.pdf;~/Google Drive/library-html/will_macaskill_what_are_the_most_important_moral_problems_of_our_time.html}
}

@thesis{Mogensen2014EvolutionaryDebunkingArguments,
	file = {~/Google Drive/library-pdf/Mogensen2014EvolutionaryDebunkingArguments.pdf},
	abstract = {{\textless}p{\textgreater}I consider whether
                  evolutionary explanations can debunk our moral
                  beliefs. Most contemporary discussion in this area is
                  centred on the question of whether debunking
                  implications follow from our ability to explain
                  elements of human morality in terms of natural
                  selection, given that there has been no selection for
                  true moral beliefs. By considering the most prominent
                  arguments in the literature today, I offer reasons to
                  think that debunking arguments of this kind fail.
                  However, I argue that a successful evolutionary
                  debunking argument can be constructed by appeal to the
                  suggestion that our moral outlook reflects arbitrary
                  contingencies of our phylogeny, much as the horizontal
                  orientation of the whale's tail reflects its descent
                  from terrestrial
                  quadrupeds.{\textless}/p{\textgreater}
                  {\textless}p{\textgreater}An introductory chapter
                  unpacks the question of whether evolutionary
                  explanations can debunk our moral beliefs, offers a
                  brief historical guide to the philosophical discussion
                  surrounding it, and explains what I mean to contribute
                  to this discussion. Thereafter follow six chapters and
                  a conclusion. The six chapters are divided into three
                  pairs. The first two chapters consider what
                  contemporary scientific evidence can tell us about the
                  evolutionary origins of morality and, in particular,
                  to what extent the evidence speaks in favour of the
                  claims on which debunking arguments rely. The next two
                  chapters offer a critique of popular debunking
                  arguments that are centred on the irrelevance of moral
                  facts in natural selection explanations. The final
                  chapters develop a novel argument for the claim that
                  evolutionary explanations can undermine our moral
                  beliefs insofar as they show that our moral outlook
                  reflects arbitrary contingencies of our phylogeny. A
                  conclusion summarizes my argument and sets out the key
                  questions that arise in its
                  wake.{\textless}/p{\textgreater}},
	author = {Mogensen, A.},
	date = {2014},
	institution = {University of Oxford},
	timestamp = {2023-09-19 12:57:26 (GMT)},
	title = {Evolutionary debunking arguments in ethics},
	type = {http://purl.org/dc/dcmitype/Text},
	url = {https://ora.ox.ac.uk/objects/uuid:420bc313-2b07-42e0-8b45-dcbebff2d5d2},
	urldate = {2023-09-19}
}

@article{Mogensen2022EvolutionUtilitarianismNormative,
	database = {Tlön},
	title = {Evolution, utilitarianism, and normative
                  uncertainty: tthe practical significance of
                  debunking arguments},
	langid = {english},
	volume = 22,
	issn = {1559-3061},
	url = {https://jesp.org/index.php/jesp/article/view/1266},
	doi = {10.26556/jesp.v22i3.1266},
	shorttitle = {Evolution, Utilitarianism, and Normative
                  Uncertainty},
	abstract = {It has been argued that evolutionary considerations
                  favour utilitarianism by selectively debunking its
                  competitors. However, evolutionary considerations
                  also seem to undermine the practical significance of
                  utilitarianism, since commonsense beliefs about
                  well-being seem like prime candidates for
                  evolutionary debunking. We argue that the practical
                  significance of utilitarianism is not undermined in
                  this way if we understand the requirements of
                  practical rationality as sensitive to normative
                  uncertainty. We consider the view that rational
                  decision-making under normative uncertainty requires
                  maximizing expected choice-worthiness, as well as
                  the possibility that different theories’
                  choice-worthiness rankings are not all
                  interval-scale measurable or intertheoretically
                  comparable. Finally, we suggest how evolutionary
                  considerations may increase the practical
                  significance of utilitarianism even if belief in
                  utilitarianism is debunked by evolutionary
                  considerations, so long as belief in competing
                  theories is undermined to an even greater extent.},
	number = 3,
	journaltitle = {Journal of Ethics and Social Philosophy},
	shortjournal = {{JESP}},
	author = {Mogensen, Andreas and {MacAskill}, William},
	urldate = {2022-10-01},
	date = {2022-09-19},
	file = {~/Google Drive/library-pdf/Mogensen2022EvolutionUtilitarianismNormative.pdf;~/Google Drive/library-pdf/Mogensen2022EvolutionUtilitarianismNormative.pdf}
}

@online{Monrad2022FourCategoriesOf,
	file = {~/Google Drive/library-pdf/Monrad2022FourCategoriesOf.pdf;~/Google Drive/library-html/Monrad2022FourCategoriesOf.html},
	date = {2022-04-09},
	journaltitle = {Effective Altruism Forum},
	author = {Monrad, Joshua},
	title = {Four categories of effective altruism critiques},
	url = {https://forum.effectivealtruism.org/posts/HzyYoLK2ERTnDmrjB/four-categories-of-effective-altruism-critiques},
	abstract = {When discussing critiques of effective altruism,[1] I
                  often find that lots of different kinds of critiques
                  are lumped together, which can lead to peo...},
	langid = {english},
	timestamp = {2023-09-25 17:27:25 (GMT)},
	urldate = {2023-09-25}
}

@online{Moorhouse2021MajorUnReport,
	file = {~/Google Drive/library-html/Moorhouse2021MajorUnReport.html;~/Google Drive/library-pdf/Moorhouse2021MajorUnReport.pdf},
	date = {2021-17-07},
	journaltitle = {Effective Altruism Forum},
	author = {Moorhouse, Fin and Balwit, Avital},
	title = {Major {UN} report discusses existential risk and
                  future generations (summary)},
	url = {https://forum.effectivealtruism.org/posts/Fwu2SLKeM5h5v95ww/major-un-report-discusses-existential-risk-and-future},
	abstract = {Introduction and Key Points On September 10th, the
                  Secretary General of the United Nations released a
                  report called "Our Common Agenda". This report...},
	langid = {english},
	timestamp = {2023-09-26 09:14:15 (GMT)},
	urldate = {2023-09-26}
}

@Book{Morris2012QuestionsOfLife,
	date = {2012},
	editor = {Morris, Christopher W.},
	langid = {english},
	database = {Tlön},
	isbn = {9780195156980},
	keywords = {Death, Moral and ethical aspects, Textbooks},
	location = {New York},
	pagetotal = {530},
	publisher = {Oxford University Press},
	shorttitle = {Questions of life and death},
	timestamp = {2023-09-27 08:52:22 (GMT)},
	title = {Questions of life and death: readings in practical ethics}
}

@online{Muehlhauser2022EffectiveAltruismAs,
	file = {~/Google Drive/library-pdf/Muehlhauser2022EffectiveAltruismAs.pdf;~/Google Drive/library-html/Muehlhauser2022EffectiveAltruismAs.html},
	date = {2022-06-06},
	journaltitle = {Luke Muehlhauser's website},
	author = {Muehlhauser, Luke},
	timestamp = {2023-09-26 07:05:35 (GMT)},
	title = {Effective altruism as I see it},
	url = {https://lukemuehlhauser.com/effective-altruism-as-i-see-it/},
	urldate = {2023-09-26}
}

@online{Nanda2021MyOverviewOf,
	file = {~/Google Drive/library-html/Nanda2021MyOverviewOf.html;~/Google Drive/library-pdf/Nanda2021MyOverviewOf.pdf},
	date = {2021-12-16},
	journaltitle = {{AI} Alignment Forum},
	author = {Nanda, Neel},
	title = {My overview of the {AI} alignment landscape: a bird's eye
                  view},
	url = {https://www.alignmentforum.org/posts/SQ9cZtfrzDJmw9A2m/my-overview-of-the-ai-alignment-landscape-a-bird-s-eye-view},
	abstract = {Disclaimer: I recently started as an interpretability
                  researcher at Anthropic, but I wrote this doc before starting,
                  and it entirely represents my pe...},
	langid = {english},
	shorttitle = {My Overview of the {AI} Alignment Landscape},
	timestamp = {2023-09-28 10:30:41 (GMT)},
	urldate = {2023-09-28}
}

@online{Nanda2022HowFormedMy,
	file = {~/Google Drive/library-html/Nanda2022HowFormedMy.html;~/Google Drive/library-pdf/Nanda2022HowFormedMy.pdf},
	journaltitle = {Effective Altruism Forum},
	author = {Nanda, Neel},
	date = {2022-03-07},
	timestamp = {2023-09-25 19:18:48 (GMT)},
	title = {How I formed my own views about {AI} safety},
	url = {https://forum.effectivealtruism.org/posts/xS9dFE3A6jdooiN7M/how-i-formed-my-own-views-about-ai-safety},
	urldate = {2023-09-25}
}

@Online{Nanda2023LonglistOfTheories,
	file = {~/Google Drive/library-pdf/Nanda2023LonglistOfTheories.pdf;~/Google Drive/library-html/Nanda2023LonglistOfTheories.html},
	date = {2022-03-11},
	journaltitle = {{AI} Alignment Forum},
	author = {Nanda, Neel},
	title = {A longlist of theories of impact for interpretability},
	url = {https://www.alignmentforum.org/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability},
	abstract = {I hear a lot of different arguments floating around for
                  exactly how mechanistically interpretability research will
                  reduce x-risk. As an interpretabil...},
	langid = {english},
	timestamp = {2023-10-06 15:53:00 (GMT)},
	urldate = {2023-10-06}
}

@article{Obrien2020AssessingRisksPosed,
	author = {O'Brien, John T. and Nelson, Cassidy},
	title = {Assessing the risks posed by the convergence of artificial
                  intelligence and biotechnology},
	volume = {18},
	number = {3},
	pages = {219--227},
	doi = {10.1089/hs.2019.0122},
	url = {https://www.liebertpub.com/doi/10.1089/hs.2019.0122},
	date = {2020-06-01},
	issn = {2326-5094, 2326-5108},
	journaltitle = {Health Security},
	langid = {english},
	shortjournal = {Health Security},
	timestamp = {2023-09-28 12:49:11 (GMT)},
	urldate = {2023-09-28}
}

@online{OpenPhilanthropy2022CareerDevelopmentAnd,
	date = {2022},
	author = {Open Philanthropy},
	abstract = {This program aims to provide support - in the form of
                  funding for graduate study, unpaid internships,
                  self-study, career transition and exploration periods,
                  and other activities relevant to building career
                  capital - for individuals at any career stage who want
                  to pursue careers that could help to reduce global
                  catastrophic risks or otherwise improve the [...]},
	journaltitle = {Open Philanthropy},
	langid = {english},
	timestamp = {2023-09-25 11:05:41 (GMT)},
	title = {Career development and transition funding},
	url = {https://www.openphilanthropy.org/career-development-and-transition-funding/},
	urldate = {2023-09-25}
}

@online{Ozymandias2023EaDedicates,
	file = {~/Google Drive/library-html/Ozymandias2023EaDedicates.html;~/Google Drive/library-pdf/Ozymandias2023EaDedicates.pdf},
	date = {2022-06-23},
	journaltitle = {Effective Altruism Forum},
	author = {{ozymandias}},
	title = {{EA} Dedicates},
	url = {https://forum.effectivealtruism.org/posts/aYifx3zd5R5N8bQ6o/ea-dedicates},
	abstract = {I've noticed a persistent competing need in effective altruist
                  communities. ...},
	langid = {english},
	timestamp = {2023-09-27 11:03:42 (GMT)},
	urldate = {2023-09-27}
}

@article{Pace2018SketchOfGood,
	file = {~/Google Drive/library-html/Pace2018SketchOfGood.html;~/Google Drive/library-pdf/Pace2018SketchOfGood.pdf},
	date = {2018-04-01},
	journaltitle = {LessWrong},
	author = {Pace, Ben},
	title = {A sketch of good communication},
	url = {https://www.lesswrong.com/posts/yeADMcScw8EW9yxpH/a-sketch-of-good-communication},
	abstract = {"Often I compare my own Fermi estimates with those of
                  other people, and that's sort of cool, but what's way
                  more interesting is when they share what...},
	langid = {english},
	timestamp = {2023-09-25 18:53:31 (GMT)},
	urldate = {2023-09-25}
}

@online{Plant2020UsingSubjectiveWell,
	file = {~/Google Drive/library-html/Plant2020UsingSubjectiveWell.html;~/Google Drive/library-pdf/Plant2020UsingSubjectiveWell.pdf},
	date = {2020-08-03},
	journaltitle = {Effective Altruism Forum},
	author = {Plant, Michael and {McGuire}, Joel and Donaldson, Clare},
	title = {Using subjective well-being to estimate the moral weights of
averting deaths and reducing poverty},
	url = {https://forum.effectivealtruism.org/posts/xgxzCkpKQsPwrd5W7/using-subjective-well-being-to-estimate-the-moral-weights-of-2},
	abstract = {[Edit: 11/02/2021: changed how results were calculated in
                  response to Aidan's comments.] [Edit: 03/09/2020: a few minor
                  typos corrected.] ...},
	langid = {english},
	timestamp = {2023-09-27 19:04:09 (GMT)},
	urldate = {2023-09-27}
}

@article{Plant2022PhilosophicalReviewOf,
	file = {~/Google Drive/library-html/Plant2022PhilosophicalReviewOf.html;~/Google Drive/library-pdf/Plant2022PhilosophicalReviewOf.pdf},
	date = {2022-07-15},
	database = {Tlön},
	journaltitle = {Effective Altruism Forum},
	author = {Plant, Michael},
	title = {A philosophical review of Open Philanthropy's cause
                  prioritisation framework},
	url = {https://forum.effectivealtruism.org/posts/bdiDW83SFAsoA4EeB/a-philosophical-review-of-open-philanthropy-s-cause},
	abstract = {Summary In this post, I undertake a philosophical
                  review of Open Philanthropy's Global Health and
                  Wellbeing Cause Prioritisation Framework, the
                  metho...},
	langid = {english},
	timestamp = {2023-09-25 19:35:41 (GMT)},
	urldate = {2023-09-25}
}

@online{Prieto2022HowAccurateAre,
	file = {~/Google Drive/library-pdf/Prieto2022HowAccurateAre.pdf;~/Google Drive/library-html/Prieto2022HowAccurateAre.html},
	date = {2022-06-16},
	database = {Tlön},
	journaltitle = {Effective Altruism Forum},
	author = {Prieto, Javier},
	title = {How accurate are open phil's predictions?},
	url = {https://forum.effectivealtruism.org/posts/RjNFyJS3jPb4DA7wA/how-accurate-are-open-phil-s-predictions},
	abstract = {When investigating a grant, Open Philanthropy staff
                  often make probabilistic predictions about
                  grant-related outcomes they care about, e.g. "I'm
                  70\%...},
	langid = {english},
	timestamp = {2023-09-26 09:00:10 (GMT)},
	urldate = {2023-09-26}
}

@online{RandomEA2018CommentProblemsWith,
	file = {~/Google Drive/library-html/Randomea2018CommentProblemsWith.html;~/Google Drive/library-pdf/Randomea2018CommentProblemsWith.pdf},
	date = {2018-08-04},
	database = {Tlön},
	journaltitle = {Effective Altruism Forum},
	author = {{RandomEA}},
	abstract = {Comment by {RandomEA} - Here are ten reasons you might
                  choose to work on near-term causes. The first five are
                  reasons you might think near term work is more
                  important, while the latter five are why you might
                  work on near term causes even if you think long term
                  future work is more important. 1. You might think the
                  future is likely to be net negative. Click here for
                  why one person initially thought this and here for why
                  another person would be reluctant to support
                  existential risk work (it makes space colonization
                  more likely, which could increase future suffering).
                  2. Your view of population ethics might cause you to
                  think existential risks are relatively unimportant. Of
                  course, if your view was merely a standard person
                  affecting view, it would be subject to the response
                  that work on existential risk is high value even if
                  only the present generation is considered. However,
                  you might go further and adopt an Epicurean view under
                  which it is not bad for a person to die a premature
                  death (meaning that death is only bad to the extent it
                  inflicts suffering on oneself or others). 3. You might
                  have a methodological objection to applying expected
                  value to cases where the probability is small. While
                  the author attributes this view to Holden Karnofsky,
                  Karnofsky now puts much more weight on the view that
                  improving the long term future is valuable. 4. You
                  might think it's hard to predict how the future will
                  unfold and what impact our actions will have. (Note
                  that the post is from five years ago and may no longer
                  reflect the views of the author.) 5. You might think
                  that {AI} is unlikely to be a concern for at least 50
                  years (perhaps based on your conversations with people
                  in the field). Given that ongoing suffering can only
                  be alleviated in the present, you might think it's
                  better to focus on that for now. 6. You might think
                  that when there is an opportunity to have an unusually
                  large impact in the present, you should take},
	langid = {english},
	timestamp = {2023-09-26 10:06:52 (GMT)},
	title = {Comment on 'Problems with {EA} representativeness and how to solve
                  it'},
	url = {https://forum.effectivealtruism.org/posts/2qGL7FoTEZTwp38ij/problems-with-ea-representativeness-and-how-to-solve-it},
	urldate = {2023-09-26}
}

@article{Remmelt2021SomeBlindspotsIn,
	file = {~/Google Drive/library-pdf/Remmelt2021SomeBlindspotsIn.pdf;~/Google Drive/library-html/Remmelt2021SomeBlindspotsIn.html},
	date = {2021-03-21},
	database = {Tlön},
	journaltitle = {Effective Altruism Forum},
	author = {Remmelt, Ellen},
	title = {Some blindspots in rationality and effective altruism},
	url = {https://forum.effectivealtruism.org/posts/LJwGdex4nn76iA8xy/some-blindspots-in-rationality-and-effective-altruism},
	abstract = {Update: appreciating Scott Alexander's humble
                  descriptions in his recent Criticism of Criticism of
                  Criticism post. A clarification I need to make is...},
	langid = {english},
	timestamp = {2023-09-25 18:41:56 (GMT)}
}

@online{Rodriguez2022MyExperienceWith,
	file = {~/Google Drive/library-html/Rodriguez2022MyExperienceWith.html;~/Google Drive/library-pdf/Rodriguez2022MyExperienceWith.pdf},
	abstract = {I've felt like an imposter since my first year of university.
                  I was accepted to the university that I believed was well out
                  of my league - my 'stretch' school. I'd gotten good grades in
                  high school, but I'd never seen myself as especially smart: I
                  wasn't selected for gifted programmes in elementary school
                  like some of my friends were, and my standardised test scores
                  were in the bottom half of those attending my university. I
                  was pretty confident I got into the university because of some
                  fluke in the system (my top hypothesis was that I was admitted
                  as part of an affirmative action initiative) - and that belief
                  stayed with me (and was amplified) during the decade that
                  followed.},
	author = {Rodriguez, Luisa},
	date = {2022-04-21},
	journaltitle = {80,000 Hours},
	langid = {american},
	timestamp = {2023-09-27 11:50:43 (GMT)},
	title = {My experience with imposter syndrome - and how to (partly)
                  overcome it},
	url = {https://80000hours.org/2022/04/imposter-syndrome/},
	urldate = {2023-09-27}
}

@online{Roger-Smith2021YouAreProbably,
	file = {~/Google Drive/library-pdf/Roger-Smith2021YouAreProbably.pdf;~/Google Drive/library-html/Roger-Smith2021YouAreProbably.html},
	date = {2021-11-14},
	journaltitle = {Effective Altruism Forum},
	author = {{Roger-Smith}, Charlie},
	title = {You Are Probably Underestimating How Good Self-Love Can Be},
	url = {https://forum.effectivealtruism.org/posts/QhPyQTXuGt58Nzxnu/you-are-probably-underestimating-how-good-self-love-can-be},
	abstract = {I am very grateful to the following people, in general, and
                  for their helpful feedback on this post: Nick Cammarata, Kaj
                  Sotala, Miranda Dixon-Luinen...},
	langid = {english},
	timestamp = {2023-09-27 11:52:37 (GMT)},
	urldate = {2023-09-27}
}

@online{Ronn2017WhyEffectiveAltruists,
	url = {https://www.youtube.com/watch?v=St8BbOWk6SI},
	date = {2017-09-08},
	journaltitle = {{EAG} San Francisco},
	title = {Why effective altruists should care about global governance},
	author = {Rönn, Kristian},
	timestamp = {2023-09-27 15:39:40 (GMT)}
}

@article{Roser2013ChildAndInfant,
	file = {~/Google Drive/library-pdf/Roser2013ChildAndInfant.pdf;~/Google Drive/library-html/Roser2013ChildAndInfant.html},
	eventdate = {2019-11},
	langid = {english},
	database = {Tlön},
	author = {Roser, Max and Ritchie, Hannah and Dadonaite,
                  Bernadeta},
	title = {Child and infant mortality},
	url = {https://ourworldindata.org/child-mortality},
	abstract = {Why are children dying and what can be done to prevent
                  it?},
	date = {2013-05-10},
	journaltitle = {Our World in Data},
	shortjournal = {Our World in Data},
	timestamp = {2023-09-21 18:47:41 (GMT)},
	urldate = {2023-09-21}
}

@online{Roser2019ShortHistoryOf,
	file = {~/Google Drive/library-html/Roser2019ShortHistoryOf.html;~/Google Drive/library-pdf/Roser2019ShortHistoryOf.pdf},
	date = {2019},
	author = {Roser, Max},
	abstract = {We are working on Our World in Data to provide
                  'Research and data to make progress against the
                  world's largest problems'.},
	journaltitle = {Our World in Data},
	timestamp = {2023-09-21 19:06:07 (GMT)},
	title = {The short history of global living conditions and why
                  it matters that we know it},
	url = {https://ourworldindata.org/a-history-of-global-living-conditions},
	urldate = {2023-09-21}
}

@online{Ruhl2022RisksFromAutonomous,
	file = {~/Google Drive/library-pdf/Ruhl2022RisksFromAutonomous.pdf;~/Google Drive/library-html/Ruhl2022RisksFromAutonomous.html},
	url = {https://web.archive.org/web/20221016010520/https://forum.effectivealtruism.org/posts/RKMNZn7r6cT2Yaorf/risks-from-autonomous-weapon-systems-and-military-ai},
	date = {2022-05-19},
	journaltitle = {Effective Altruism Forum},
	title = {Risks from autonomous weapon systems and military AI},
	author = {Ruhl, Christian},
	timestamp = {2023-09-28 12:27:21 (GMT)}
}

@online{Sabien2017DoubleCruxStrategy,
	file = {~/Google Drive/library-html/Sabien2017DoubleCruxStrategy.html;~/Google Drive/library-pdf/Sabien2017DoubleCruxStrategy.pdf},
	date = {2017-01-02},
	journaltitle = {LessWrong},
	author = {Sabien, Duncan},
	title = {Double Crux - a Strategy for Mutual Understanding},
	url = {https://www.lesswrong.com/posts/exa5kmvopeRyfJgCy/double-crux-a-strategy-for-mutual-understanding},
	abstract = {Preamble \textbullet Double crux is one of {CFAR}'s
                  newer concepts, and one that's forced a re-examination
                  and refactoring of a lot of our curriculum (in the
                  sam...},
	langid = {english},
	timestamp = {2023-09-26 07:28:17 (GMT)},
	urldate = {2023-09-26}
}

@book{Sagan2006PuntoAzulPalido,
	file = {~/Google Drive/library-pdf/Sagan2006PuntoAzulPalido.pdf},
	translation = {Sagan1994PaleBlueDot},
	address = {Barcelona},
	publisher = {Planeta},
	translator = {Widmer Caminal, Marina},
	author = {Sagan, Carl},
	title = {Un punto azul pálido},
	date = {2006},
	timestamp = {2023-09-30 10:08:24 (GMT)},
	urldate = {2023-09-30}
}

@online{Salamon2023HumansAreNot,
	file = {~/Google Drive/library-html/Salamon2023HumansAreNot.html;~/Google Drive/library-pdf/Salamon2023HumansAreNot.pdf},
	journaltitle = {LessWrong},
	author = {Salamon, Anna},
	title = {Humans are not automatically strategic},
	url = {https://www.lesswrong.com/posts/PBRWb2Em5SNeWYwwB/humans-are-not-automatically-strategic},
	abstract = {Reply to: A "Failure to Evaluate Return-on-Time"
                  Fallacy \textbullet Lionhearted writes: ...},
	langid = {english},
	timestamp = {2023-09-26 07:30:22 (GMT)},
	urldate = {2023-09-26}
}

@online{Samuel2019ShouldAnimalsPlants,
	file = {~/Google Drive/library-html/Samuel2019ShouldAnimalsPlants.html;~/Google Drive/library-pdf/Samuel2019ShouldAnimalsPlants.pdf},
	abstract = {How humanity's idea of who deserves moral concern has grown -
                  and will keep growing.},
	author = {Samuel, Sigal},
	date = {2019-04-04},
	journaltitle = {Vox},
	langid = {english},
	timestamp = {2023-09-27 07:09:18 (GMT)},
	title = {Should animals, plants, and robots have the same rights as
                  you?},
	url = {https://www.vox.com/future-perfect/2019/4/4/18285986/robot-animal-nature-expanding-moral-circle-peter-singer},
	urldate = {2023-09-27}
}

@online{Sanders2018GlobalAnimalSlaughter,
	file = {~/Google Drive/library-pdf/Sanders2018GlobalAnimalSlaughter.pdf;~/Google Drive/library-html/Sanders2018GlobalAnimalSlaughter.html},
	abstract = {Worldwide, more than 70 billion land animals are
                  killed for food every year. Our series of charts based
                  on United Nations data shows the trends by type of
                  animal.},
	database = {Tlön},
	author = {Sanders, Bas},
	date = {2018-10-10},
	journaltitle = {Faunalytics},
	langid = {american},
	timestamp = {2023-09-21 18:54:12 (GMT)},
	title = {Global animal slaughter statistics and charts},
	url = {https://faunalytics.org/global-animal-slaughter-statistics-and-charts/},
	urldate = {2023-09-21}
}

@online{Savoie2018WhyFoundingCharities,
	file = {~/Google Drive/library-html/Savoie2018WhyFoundingCharities.html;~/Google Drive/library-pdf/Savoie2018WhyFoundingCharities.pdf},
	date = {2018-05-13},
	journaltitle = {Effective Altruism Forum},
	author = {Savoie, Joey},
	title = {Why founding charities is one of the highest impact things one
                  can do},
	url = {https://forum.effectivealtruism.org/posts/d8Q3tFQsjG4iiXQgs/why-founding-charities-is-one-of-the-highest-impact-things},
	abstract = {Track record and tractability \textbullet Historically, some
                  of the highest impact individuals in the {EA} movement and
                  across the broader world have been people...},
	langid = {english},
	timestamp = {2023-09-27 09:57:31 (GMT)},
	urldate = {2023-09-27}
}

@online{Savoie2022DeferenceCultureIn,
	file = {~/Google Drive/library-pdf/Savoie2022DeferenceCultureIn.pdf},
	author = {Savoie, Joey},
	langid = {english},
	database = {Tlön},
	journaltitle = {Effective Altruism Forum},
	date = {2022-06-07},
	timestamp = {2023-09-25 19:24:01 (GMT)},
	title = {Deference culture in {EA}},
	url = {https://web.archive.org/web/20230329162326/https://forum.effectivealtruism.org/posts/Jx6ncakmergiC74kG/deference-culture-in-ea},
	urldate = {2023-09-25}
}

@online{Sempere2023MyHighlyPersonal,
	file = {~/Google Drive/library-pdf/Sempere2023MyHighlyPersonal.pdf;~/Google Drive/library-html/Sempere2023MyHighlyPersonal.html},
	journaltitle = {Measure is Unceasing},
	date = {2023-01-23},
	author = {Sempere, Nuño},
	timestamp = {2023-01-25 09:58:21 (GMT)},
	title = {My highly personal skepticism braindump on
                  existential risk from artificial intelligence.},
	url = {https://nunosempere.com/blog/2023/01/23/my-highly-personal-skepticism-braindump-on-existential-risk/},
	urldate = {2023-01-25}
}

@online{Shapiro2002UserAgentValue,
	url = {https://web.archive.org/web/20221016011851/https://www.aaai.org/Papers/Symposia/Spring/2002/SS-02-07/SS02-07-002.pdf},
	journaltitle = {AAAI Technical Report SS-02-07},
	booktitle = {AAAI Technical Report SS-02-07},
	date = {2002},
	title = {User-agent value alignment},
	author = {Shapiro, Daniel and Schachter, Ross},
	timestamp = {2023-09-28 12:04:14 (GMT)}
}

@Book{Singer2009LifeYouCan1,
	file = {~/Google Drive/library-pdf/Singer2009LifeYouCan.pdf},
	edition = {tenth anniversary edition},
	address = {Sydney},
	publisher = {The Life You Can Save},
	isbn = {978-1-7336727-0-2},
	date = {2009},
	title = {The life you can save},
	author = {Singer, Peter},
	timestamp = {2023-09-21 17:44:58 (GMT)}
}

@online{Singer2023WhyAndHow,
	file = {~/Google Drive/library-html/Singer2023WhyAndHow.html},
	journaltitle = {TED},
	langid = {english},
	database = {Tlön},
	date = {2013-03},
	abstract = {If you're lucky enough to live without want, it's a
                  natural impulse to be altruistic to others. But, asks
                  philosopher Peter Singer, what's the most effective
                  way to give? He talks through some surprising thought
                  experiments to help you balance emotion and
                  practicality -- and make the biggest impact with
                  whatever you can share. {NOTE}: Starting at 0:30, this
                  talk contains 30 seconds of graphic footage.},
	author = {Singer, Peter},
	timestamp = {2023-09-21 17:12:31 (GMT)},
	title = {The why and how of effective altruism},
	url = {https://www.ted.com/talks/peter_singer_the_why_and_how_of_effective_altruism},
	urldate = {2023-09-21}
}

@Report{Soares2014AligningSuperintelligenceWith,
	file = {~/Google Drive/library-pdf/Soares2014AligningSuperintelligenceWith.pdf},
	date = {2014},
	url = {https://intelligence.org/files/obsolete/TechnicalAgenda%5Bold%5D.pdf},
	number = {technical report 8},
	institution = {Machine Intelligence Research Institute},
	title = {Aligning superintelligence with human interests: a technical research agenda},
	author = {Soares, Nate and Fallenstein, Benja},
	timestamp = {2023-09-28 12:13:14 (GMT)}
}

@online{Soares2015ValueOfLife,
	file = {~/Google Drive/library-pdf/Soares2015ValueOfLife.pdf;~/Google Drive/library-html/Soares2015ValueOfLife.html},
	author = {Soares, Nate},
	langid = {english},
	database = {Tlön},
	abstract = {If you have money and want to save lives, you had
                  better put a price on life. Scott Alexander explains
                  it better than I can. But don't mix up the price of a
                  life with the value of a life. I see this happen all
                  too frequently. To correct this},
	date = {2015-02-16},
	journaltitle = {Minding our way},
	timestamp = {2023-09-26 07:15:45 (GMT)},
	title = {The value of a life},
	url = {https://mindingourway.com/the-value-of-a-life/},
	urldate = {2023-09-26}
}

@online{Speziali2022PathsToImpact,
	file = {~/Google Drive/library-html/Speziali2022PathsToImpact.html;~/Google Drive/library-pdf/Speziali2022PathsToImpact.pdf},
	abstract = {Intro The effective altruism ({EA}) movement needs several
                  career paths which can absorb a large number of people (as
                  described in this recent {EA} forum post), that have clear
                  paths to impact and are highly regarded by the community. We
                  also believe that having support for those career paths can
                  lead to {EA} having a bigger tent and generally allowing
                  healthier community norms. We believe that the for-profit
                  career path done right can be one of those, as it can
                  definitely absorb a lot of people and h},
	author = {Speziali, Federico},
	date = {2022-06-10},
	journaltitle = {High Impact Professionals},
	langid = {english},
	timestamp = {2023-09-27 10:58:39 (GMT)},
	title = {Paths to Impact for {EA} Working Professionals},
	url = {https://www.highimpactprofessionals.org/post/paths-to-impact-for-ea-working-professionals},
	urldate = {2023-09-27}
}

@Book{Spiros2023ModernMeat,
	date = {2023},
	langid = {english},
	edition = {1},
	url = {https://www.cellag.org/wp-content/uploads/2023/09/Modern-Meat-9-18-23.pdf},
	publisher = {Cellular Agriculture Society},
	editor = {Spiros, Kris and Sawyer, Hall and Darling, Jane and Benami, Maya},
	title = {Modern Meat},
	timestamp = {2023-09-19 12:40:18 (GMT)}
}

@online{Stafforini2021StyleGuide,
	url = {https://forum.effectivealtruism.org/topics/style-guide},
	date = {2021-04-17},
	journaltitle = {Effective Altruism Wiki},
	title = {Style guide},
	author = {Stafforini, Pablo},
	timestamp = {2023-10-07 11:52:12 (GMT)}
}

@article{Steinhardt2014AnotherCritiqueOf,
	file = {~/Google Drive/library-html/Steinhardt2014AnotherCritiqueOf.html;~/Google Drive/library-pdf/Steinhardt2014AnotherCritiqueOf.pdf},
	date = {2014-01-05},
	database = {Tlön},
	journaltitle = {LessWrong},
	author = {Steinhardt, Jacob },
	title = {Another critique of effective altruism},
	url = {https://www.lesswrong.com/posts/CZmkPvzkMdQJxXy54/another-critique-of-effective-altruism},
	abstract = {Recently Ben Kuhn wrote a critique of effective
                  altruism. I'm glad to see such self-examination taking
                  place, but I'm also concerned that the essay d...},
	langid = {english},
	timestamp = {2023-09-25 18:28:02 (GMT)},
	urldate = {2023-09-25}
}

@online{Swain2017PlentyOfFish,
	file = {~/Google Drive/library-html/Swain2017PlentyOfFish.html;~/Google Drive/library-pdf/Swain2017PlentyOfFish.pdf},
	date = {2017-04-13},
	author = {Swain, Marian},
	database = {Tlön},
	abstract = {Demand for seafood is growing, but many wild fish
                  stocks are already under strain from overfishing.
                  Instead of harvesting more wild fish, aquaculture-or
                  fish farming-is poised to dominate the future of
                  seafood production. While intensive commercial fish
                  farming has taken a toll on the environment,...},
	journaltitle = {The Breakthrough Institute},
	langid = {english},
	timestamp = {2023-09-19 12:41:15 (GMT)},
	title = {Plenty of fish on the farm},
	url = {https://thebreakthrough.org/articles/plenty-of-fish-on-the-farm},
	urldate = {2023-09-19}
}

@article{Sweeney2013DiscriminationInOnline,
	file = {~/Google Drive/library-pdf/Sweeney2013DiscriminationInOnline.pdf},
	author = {Sweeney, Latanya},
	title = {Discrimination in online ad delivery: Google ads, black names
                  and white names, racial discrimination, and click advertising},
	volume = {11},
	number = {3},
	pages = {10--29},
	doi = {10.1145/2460276.2460278},
	url = {https://dl.acm.org/doi/10.1145/2460276.2460278},
	abstract = {Do online ads suggestive of arrest records appear more often
                  with searches of black-sounding names than white-sounding
                  names? What is a black-sounding name or white-sounding name,
                  anyway? How many more times would an ad have to appear
                  adversely affecting one racial group for it to be considered
                  discrimination? Is online activity so ubiquitous that computer
                  scientists have to think about societal consequences such as
                  structural racism in technology design? If so, how is this
                  technology to be built? Let's take a scientific dive into
                  online ad delivery to find answers.},
	date = {2013-03},
	issn = {1542-7730, 1542-7749},
	journaltitle = {Queue},
	langid = {english},
	shortjournal = {Queue},
	shorttitle = {Discrimination in Online Ad Delivery},
	timestamp = {2023-09-28 10:46:38 (GMT)},
	urldate = {2023-09-28}
}

@online{Tilli2023CanMySelf,
	file = {~/Google Drive/library-html/Tilli2023CanMySelf.html;~/Google Drive/library-pdf/Tilli2023CanMySelf.pdf},
	date = {2020-10-11},
	journaltitle = {Effective Altruism Forum},
	author = {Tilli, Cecilia},
	title = {Can my self-worth compare to my instrumental value?},
	url = {https://forum.effectivealtruism.org/posts/yYiLv7rCHMNP98dZ5/can-my-self-worth-compare-to-my-instrumental-value},
	abstract = {A personal reflection on how my experience of {EA} is similar
                  to my experience of religious faith in that it provides a
                  sense of purpose and belonging,...},
	langid = {english},
	timestamp = {2023-09-27 11:26:18 (GMT)},
	urldate = {2023-09-27}
}

@online{Todd2017AllEvidenceBased,
	file = {~/Google Drive/library-html/Todd2017AllEvidenceBased.html;~/Google Drive/library-pdf/Todd2017AllEvidenceBased.pdf},
	date = {2017-04},
	database = {Tlön},
	author = {Todd, Benjamin},
	abstract = {Much advice on how to be successful is wrong, or
                  useless cliches. Here we cover the best advice we've
                  found in the last 5 years that's backed by
                  evidence.},
	langid = {american},
	shorttitle = {From social skills to sleep patterns},
	timestamp = {2023-01-16 14:53:29 (GMT)},
	title = {All the evidence-based advice we found on how to be more successful in any job},
	journaltitle = {80,000 Hours},
	url = {https://80000hours.org/career-guide/how-to-be-successful/},
	urldate = {2023-01-16}
}

@online{Todd2017NoMatterYour,
	file = {~/Google Drive/library-pdf/Todd2017NoMatterYour.pdf;~/Google Drive/library-html/Todd2017NoMatterYour.html},
	eventdate = {2023-05},
	database = {Tlön},
	date = {2017-04},
	author = {Todd, Benjamin},
	abstract = {Any college graduate in a rich country can do a huge
                  amount to improve lives.},
	journaltitle = {80,000 Hours},
	langid = {american},
	timestamp = {2023-09-26 07:09:59 (GMT)},
	title = {No matter your job, here's 3 evidence-based ways to
                  have a real impact},
	url = {https://80000hours.org/career-guide/making-a-difference/},
	urldate = {2023-09-26}
}

@online{Todd2018OperationsManagementIn,
	file = {~/Google Drive/library-html/Todd2018OperationsManagementIn.html;~/Google Drive/library-pdf/Todd2018OperationsManagementIn.pdf},
	eventdate = {2022-03},
	date = {2018-03},
	author = {Todd, Benjamin and Duda, Roman},
	abstract = {We argue that operations management is among the
                  highest-impact roles in the effective altruism and existential
                  risk communities right now, and address common misconceptions
                  about the roles.},
	journaltitle = {80,000 Hours},
	langid = {american},
	timestamp = {2023-09-27 10:03:23 (GMT)},
	title = {Operations management in high-impact organizations},
	url = {https://80000hours.org/articles/operations-management/},
	urldate = {2023-09-27}
}

@online{Todd2019AdviceHowTo,
	file = {~/Google Drive/library-html/Todd2019AdviceHowTo.html;~/Google Drive/library-pdf/Todd2019AdviceHowTo.pdf},
	date = {2019-11},
	author = {Todd, Benjamin},
	abstract = {Eight ways readers sometimes apply our advice that we wouldn't
                  necessarily recommend.},
	journaltitle = {80,000 Hours},
	langid = {american},
	timestamp = {2023-09-27 09:50:52 (GMT)},
	title = {Advice on how to read our advice},
	url = {https://80000hours.org/articles/advice-on-how-to-read-our-advice/},
	urldate = {2023-09-27}
}

@online{Todd2020WheresBestPlace,
	file = {~/Google Drive/library-html/Todd2020WheresBestPlace.html;~/Google Drive/library-pdf/Todd2020WheresBestPlace.pdf},
	eventdate = {2016-12},
	date = {2020-12},
	author = {Todd, Benjamin},
	abstract = {We talk a lot about where to donate, but people also often ask
                  us about where is best to volunteer. This is not something
                  we've researched, but here are some quick notes on the topic.
                  First we'll discuss some ways in which volunteering can be
                  overrated and not as impactful as it first seems. But then
                  we'll go through how volunteering can be impactful in some
                  circumstances, especially if you're trying to explore an area
                  and build career capital, or are applying a particular skill
                  you've developed.},
	journaltitle = {80,000 Hours},
	langid = {american},
	timestamp = {2023-09-27 10:51:25 (GMT)},
	title = {Where's the best place to volunteer?},
	url = {https://80000hours.org/articles/volunteering/},
	urldate = {2023-09-27}
}

@online{Todd2023FounderOfNew,
	file = {~/Google Drive/library-html/Todd2023FounderOfNew.html;~/Google Drive/library-pdf/Todd2023FounderOfNew.pdf},
	date = {2023-09-20},
	author = {Todd, Benjamin},
	abstract = {This path involves aiming to found new organisations that aim
                  to tackle bottlenecks in the problems we think are most
                  pressing. In particular, it involves identifying an idea,
                  testing it, and then helping to build an organisation by
                  investing in strategy, hiring, management, culture and so on,
                  with the aim that the organisation can continue to function
                  well without you in the long term. Our focus is on non-profit
                  models since they have the greatest need right now, but
                  [for-profits can also be a route to
                  impact](https://twitter.com/ben\_j\_todd/status/1425053800675614726).},
	journaltitle = {80,000 Hours},
	langid = {american},
	timestamp = {2023-09-27 10:00:27 (GMT)},
	title = {Founder of new projects tackling top problems},
	url = {https://80000hours.org/career-reviews/founder-impactful-organisations/},
	urldate = {2023-09-27}
}

@online{Todd2023HowMuchDo,
	file = {~/Google Drive/library-pdf/Todd2023HowMuchDo.pdf;~/Google Drive/library-html/Todd2023HowMuchDo.html},
	abstract = {In a 2013 paper, Dr Toby Ord reviewed data that
                  compared about 100 health interventions in developing
                  countries in terms of how many years of illness they
                  prevent per dollar. He discovered some striking facts
                  about the data.},
	author = {Todd, Benjamin},
	date = {2023-02-14},
	journaltitle = {80,000 Hours},
	langid = {american},
	shorttitle = {How much do solutions to social problems differ in
                  their effectiveness?},
	timestamp = {2023-09-20 18:57:59 (GMT)},
	title = {How much do solutions to social problems differ in
                  their effectiveness? A collection of all the studies
                  we could find.},
	url = {https://80000hours.org/2023/02/how-much-do-solutions-differ-in-effectiveness/},
	urldate = {2023-09-20}
}

@online{Tomasik2012SufferingInAnimals,
	file = {~/Google Drive/library-html/Tomasik2012SufferingInAnimals.html;~/Google Drive/library-pdf/Tomasik2012SufferingInAnimals.pdf},
	eventdate = {2017-08-04},
	date = {2012},
	journaltitle = {Essays on Reducing Suffering},
	title = {Suffering in animals vs. humans},
	author = {Tomasik, Brian},
	timestamp = {2023-09-27 07:17:24 (GMT)}
}

@online{Tomasik2016HowSimulationArgument,
	file = {~/Google Drive/library-html/Tomasik2016HowSimulationArgument.html;~/Google Drive/library-pdf/Tomasik2016HowSimulationArgument.pdf},
	abstract = {The simulation argument suggests a non-trivial chance
                  that most of the copies of ourselves are instantiated
                  in relatively short-lived ancestor simulations run by
                  superintelligent civilizations. If so, when we act to
                  help others in the short run, our good deeds are
                  duplicated many times over. This reasoning
                  dramatically upshifts the relative importance of
                  short-term helping over focusing on the far future.},
	author = {Tomasik, Brian},
	date = {2016-08-23},
	journaltitle = {Center on Long-Term Risk},
	langid = {american},
	timestamp = {2023-09-26 09:52:18 (GMT)},
	title = {How the simulation argument dampens future fanaticism},
	url = {https://longtermrisk.org/how-the-simulation-argument-dampens-future-fanaticism/},
	urldate = {2023-09-26}
}

@online{Toner2019SustainableMotivation,
	file = {~/Google Drive/library-pdf/Toner2019SustainableMotivation.pdf;~/Google Drive/library-html/Toner2019SustainableMotivation.html},
	date = {2019-08-29},
	journaltitle = {EAGlobal},
	author = {Toner, Helen},
	title = {Sustainable motivation},
	url = {https://forum.effectivealtruism.org/posts/WuWDS4SmtLNd6sKtb/helen-toner-sustainable-motivation},
	abstract = {---------------------------------------- ...},
	langid = {english},
	shorttitle = {Helen Toner},
	timestamp = {2023-09-26 07:23:21 (GMT)},
	urldate = {2023-09-26}
}

@report{Trajtenberg2018AiAsNext,
	file = {~/Google Drive/library-pdf/Trajtenberg2018AiAsNext.pdf},
	author = {Trajtenberg, Manuel},
	date = {2018-01},
	doi = {10.3386/w24245},
	institution = {National Bureau of Economic Research},
	langid = {english},
	location = {Cambridge, {MA}},
	number = {working paper 24245},
	shorttitle = {{AI} as the next {GPT}},
	timestamp = {2023-09-28 10:57:29 (GMT)},
	title = {{AI} as the next {GPT}: a political-economy perspective},
	url = {http://www.nber.org/papers/w24245.pdf},
	urldate = {2023-09-28}
}

@online{Triedman2022TowardsIneffectiveAltruism,
	file = {~/Google Drive/library-pdf/Triedman2022TowardsIneffectiveAltruism.pdf;~/Google Drive/library-html/Triedman2022TowardsIneffectiveAltruism.html},
	date = {2022-05-22},
	journaltitle = {Reboot},
	abstract = {There are boundless ways of doing good that are
                  fundamentally immeasurable or, if they are measurable,
                  may not be optimized.},
	author = {Triedman, Hal and Ahlawat, Archana and Dai, Jessica},
	langid = {english},
	timestamp = {2023-09-25 18:22:04 (GMT)},
	title = {Towards ineffective altruism},
	url = {https://joinreboot.org/p/ineffective-altruism},
	urldate = {2023-09-25}
}

@article{Turchin2020ClassificationOfGlobal,
	author = {Turchin, Alexey and Denkenberger, David},
	title = {classification of global catastrophic risks connected with
artificial intelligence},
	volume = {35},
	number = {1},
	pages = {147--163},
	doi = {10.1007/s00146-018-0845-5},
	url = {http://link.springer.com/10.1007/s00146-018-0845-5},
	date = {2020-03},
	issn = {0951-5666, 1435-5655},
	journaltitle = {{AI} \& {SOCIETY}},
	langid = {english},
	shortjournal = {{AI} \& Soc},
	timestamp = {2023-09-28 12:50:58 (GMT)},
	urldate = {2023-09-28}
}

@online{Urban2015ArtificialIntelligenceRevolution,
	file = {~/Google Drive/library-html/Urban2015ArtificialIntelligenceRevolution.html;~/Google Drive/library-pdf/Urban2015ArtificialIntelligenceRevolution.pdf},
	abstract = {Part 1 of 2: "The Road to Superintelligence".
                  Artificial Intelligence - the topic everyone in the
                  world should be talking about.},
	author = {Urban, Tim},
	date = {2015-01-22},
	journaltitle = {Wait But Why},
	langid = {american},
	shorttitle = {The Artificial Intelligence Revolution},
	timestamp = {2023-09-26 11:28:59 (GMT)},
	title = {The artificial intelligence revolution: Part 1},
	url = {https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html},
	urldate = {2023-09-26}
}

@article{Urbina2022DualUseOf,
	file = {~/Google Drive/library-pdf/Urbina2022DualUseOf.pdf},
	author = {Urbina, Fabio and Lentzos, Filippa and Invernizzi, C{\'e}dric
                  and Ekins, Sean},
	title = {Dual use of artificial-intelligence-powered drug discovery},
	volume = {4},
	number = {3},
	pages = {189--191},
	doi = {10.1038/s42256-022-00465-9},
	url = {https://www.nature.com/articles/s42256-022-00465-9},
	date = {2022-03-07},
	issn = {2522-5839},
	journaltitle = {Nature Machine Intelligence},
	langid = {english},
	shortjournal = {Nat Mach Intell},
	timestamp = {2023-09-28 12:46:26 (GMT)},
	urldate = {2023-09-28}
}

@book{Wells2022WorldSetFree,
	address = {London},
	author = {Wells, H. G.},
	title = {The World Set Free},
	publisher = {Macmillan & Co.},
	date = {1914},
	langid = {english},
	timestamp = {2023-10-05 13:42:28 (GMT)}
}

@online{Wiblin2015DisagreeingAboutWhats,
	file = {~/Google Drive/library-pdf/Wiblin2015DisagreeingAboutWhats.pdf;~/Google Drive/library-html/Wiblin2015DisagreeingAboutWhats.html},
	abstract = {Lately I have had the uncanny experience of reading
                  supposed 'rebuttals' of effective altruism that just
                  say a bunch of things that I and most of my colleagues
                  agree with. As we are some of the most involved people
                  in the effective altruism movement, this is strange to
                  say the least. What is going on here is that effective
                  altruism is both a narrow core idea, and a bunch of
                  associated ideas. Some of these associated ideas
                  happen to be widely held by people who describe
                  themselves as effective altruists - others don't even
                  meet that standard.},
	database = {Tlön},
	author = {Wiblin, Robert},
	date = {2015-07-16},
	journaltitle = {80,000 Hours},
	langid = {american},
	timestamp = {2023-09-25 17:25:14 (GMT)},
	title = {Disagreeing about what's effective isn't disagreeing
                  with effective altruism},
	url = {https://80000hours.org/2015/07/disagreeing-about-whats-effective-isnt-disagreeing-with-effective-altruism/},
	urldate = {2023-09-25}
}

@online{Wiblin2020FlawsThatMake,
	file = {~/Google Drive/library-html/Wiblin2020FlawsThatMake.html;~/Google Drive/library-pdf/Wiblin2020FlawsThatMake.pdf},
	database = {Tlön},
	title = {Stuart Russell on the flaws that make today’s {AI} architecture unsafe and a new approach that could fix it},
	langid = {english},
	url = {https://80000hours.org/podcast/episodes/stuart-russell-human-compatible-ai/},
	journaltitle = {80,000 Hours},
	author = {Wiblin, Robert and Harris, Keiran},
	date = {2020-06-22}
}

@online{Wiblin2022AndreasMogensenWhether,
	database = {Tlön},
	title = {Andreas Mogensen on whether effective altruism is just for consequentialists},
	url = {https://80000hours.org/podcast/episodes/andreas-mogensen-deontology-and-effective-altruism/},
	abstract = {In a world as full of preventable suffering as our own, a simple 'principle of beneficence' is probably the only premise one needs to grant for the effective altruist project of identifying the most impactful ways to help others to be of great moral interest and importance.},
	journaltitle = {80,000 Hours Podcast},
	author = {Wiblin, Robert and Harris, Keiran},
	urldate = {2022-10-20},
	date = {2022-09-08},
	langid = {american},
	file = {~/Google Drive/library-html/andreas-mogensen-deontology-and-effective-altruism.html}
}

@online{Wikipedia2016ListOfNuclear,
	file = {~/Google Drive/library-pdf/Wikipedia2016ListOfNuclear.pdf;~/Google Drive/library-html/Wikipedia2016ListOfNuclear.html},
	eventdate = {2023-09-26},
	database = {Tlön},
	author = {Wikipedia},
	abstract = {A nuclear close call is an incident that might have led to at
                  least one unintended nuclear detonation or explosion, but did
                  not. These incidents typically involve a perceived imminent
                  threat to a nuclear-armed country which could lead to
                  retaliatory strikes against the perceived aggressor. The
                  damage caused by international nuclear exchange is not
                  necessarily limited to the participating countries, as the
                  hypothesized rapid climate change associated with even
                  small-scale regional nuclear war could threaten food
                  production worldwide-a scenario known as nuclear
                  famine.Despite a reduction in global nuclear tensions and
                  major nuclear arms reductions after the end of the Cold War
                  following the collapse of the Soviet Union in 1991, estimated
                  nuclear warhead stockpiles total roughly 15,000 worldwide,
                  with the United States and Russia holding 90\% of the
                  total.Though exact details on many nuclear close calls are
                  hard to come by, the analysis of particular cases has
                  highlighted the importance of a variety of factors in
                  preventing accidents. At an international level, this includes
                  the importance of context and outside mediation; at the
                  national level, effectiveness in government communications,
                  and involvement of key decision-makers; and, at the individual
                  level, the decisive role of individuals in following intuition
                  and prudent decision-making, often in violation of protocol.},
	booktitle = {Wikipedia},
	date = {2016-04-03},
	langid = {english},
	note = {Page Version {ID}: 1177122161},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	timestamp = {2023-09-27 15:20:53 (GMT)},
	title = {List of nuclear close calls},
	url = {https://en.wikipedia.org/w/index.php?title=List_of_nuclear_close_calls&oldid=1177122161},
	urldate = {2023-09-27}
}

@online{WildAnimalInitiative2023Faq,
	file = {~/Google Drive/library-html/WildAnimalInitiative2023Faq.html;~/Google Drive/library-pdf/WildAnimalInitiative2023Faq.pdf},
	date = {2023},
	author = {{Wild Animal Initiative}},
	journaltitle = {Wild Animal Initiative},
	langid = {american},
	timestamp = {2023-09-27 07:37:14 (GMT)},
	title = {{FAQ}},
	url = {https://www.wildanimalinitiative.org/faq},
	urldate = {2023-09-27}
}

@online{Wise2012Tradeoffs,
	file = {~/Google Drive/library-pdf/Wise2012Tradeoffs.pdf;~/Google Drive/library-html/Wise2012Tradeoffs.html},
	date = {2012-03-23},
	langid = {english},
	database = {Tlön},
	journaltitle = {Giving gladly},
	abstract = {Economists love to think about tradeoffs (or
                  opportunity costs, as they call them). Any money we
                  spend can't be spent on something else, so...},
	author = {Wise, Julia},
	timestamp = {2023-09-26 07:19:48 (GMT)},
	title = {Tradeoffs},
	url = {http://www.givinggladly.com/2012/03/tradeoffs.html},
	urldate = {2023-09-26}
}

@online{Wu2022EffectiveAltruismNotb,
	file = {~/Google Drive/library-html/Wu2022EffectiveAltruismNotb.html;~/Google Drive/library-pdf/Wu2022EffectiveAltruismNotb.pdf},
	journaltitle = {The Phoenix},
	database = {Tlön},
	author = {Wu, Megan},
	date = {2022-03-31},
	langid = {english},
	shorttitle = {Effective Altruism},
	timestamp = {2023-09-25 18:49:08 (GMT)},
	title = {Effective altruism: not effective and not altruistic},
	url = {https://forum.effectivealtruism.org/posts/Jx6ncakmergiC74kG/deference-culture-in-ea},
	urldate = {2023-09-25}
}

@online{Xuan2020AiAlignmentPhilosophical,
	database = {Tlön},
	date = {2020-11-30},
	journaltitle = {{EAGx} Asia-Pacific},
	author = {Xuan, Tan Zhi},
	abstract = {How can we build (super) intelligent machines that are
                  robustly aligned with human values? {AI} alignment
                  researchers strive to meet this challenge, but
                  curren...},
	langid = {spanish},
	timestamp = {2023-09-25 19:55:42 (GMT)},
	title = {{AI} alignment, philosophical pluralism, and the
                  relevance of non-Western philosophy},
	url = {https://www.youtube.com/watch?v=dbMp4pFVwnU},
	urldate = {2023-09-25}
}

@misc{Zhang2022ForecastingAiProgress,
	file = {~/Google Drive/library-pdf/Zhang2022ForecastingAiProgress.pdf},
	abstract = {Advances in artificial intelligence ({AI}) are shaping modern
                  life, from transportation, health care, science, finance, to
                  national defense. Forecasts of {AI} development could help
                  improve policy- and decision-making. We report the results
                  from a large survey of {AI} and machine learning ({ML})
                  researchers on their beliefs about progress in {AI}. The
                  survey, fielded in late 2019, elicited forecasts for near-term
                  {AI} development milestones and high- or human-level machine
                  intelligence, defined as when machines are able to accomplish
                  every or almost every task humans are able to do currently. As
                  part of this study, we re-contacted respondents from a
                  highly-cited study by Grace et al. (2018), in which {AI}/{ML}
                  researchers gave forecasts about high-level machine
                  intelligence and near-term milestones in {AI} development.
                  Results from our 2019 survey show that, in aggregate,
                  {AI}/{ML} researchers surveyed placed a 50\% likelihood of
                  human-level machine intelligence being achieved by 2060. The
                  results show researchers newly contacted in 2019 expressed
                  similar beliefs about the progress of advanced {AI} as
                  respondents in the Grace et al. (2018) survey. For the
                  recontacted participants from the Grace et al. (2018) study,
                  the aggregate forecast for a 50\% likelihood of high-level
                  machine intelligence shifted from 2062 to 2076, although this
                  change is not statistically significant, likely due to the
                  small size of our panel sample. Forecasts of several near-term
                  {AI} milestones have reduced in time, suggesting more optimism
                  about {AI} progress. Finally, {AI}/{ML} researchers also
                  exhibited significant optimism about how human-level machine
                  intelligence will impact society.},
	author = {Zhang, Baobao and Dreksler, Noemi and Anderljung, Markus and
                  Kahn, Lauren and Giattino, Charlie and Dafoe, Allan and
                  Horowitz, Michael C.},
	date = {2022-06-08},
	doi = {10.48550/arXiv.2206.04132},
	eprint = {2206.04132 [cs]},
	eprinttype = {arxiv},
	keywords = {Computer Science - Computers and Society, K.4.1},
	number = {{arXiv}:2206.04132},
	publisher = {{arXiv}},
	shorttitle = {Forecasting {AI} Progress},
	timestamp = {2023-10-08 19:56:53 (GMT)},
	title = {Forecasting {AI} progress: evidence from a survey of machine
                  learning researchers},
	url = {http://arxiv.org/abs/2206.04132},
	urldate = {2023-10-08}
}

